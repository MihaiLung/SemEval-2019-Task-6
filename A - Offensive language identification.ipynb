{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-task A - Offensive language identification\n",
    "Given a text, your system should categorise it into OFF (offensive) and NOT (not offensive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/Test A Release/testset-taska.tsv'\n",
    "path = '../data/start-kit/training-v1/offenseval-training-v1.tsv'\n",
    "data_train = pd.read_table(path,index_col='id')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the text of each tweet : Extracts : \n",
    "- Hashtags\n",
    "- Words\n",
    "- Words cut on CAPs and Numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train['hashtags'] = data_train['tweet'].apply(lambda x : re.compile('#(\\w+)').findall(x))\n",
    "data_train['words'] = data_train['tweet'].apply(lambda x : re.compile('\\w+').findall(x))\n",
    "data_train['words_cut'] = data_train['tweet'].apply(lambda x : re.compile('[0-9]+|[A-Z][a-z]+|[A-Z]+|[a-z]+').findall(x))\n",
    "data_train['words_cut_low'] = data_train['words_cut'].apply(lambda l : list(map(lambda x: x.lower(),l)))\n",
    "data_train['words_clean'] = data_train['words_cut_low'].apply(lambda l : list(filter(lambda x : x != 'user' ,l)))\n",
    "data_train.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.iloc[0].tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x.lower(),u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x : x != 's' ,u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('')\n",
    "tokenizer.tokenize('@USER Sh ðŸ–•ðŸ–•ðŸ–•ðŸ–•ðŸ–• e is drinking WeAreTheChampions Fort so iLoveTheCat much koolaid sheâ€™s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
