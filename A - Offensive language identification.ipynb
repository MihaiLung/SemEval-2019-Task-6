{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import mkdir\n",
    "from os.path import join, isfile, isdir, exists\n",
    "import bcolz\n",
    "import pickle \n",
    "import emoji\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from pattern.en import spelling\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings : GloVe\n",
    "This Class Loads the GloVe Embeding, processes it, and create a word embedding given the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe_embedding(object):\n",
    "    def __init__(self,dim_vect = 25 ):\n",
    "        ########## VARIABLES ##########\n",
    "        self.dim_vect = dim_vect\n",
    "\n",
    "        # Defining variables for GloVe: \n",
    "        self.words = []\n",
    "        self.word2idx = {}\n",
    "        self.glove_dict = {}\n",
    "        \n",
    "        ########## LOADING GLOVE DATA ##########\n",
    "        \n",
    "        # Defining path for GloVe Data : \n",
    "        self.path = join('..','data','glove') # Path of glove\n",
    "        self.path_glove = join(self.path,'glove.twitter.27B.'+str(dim_vect))\n",
    "        if not(isdir(self.path_glove)):\n",
    "            mkdir(self.path_glove)\n",
    "        self.path_vec_original = join(self.path,'glove.twitter.27B.'+str(dim_vect)+'d.txt') # Path of glove original vectors\n",
    "        self.path_vec_save = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.vectors.dat')  # Path of glove saved vectors\n",
    "        self.path_words = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.words.pkl')\n",
    "        self.path_word2idx = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.word2idx.pkl')\n",
    "                \n",
    "        if not(isdir(self.path_vec_save) and isfile(self.path_words) and isfile(self.path_word2idx)) : \n",
    "            # If files are allready processed, just load them\n",
    "            print('---- Processing the GloVe files : ',end='')\n",
    "            self.process_GloVe()\n",
    "            print('Done')\n",
    "            \n",
    "        # Load the wordvec files\n",
    "        print('---- Loading the processed GloVe files : ',end='')\n",
    "        self.load_GloVe()\n",
    "        print('Done')\n",
    "        \n",
    "        ########## TORCH EMBEDDING ##########\n",
    "        \n",
    "        # Defining variables for our Embedding:\n",
    "        self.size_vocab = len(self.words)\n",
    "        \n",
    "        # Creating the Pytorch Embedding Layer : \n",
    "        print('---- Creating the Pytorch Embedding Layer  : ',end='')\n",
    "        self.emb_layer = nn.Embedding(self.size_vocab, self.dim_vect)\n",
    "        self.create_emb_layer(non_trainable=True)\n",
    "        print('Done')\n",
    "\n",
    "               \n",
    "    def process_GloVe(self):\n",
    "        ''' Processes the GloVe Dataset - Saves files'''\n",
    "        words = []\n",
    "        word2idx = {}\n",
    "        \n",
    "        vectors = bcolz.carray(np.zeros(1) , rootdir=self.path_vec_save , mode='w' ) # defining vector saved\n",
    "        \n",
    "        # Adding Padding vector : \n",
    "        word2idx['<pad>'] = 0\n",
    "        words.append('<pad>')\n",
    "        #vect = np.random.normal(scale=0.6, size=(self.dim_vect , )) # random padding vect\n",
    "        vect = np.zeros((self.dim_vect , )) # 0's padding vect. \n",
    "        vectors.append(vect)\n",
    "        \n",
    "        idx = 1\n",
    "        with open(self.path_vec_original, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "                \n",
    "\n",
    "        vectors = bcolz.carray(vectors[:].reshape((-1, self.dim_vect)), rootdir=self.path_vec_save, mode='w')\n",
    "\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(self.path_words, 'wb'))\n",
    "        pickle.dump(word2idx, open(self.path_word2idx, 'wb'))\n",
    "        \n",
    "    def load_GloVe(self):\n",
    "        ''' Loads previously processed dataset'''\n",
    "        \n",
    "        vectors = bcolz.open(self.path_vec_save)[:]\n",
    "        \n",
    "        self.words = pickle.load(open(self.path_words, 'rb'))\n",
    "        self.word2idx = pickle.load(open(self.path_word2idx, 'rb'))\n",
    "        \n",
    "        self.glove_dict = {w: vectors[self.word2idx[w]] for w in self.words}\n",
    "        self.emb_matrix = torch.Tensor(vectors)\n",
    "    \n",
    "    def create_emb_layer(self, non_trainable=True):\n",
    "        self.emb_layer.load_state_dict({'weight': self.emb_matrix})\n",
    "        if non_trainable:\n",
    "            self.emb_layer.weight.requires_grad = False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n"
     ]
    }
   ],
   "source": [
    "myEmbedding = GloVe_embedding(dim_vect=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This Class Loads the Tweet Dataset, Cleans it. It also enables the loading for the training and testing. \n",
    "TODO : Loading for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetDataset(Dataset):\n",
    "    def __init__(self,data, subtask):\n",
    "        data_  = data\n",
    "        \n",
    "        self.id = data_.index.tolist()\n",
    "        self.token = data_.token.tolist()\n",
    "        self.token_id = data_.token_id.tolist()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "    \n",
    "class TrainTweetDataset(Dataset):\n",
    "    def __init__(self,data, subtask ):\n",
    "        \n",
    "          # Adapt to each subtask : \n",
    "        if subtask == 'subtask_a' :\n",
    "            label_id_n = 'labelA'\n",
    "            self.classes_dict = {'NOT' : 0 ,'OFF' : 1}\n",
    "            data_ = data\n",
    "            \n",
    "        elif subtask == 'subtask_b' :\n",
    "            label_id_n = 'labelB'\n",
    "            self.classes_dict = {'UNT' : 0 ,'TIN' : 1}\n",
    "            data_ = data.loc[(data['subtask_b'] == 'UNT') | (data['subtask_b'] == 'TIN') ]\n",
    "            \n",
    "        elif subtask == 'subtask_c' :\n",
    "            label_id_n = 'labelC'\n",
    "            self.classes_dict = {'IND' : 0 ,'OTH' : 1, 'GRP' : 2}\n",
    "            data_ = data.loc[(data['subtask_c'] == 'IND') | (data['subtask_c'] == 'OTH') | (data['subtask_c'] == 'GRP') ]\n",
    "            \n",
    "        data_[label_id_n] = data_[subtask].apply(lambda x : self.classes_dict[x])  \n",
    "        self.id = data_.index.tolist()\n",
    "        self.label = data_[subtask].tolist()\n",
    "        self.label_id = data_[label_id_n].tolist()\n",
    "        self.token = data_.token.tolist()\n",
    "        self.token_id = data_.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.label_id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "\n",
    "\n",
    "    \n",
    "class DataHandling(object):\n",
    "    def __init__(self, embedding):\n",
    "        \n",
    "        self.embedding = embedding\n",
    "\n",
    "        # All the Text Data path\n",
    "        self.path = {}\n",
    "        self.path_clean = {}\n",
    "        \n",
    "        self.path['train'] = join('..','data','start-kit','training-v1','offenseval-training-v1.tsv')\n",
    "        self.path_clean['train'] = join('..','data','start-kit','training-v1','clean-offenseval-training-v1.tsv')\n",
    "        \n",
    "        self.path['subtask_a'] = join('..','data','Test A Release','testset-taska.tsv')\n",
    "        self.path_clean['subtask_a'] = join('..','data','Test A Release','clean-testset-taska.tsv')\n",
    "        \n",
    "        self.path['subtask_b'] = join('..','data','Test B Release','testset-taskb.tsv')\n",
    "        self.path_clean['subtask_b'] = join('..','data','Test B Release','clean-testset-taskb.tsv')\n",
    "        \n",
    "        self.path['subtask_c'] = join('..','data','Test C Release','test_set_taskc.tsv')\n",
    "        self.path_clean['subtask_c'] = join('..','data','Test C Release','clean-test_set_taskc.tsv')\n",
    "        \n",
    "        self.data = {}\n",
    "        \n",
    "        if not(isfile(self.path_clean['train']) and isfile(self.path_clean['subtask_a']) and isfile(self.path_clean['subtask_b']) and isfile(self.path_clean['subtask_c']) ) : \n",
    "            ### PROCESSING OF THE ORIGINAL DATASET\n",
    "            # Load, Clean and Tokenize the Datasets\n",
    "            print('---- Load, Clean and Tokensize Dataset : ',end='')\n",
    "            \n",
    "            self.inital_dataload()\n",
    "            print('Done')\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            print('---- Finalize tokenized words and translation to id : ',end='')\n",
    "            self.all_words_freq = {}\n",
    "            self.all_words = []\n",
    "            self.compute_wordlist()\n",
    "            self.token2id()\n",
    "            print('Done')\n",
    "\n",
    "            # Add Embedding and correct clean the words not in embedding : \n",
    "            print('---- Adapt Dataset for Embedding : ',end='')\n",
    "            self.adaptDataset()\n",
    "            print('Done')\n",
    "\n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Saving all tokenized words : ',end='')\n",
    "            self.save_cleanDataset()\n",
    "            print('Done')\n",
    "        else : \n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Load the Clean Adapted Dataset : ',end='')\n",
    "            self.load_cleanDataset()\n",
    "            print('Done')\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            print('---- Compute word list : ',end='')\n",
    "            self.all_words_freq = {}\n",
    "            self.all_words = []\n",
    "            self.compute_wordlist()\n",
    "            print('Done')\n",
    "        \n",
    "    def getDataset(self, dataT='train',subtask='subtask_a'):\n",
    "        ''' Returns the pytorch Dataset\n",
    "            - file : {'train','test'}\n",
    "            - subtask : {'subtask_a','subtask_b','subtask_c'} '''\n",
    "        \n",
    "        if dataT == 'train':\n",
    "            dataset = TrainTweetDataset(self.data[dataT], subtask)\n",
    "        elif dataT == 'test':\n",
    "            dataset = TestTweetDataset(self.data[subtask], subtask)\n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    def token2id(self):\n",
    "        for f in self.path : \n",
    "            def token2id_x(x):\n",
    "                \n",
    "                return [self.embedding.word2idx[k] for k in x if k in self.embedding.words]\n",
    "            self.data[f]['token_id'] = self.data[f]['token'].apply(lambda x : token2id_x(x))\n",
    "            #print(self.data[f])\n",
    "\n",
    "    def save_cleanDataset(self):\n",
    "        for f in self.path : \n",
    "            self.data[f].to_csv(self.path_clean[f])\n",
    "        \n",
    "    def load_cleanDataset(self):\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_csv(self.path_clean[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : ast.literal_eval(x))\n",
    "            self.data[f]['token_id'] = self.data[f]['token_id'].apply(lambda x : ast.literal_eval(x))\n",
    "           \n",
    "               \n",
    "    def adaptDataset(self):\n",
    "        # Find all words wich are not in the Embedding :\n",
    "        missing_words = []\n",
    "        for i, word in enumerate(self.all_words) :\n",
    "            if self.embedding.word2idx.get(word) == None : \n",
    "                missing_words.append(word)\n",
    "        \n",
    "        # Correct if possible the missing_words : \n",
    "        ### We use theshold over which we correct the word. Under which we discard the word\n",
    "        t = 0.5 # threshold\n",
    "        rejected_words = []\n",
    "        corrected_words = {}\n",
    "        for word in tqdm(missing_words) : \n",
    "            suggestion, prob = spelling.suggest(word)[0]\n",
    "            if prob < t : \n",
    "                rejected_words.append(word)\n",
    "            else : \n",
    "                corrected_words[word] = suggestion\n",
    "        \n",
    "        # Modify the Original Datasets with those corrected_words : \n",
    "        for f in self.path : \n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [corrected_words.get(k,k) for k in x])\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [k for k in x if k not in rejected_words ])\n",
    "        nb_rejected = len(rejected_words)\n",
    "        nb_corrected = len(corrected_words)\n",
    "        nb_vocab = len(self.embedding.glove_dict)\n",
    "        p_rejected = 100* nb_rejected / nb_vocab\n",
    "        p_corrected = 100* nb_corrected / nb_vocab\n",
    "        print('---- Words removed   : {0:} / {1:.2f} - {2:} %'.format(nb_rejected,nb_vocab,p_rejected))\n",
    "        print('---- Words corrected : {0:} / {1:.2f} - {2:} %'.format(nb_corrected,nb_vocab,p_corrected))\n",
    "        \n",
    "    def inital_dataload(self):\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_table(self.path[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['tweet'].apply(lambda x : self.clean_tweet(x))\n",
    "            \n",
    "    def compute_wordlist(self):\n",
    "        \n",
    "        for f in self.data : \n",
    "            for i in range(len(self.data[f])):\n",
    "                for e in self.data[f].iloc[i].token:\n",
    "                    self.all_words_freq[e] = 1 + self.all_words_freq.get(e,0)\n",
    "        self.all_words = list(self.all_words_freq.keys())\n",
    "    \n",
    "    def clean_tweet(self,text):\n",
    "        ''' Function that is applied to every to tweet in the dataset '''\n",
    "        \n",
    "        # =========== TEXT ===========\n",
    "        # Replace @USER by <user>\n",
    "        text = re.compile(r'@USER').sub(r'<user>',text)\n",
    "\n",
    "        # Replace URL by <url>\n",
    "        text = re.compile(r'URL').sub(r'<url>',text)\n",
    "\n",
    "        # Remove numbers :\n",
    "        text = re.compile(r'[0-9]+').sub(r' ',text)\n",
    "\n",
    "        # Remove some special characters\n",
    "        text = re.compile(r'([\\xa0_\\{\\}\\[\\]¬•$,:;/@#|\\^*%().~`”\"“-])').sub(r' ',text) \n",
    "\n",
    "        # Space the special characters with white spaces\n",
    "        text = re.compile(r'([$&+,:;=?@#|\\'.^*()%!\"’“-])').sub(r' \\1 ',text)\n",
    "        \n",
    "        # Replace some special characters : \n",
    "        replace_dict = {r'&' : 'and' , \n",
    "                        r'\\+' : 'plus'}\n",
    "        for cha in replace_dict:\n",
    "            text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
    "            \n",
    "        # Handle Emoji : translate some and delete the others\n",
    "        text = self.handle_emoji(text)\n",
    "        \n",
    "        # Word delengthening : \n",
    "        text = re.compile(r'(.)\\1{3,}').sub(r'\\1\\1',text)\n",
    "\n",
    "        # Cut the words with caps in them : \n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
    "        # =========== TOKENS ===========\n",
    "        # TOKENIZE \n",
    "        text = text.split(' ')\n",
    "\n",
    "        # Remove white spaces tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
    "\n",
    "        # Remove empty tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != '']\n",
    "\n",
    "        # Remove repetition in tokens (!!! => !)\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
    "\n",
    "        #  Handle the ALL CAPS Tweets \n",
    "        ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
    "        caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
    "        if caps_r > 0.6 : \n",
    "            text.append('<allcaps>')\n",
    "\n",
    "        # Lower Case : \n",
    "        text = [text[i].lower() for i in range(len(text))]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def handle_emoji(self,text):\n",
    "        # Dictionnary of \"important\" emojis : \n",
    "        emoji_dict =  {'♥️': ' love ',\n",
    "                       '❤️' : ' love ',\n",
    "                       '❤' : ' love ',\n",
    "                       '😘' : ' kisses ',\n",
    "                      '😭' : ' cry ',\n",
    "                      '💪' : ' strong ',\n",
    "                      '🌍' : ' earth ',\n",
    "                      '💰' : ' money ',\n",
    "                      '👍' : ' ok ',\n",
    "                       '👌' : ' ok ',\n",
    "                      '😡' : ' angry ',\n",
    "                      '🍆' : ' dick ',\n",
    "                      '🤣' : ' haha ',\n",
    "                      '😂' : ' haha ',\n",
    "                      '🖕' : ' fuck you '}\n",
    "\n",
    "        for cha in emoji_dict:\n",
    "            text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
    "        # Remove ALL emojis\n",
    "        text = emoji.get_emoji_regexp().sub(r' ',text) \n",
    "        text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
    "        text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
    "\n",
    "        # Add Space between  the Emoji Expressions : \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load the Clean Adapted Dataset : Done\n",
      "---- Compute word list : Done\n"
     ]
    }
   ],
   "source": [
    "mydata = DataHandling(myEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrian/anaconda3/envs/ic_std/lib/python3.6/site-packages/ipykernel/__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([     1,  30168,     71,     76, 666233,     10]), tensor([1.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = mydata.getDataset(dataT='train',subtask='subtask_c')\n",
    "train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " 'the',\n",
       " 'is',\n",
       " 'to',\n",
       " 'and',\n",
       " 'a',\n",
       " 'you',\n",
       " 'of',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'are',\n",
       " '!',\n",
       " 'he',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'she',\n",
       " '?',\n",
       " 's',\n",
       " 'for']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(list(mydata.all_words_freq.keys()))\n",
    "list(words[list(np.argsort(list(mydata.all_words_freq.values())))[::-1][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86426</th>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, she, should, ask, a, few, native, ame...</td>\n",
       "      <td>[1, 148, 277, 565, 12, 1087, 17411, 7437, 87, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90194</th>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>[&lt;user&gt;, go, home, you, re, drunk, !, &lt;user&gt;, ...</td>\n",
       "      <td>[1, 112, 336, 16, 797, 1511, 10, 1, 29896, 286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16820</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
       "      <td>[2547, 33, 39459, 3492, 16270, 128, 71, 5442, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62688</th>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, someone, should, ', ve, taken, this, ...</td>\n",
       "      <td>[1, 239, 277, 49, 572, 1942, 54, 2479, 40, 186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43605</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, obama, wanted, liberals, and, amp, il...</td>\n",
       "      <td>[1, 1382, 953, 30168, 27, 12801, 114750, 17, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97670</th>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>[&lt;user&gt;, liberals, are, all, kookoo, !]</td>\n",
       "      <td>[1, 30168, 71, 76, 666233, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77444</th>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, oh, noes, !, tough, shit]</td>\n",
       "      <td>[1, 194, 73880, 10, 3832, 186]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52415</th>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[&lt;user&gt;, was, literally, just, talking, about,...</td>\n",
       "      <td>[1, 94, 1469, 60, 654, 122, 54, 89, 76, 6866, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45157</th>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, buy, more, icecream, !]</td>\n",
       "      <td>[1, 873, 146, 18658, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13384</th>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>[&lt;user&gt;, canada, doesn, t, need, another, cuck...</td>\n",
       "      <td>[1, 3160, 69393, 188, 172, 599, 407870, 10, 81...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet subtask_a subtask_b  \\\n",
       "id                                                                             \n",
       "86426  @USER She should ask a few native Americans wh...       OFF       UNT   \n",
       "90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF       TIN   \n",
       "16820  Amazon is investigating Chinese employees who ...       NOT       NaN   \n",
       "62688  @USER Someone should'veTaken\" this piece of sh...       OFF       UNT   \n",
       "43605  @USER @USER Obama wanted liberals &amp; illega...       NOT       NaN   \n",
       "97670                  @USER Liberals are all Kookoo !!!       OFF       TIN   \n",
       "77444                   @USER @USER Oh noes! Tough shit.       OFF       UNT   \n",
       "52415  @USER was literally just talking about this lo...       OFF       TIN   \n",
       "45157                         @USER Buy more icecream!!!       NOT       NaN   \n",
       "13384  @USER Canada doesn’t need another CUCK! We alr...       OFF       TIN   \n",
       "\n",
       "      subtask_c                                              token  \\\n",
       "id                                                                   \n",
       "86426       NaN  [<user>, she, should, ask, a, few, native, ame...   \n",
       "90194       IND  [<user>, go, home, you, re, drunk, !, <user>, ...   \n",
       "16820       NaN  [amazon, is, investigating, chinese, employees...   \n",
       "62688       NaN  [<user>, someone, should, ', ve, taken, this, ...   \n",
       "43605       NaN  [<user>, obama, wanted, liberals, and, amp, il...   \n",
       "97670       OTH            [<user>, liberals, are, all, kookoo, !]   \n",
       "77444       NaN                 [<user>, oh, noes, !, tough, shit]   \n",
       "52415       GRP  [<user>, was, literally, just, talking, about,...   \n",
       "45157       NaN                   [<user>, buy, more, icecream, !]   \n",
       "13384       IND  [<user>, canada, doesn, t, need, another, cuck...   \n",
       "\n",
       "                                                token_id  \n",
       "id                                                        \n",
       "86426  [1, 148, 277, 565, 12, 1087, 17411, 7437, 87, ...  \n",
       "90194  [1, 112, 336, 16, 797, 1511, 10, 1, 29896, 286...  \n",
       "16820  [2547, 33, 39459, 3492, 16270, 128, 71, 5442, ...  \n",
       "62688  [1, 239, 277, 49, 572, 1942, 54, 2479, 40, 186...  \n",
       "43605  [1, 1382, 953, 30168, 27, 12801, 114750, 17, 9...  \n",
       "97670                     [1, 30168, 71, 76, 666233, 10]  \n",
       "77444                     [1, 194, 73880, 10, 3832, 186]  \n",
       "52415  [1, 94, 1469, 60, 654, 122, 54, 89, 76, 6866, ...  \n",
       "45157                           [1, 873, 146, 18658, 10]  \n",
       "13384  [1, 3160, 69393, 188, 172, 599, 407870, 10, 81...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.data['train'].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier \n",
    "Set of Classes used as classifier for the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
    "        print('---- Creating FFNN : ',end='')\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "        \n",
    "        # Activation Layers\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.output = nn.Sigmoid()\n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # we average the embeddings of words in a sentence\n",
    "        averaged = embedded.mean(1)\n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_tweet(batch):\n",
    "    '''\n",
    "    Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. \n",
    "    args: \n",
    "        batch - List of elements ( x , label )\n",
    "    return \n",
    "        batch - Padded ( list(x) , list(label))\n",
    "    \n",
    "    '''\n",
    "    batch = list(zip(*batch))\n",
    "    max_len = max([len(t) for t in batch[0]])\n",
    "    batch[0] = torch.stack([pad_tensor(vec=t, pad=max_len, dim=0) for t in batch[0]],dim=0)\n",
    "    batch[1] = torch.stack(batch[1])\n",
    "    return batch[0] , batch[1]\n",
    "\n",
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    return torch.cat([vec, torch.zeros(*pad_size,dtype=torch.long)], dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class : Trainer\n",
    "Main Class for the loading, training, testing etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffensiveClassifier(object):\n",
    "    ''' Main Class for the loading, training, testing etc ...'''\n",
    "    def __init__(self,subtask='subtask_a', dim_vect=25,cType='FFNN'):\n",
    "        \n",
    "        self.dim_vect = dim_vect\n",
    "        \n",
    "        self.subtask = subtask\n",
    "\n",
    "        # Loading the GloVe Embedding and Torch Formating of this Embedding\n",
    "        self.GloVe = GloVe_embedding(dim_vect= dim_vect )\n",
    "        self.embedding = self.GloVe.emb_layer\n",
    "        \n",
    "        # Loading the Tweet Data : \n",
    "        self.dataHandler = DataHandling(self.GloVe)\n",
    "        \n",
    "        # Retrieving Training DataSet (pytorch)\n",
    "        self.train_set = self.dataHandler.getDataset('train',subtask)\n",
    "        \n",
    "        \n",
    "        # Retrieving Test DataSet (pytorch)\n",
    "        self.test_set = self.dataHandler.getDataset('test',subtask)\n",
    "        \n",
    "        # Classification : \n",
    "        if cType == 'FFNN':\n",
    "            # Creating the Neuronal Network\n",
    "            self.model = FFNN(self.embedding, 20, 1, self.dim_vect)\n",
    "        if cType == 'logistic':\n",
    "            pass\n",
    "        \n",
    "\n",
    "    def accuracy(self, output, target ):\n",
    "\n",
    "        output = torch.round(torch.sigmoid(output))\n",
    "        correct = (output == target).float()\n",
    "        acc = correct.sum()/len(correct)\n",
    "        return acc\n",
    "    \n",
    "    def train( self, nb_epochs, lr=0.001, batch_size = 1000 ):\n",
    "        \n",
    "        self.train_generator = DataLoader(self.train_set, batch_size=batch_size,collate_fn=padding_tweet, shuffle=True)\n",
    "\n",
    "        # we use the stochastic gradient descent (SGD) optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(),lr=lr)\n",
    "        \n",
    "        loss_fn = self.model.loss_fn()\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "\n",
    "            for tokens, target  in self.train_generator :\n",
    "                target = target.float()\n",
    "                tokens = tokens.long()\n",
    "                #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "                #good practice to include even if do not use here\n",
    "                self.model.train()\n",
    "\n",
    "                #we zero the gradients as they are not removed automatically\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # queeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
    "                predictions = self.model(tokens)\n",
    "                #print(predictions,target)\n",
    "                loss = loss_fn(predictions, target)\n",
    "                acc = self.accuracy(predictions, target)\n",
    "                \n",
    "                #calculate the gradient of each parameter\n",
    "                loss.backward()\n",
    "                \n",
    "                #update the parameters using the gradients and optimizer algorithm \n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss = loss.item()\n",
    "                epoch_acc = acc\n",
    "\n",
    "                print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}%')\n",
    "        \n",
    "    def test(self):\n",
    "        ''' \n",
    "            Test Function : Tests the Network on the Test Data of the Subtask and Saves in a file\n",
    "        '''\n",
    "        self.test_generator = DataLoader(self.test_set,collate_fn= padding_tweet )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n",
      "---- Load the Clean Adapted Dataset : Done\n",
      "---- Compute word list : Done\n",
      "---- Creating FFNN : Done\n"
     ]
    }
   ],
   "source": [
    "taskAclassifier = OffensiveClassifier(subtask='subtask_a', dim_vect=100, cType='FFNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 00 | Train Loss: 0.536 | Train Acc: 32.95%\n",
      "| Epoch: 00 | Train Loss: 1.264 | Train Acc: 33.60%\n",
      "| Epoch: 00 | Train Loss: 0.639 | Train Acc: 32.65%\n",
      "| Epoch: 00 | Train Loss: 0.833 | Train Acc: 32.80%\n",
      "| Epoch: 00 | Train Loss: 0.798 | Train Acc: 32.80%\n",
      "| Epoch: 00 | Train Loss: 0.561 | Train Acc: 34.55%\n",
      "| Epoch: 00 | Train Loss: 0.621 | Train Acc: 33.31%\n",
      "| Epoch: 01 | Train Loss: 0.674 | Train Acc: 32.60%\n",
      "| Epoch: 01 | Train Loss: 0.706 | Train Acc: 33.05%\n",
      "| Epoch: 01 | Train Loss: 0.645 | Train Acc: 33.75%\n",
      "| Epoch: 01 | Train Loss: 0.565 | Train Acc: 32.90%\n",
      "| Epoch: 01 | Train Loss: 0.572 | Train Acc: 34.05%\n",
      "| Epoch: 01 | Train Loss: 0.622 | Train Acc: 32.50%\n",
      "| Epoch: 01 | Train Loss: 0.629 | Train Acc: 34.11%\n",
      "| Epoch: 02 | Train Loss: 0.585 | Train Acc: 33.70%\n",
      "| Epoch: 02 | Train Loss: 0.538 | Train Acc: 31.80%\n",
      "| Epoch: 02 | Train Loss: 0.566 | Train Acc: 33.30%\n",
      "| Epoch: 02 | Train Loss: 0.614 | Train Acc: 34.70%\n",
      "| Epoch: 02 | Train Loss: 0.600 | Train Acc: 33.65%\n",
      "| Epoch: 02 | Train Loss: 0.584 | Train Acc: 32.90%\n",
      "| Epoch: 02 | Train Loss: 0.563 | Train Acc: 32.18%\n",
      "| Epoch: 03 | Train Loss: 0.560 | Train Acc: 33.45%\n",
      "| Epoch: 03 | Train Loss: 0.574 | Train Acc: 34.65%\n",
      "| Epoch: 03 | Train Loss: 0.577 | Train Acc: 32.45%\n",
      "| Epoch: 03 | Train Loss: 0.566 | Train Acc: 31.15%\n",
      "| Epoch: 03 | Train Loss: 0.572 | Train Acc: 34.75%\n",
      "| Epoch: 03 | Train Loss: 0.557 | Train Acc: 33.20%\n",
      "| Epoch: 03 | Train Loss: 0.558 | Train Acc: 32.82%\n",
      "| Epoch: 04 | Train Loss: 0.573 | Train Acc: 32.60%\n",
      "| Epoch: 04 | Train Loss: 0.559 | Train Acc: 32.25%\n",
      "| Epoch: 04 | Train Loss: 0.556 | Train Acc: 34.15%\n",
      "| Epoch: 04 | Train Loss: 0.571 | Train Acc: 33.90%\n",
      "| Epoch: 04 | Train Loss: 0.562 | Train Acc: 32.45%\n",
      "| Epoch: 04 | Train Loss: 0.552 | Train Acc: 33.85%\n",
      "| Epoch: 04 | Train Loss: 0.563 | Train Acc: 33.55%\n",
      "| Epoch: 05 | Train Loss: 0.554 | Train Acc: 33.05%\n",
      "| Epoch: 05 | Train Loss: 0.559 | Train Acc: 33.20%\n",
      "| Epoch: 05 | Train Loss: 0.545 | Train Acc: 31.25%\n",
      "| Epoch: 05 | Train Loss: 0.553 | Train Acc: 33.30%\n",
      "| Epoch: 05 | Train Loss: 0.583 | Train Acc: 35.00%\n",
      "| Epoch: 05 | Train Loss: 0.563 | Train Acc: 33.55%\n",
      "| Epoch: 05 | Train Loss: 0.556 | Train Acc: 33.31%\n",
      "| Epoch: 06 | Train Loss: 0.562 | Train Acc: 32.85%\n",
      "| Epoch: 06 | Train Loss: 0.546 | Train Acc: 32.55%\n",
      "| Epoch: 06 | Train Loss: 0.561 | Train Acc: 33.70%\n",
      "| Epoch: 06 | Train Loss: 0.548 | Train Acc: 32.65%\n",
      "| Epoch: 06 | Train Loss: 0.561 | Train Acc: 34.45%\n",
      "| Epoch: 06 | Train Loss: 0.545 | Train Acc: 32.45%\n",
      "| Epoch: 06 | Train Loss: 0.581 | Train Acc: 34.44%\n",
      "| Epoch: 07 | Train Loss: 0.557 | Train Acc: 33.10%\n",
      "| Epoch: 07 | Train Loss: 0.547 | Train Acc: 33.95%\n",
      "| Epoch: 07 | Train Loss: 0.557 | Train Acc: 33.45%\n",
      "| Epoch: 07 | Train Loss: 0.553 | Train Acc: 32.90%\n",
      "| Epoch: 07 | Train Loss: 0.555 | Train Acc: 33.10%\n",
      "| Epoch: 07 | Train Loss: 0.552 | Train Acc: 33.40%\n",
      "| Epoch: 07 | Train Loss: 0.536 | Train Acc: 32.42%\n",
      "| Epoch: 08 | Train Loss: 0.536 | Train Acc: 31.60%\n",
      "| Epoch: 08 | Train Loss: 0.560 | Train Acc: 33.30%\n",
      "| Epoch: 08 | Train Loss: 0.551 | Train Acc: 34.75%\n",
      "| Epoch: 08 | Train Loss: 0.552 | Train Acc: 34.20%\n",
      "| Epoch: 08 | Train Loss: 0.541 | Train Acc: 31.25%\n",
      "| Epoch: 08 | Train Loss: 0.549 | Train Acc: 34.10%\n",
      "| Epoch: 08 | Train Loss: 0.551 | Train Acc: 33.55%\n",
      "| Epoch: 09 | Train Loss: 0.557 | Train Acc: 34.55%\n",
      "| Epoch: 09 | Train Loss: 0.536 | Train Acc: 32.30%\n",
      "| Epoch: 09 | Train Loss: 0.546 | Train Acc: 33.00%\n",
      "| Epoch: 09 | Train Loss: 0.549 | Train Acc: 34.15%\n",
      "| Epoch: 09 | Train Loss: 0.542 | Train Acc: 32.55%\n",
      "| Epoch: 09 | Train Loss: 0.547 | Train Acc: 33.00%\n",
      "| Epoch: 09 | Train Loss: 0.535 | Train Acc: 32.98%\n",
      "| Epoch: 10 | Train Loss: 0.539 | Train Acc: 32.00%\n",
      "| Epoch: 10 | Train Loss: 0.550 | Train Acc: 34.00%\n",
      "| Epoch: 10 | Train Loss: 0.544 | Train Acc: 33.30%\n",
      "| Epoch: 10 | Train Loss: 0.544 | Train Acc: 33.90%\n",
      "| Epoch: 10 | Train Loss: 0.550 | Train Acc: 33.05%\n",
      "| Epoch: 10 | Train Loss: 0.544 | Train Acc: 32.65%\n",
      "| Epoch: 10 | Train Loss: 0.541 | Train Acc: 34.03%\n",
      "| Epoch: 11 | Train Loss: 0.549 | Train Acc: 33.25%\n",
      "| Epoch: 11 | Train Loss: 0.554 | Train Acc: 33.75%\n",
      "| Epoch: 11 | Train Loss: 0.552 | Train Acc: 33.95%\n",
      "| Epoch: 11 | Train Loss: 0.528 | Train Acc: 32.50%\n",
      "| Epoch: 11 | Train Loss: 0.535 | Train Acc: 32.20%\n",
      "| Epoch: 11 | Train Loss: 0.550 | Train Acc: 33.55%\n",
      "| Epoch: 11 | Train Loss: 0.541 | Train Acc: 33.55%\n",
      "| Epoch: 12 | Train Loss: 0.536 | Train Acc: 33.45%\n",
      "| Epoch: 12 | Train Loss: 0.543 | Train Acc: 31.70%\n",
      "| Epoch: 12 | Train Loss: 0.555 | Train Acc: 33.75%\n",
      "| Epoch: 12 | Train Loss: 0.546 | Train Acc: 33.20%\n",
      "| Epoch: 12 | Train Loss: 0.546 | Train Acc: 34.55%\n",
      "| Epoch: 12 | Train Loss: 0.548 | Train Acc: 32.90%\n",
      "| Epoch: 12 | Train Loss: 0.526 | Train Acc: 32.98%\n",
      "| Epoch: 13 | Train Loss: 0.536 | Train Acc: 32.65%\n",
      "| Epoch: 13 | Train Loss: 0.525 | Train Acc: 31.35%\n",
      "| Epoch: 13 | Train Loss: 0.564 | Train Acc: 34.00%\n",
      "| Epoch: 13 | Train Loss: 0.542 | Train Acc: 33.15%\n",
      "| Epoch: 13 | Train Loss: 0.537 | Train Acc: 33.60%\n",
      "| Epoch: 13 | Train Loss: 0.560 | Train Acc: 35.30%\n",
      "| Epoch: 13 | Train Loss: 0.537 | Train Acc: 32.18%\n",
      "| Epoch: 14 | Train Loss: 0.558 | Train Acc: 33.20%\n",
      "| Epoch: 14 | Train Loss: 0.554 | Train Acc: 34.45%\n",
      "| Epoch: 14 | Train Loss: 0.535 | Train Acc: 31.65%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-397d3a3be0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaskAclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-139-0fec87b017bc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ic_std/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-53373215c55b>\u001b[0m in \u001b[0;36mpadding_tweet\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpad_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "taskAclassifier.train(500,lr=0.001,batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
