{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import mkdir\n",
    "from os.path import join, isfile, isdir, exists\n",
    "import bcolz\n",
    "import pickle \n",
    "import emoji\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from pattern.en import spelling\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings : GloVe\n",
    "This Class Loads the GloVe Embeding, processes it, and create a word embedding given the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe_embedding(object):\n",
    "    def __init__(self,dim_vect = 25 ):\n",
    "        ########## VARIABLES ##########\n",
    "        self.dim_vect = dim_vect\n",
    "\n",
    "        # Defining variables for GloVe: \n",
    "        self.words = []\n",
    "        self.word2idx = {}\n",
    "        self.glove_dict = {}\n",
    "        \n",
    "        ########## LOADING GLOVE DATA ##########\n",
    "        \n",
    "        # Defining path for GloVe Data : \n",
    "        self.path = join('..','data','glove') # Path of glove\n",
    "        self.path_glove = join(self.path,'glove.twitter.27B.'+str(dim_vect))\n",
    "        if not(isdir(self.path_glove)):\n",
    "            mkdir(self.path_glove)\n",
    "        self.path_vec_original = join(self.path,'glove.twitter.27B.'+str(dim_vect)+'d.txt') # Path of glove original vectors\n",
    "        self.path_vec_save = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.vectors.dat')  # Path of glove saved vectors\n",
    "        self.path_words = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.words.pkl')\n",
    "        self.path_word2idx = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.word2idx.pkl')\n",
    "                \n",
    "        if not(isdir(self.path_vec_save) and isfile(self.path_words) and isfile(self.path_word2idx)) : \n",
    "            # If files are allready processed, just load them\n",
    "            print('---- Processing the GloVe files : ',end='')\n",
    "            self.process_GloVe()\n",
    "            print('Done')\n",
    "            \n",
    "        # Load the wordvec files\n",
    "        print('---- Loading the processed GloVe files : ',end='')\n",
    "        self.load_GloVe()\n",
    "        print('Done')\n",
    "        \n",
    "        ########## TORCH EMBEDDING ##########\n",
    "        \n",
    "        # Defining variables for our Embedding:\n",
    "        self.size_vocab = len(self.words)\n",
    "        \n",
    "        # Creating the Pytorch Embedding Layer : \n",
    "        print('---- Creating the Pytorch Embedding Layer  : ',end='')\n",
    "        self.emb_layer = nn.Embedding(self.size_vocab, self.dim_vect)\n",
    "        self.create_emb_layer(non_trainable=True)\n",
    "        print('Done')\n",
    "\n",
    "               \n",
    "    def process_GloVe(self):\n",
    "        ''' Processes the GloVe Dataset - Saves files'''\n",
    "        words = []\n",
    "        word2idx = {}\n",
    "        \n",
    "        vectors = bcolz.carray(np.zeros(1) , rootdir=self.path_vec_save , mode='w' ) # defining vector saved\n",
    "        \n",
    "        # Adding Padding vector : \n",
    "        word2idx['<pad>'] = 0\n",
    "        words.append('<pad>')\n",
    "        #vect = np.random.normal(scale=0.6, size=(self.dim_vect , )) # random padding vect\n",
    "        vect = np.zeros((self.dim_vect , )) # 0's padding vect. \n",
    "        vectors.append(vect)\n",
    "        \n",
    "        idx = 1\n",
    "        with open(self.path_vec_original, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "                \n",
    "\n",
    "        vectors = bcolz.carray(vectors[:].reshape((-1, self.dim_vect)), rootdir=self.path_vec_save, mode='w')\n",
    "\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(self.path_words, 'wb'))\n",
    "        pickle.dump(word2idx, open(self.path_word2idx, 'wb'))\n",
    "        \n",
    "    def load_GloVe(self):\n",
    "        ''' Loads previously processed dataset'''\n",
    "        \n",
    "        vectors = bcolz.open(self.path_vec_save)[:]\n",
    "        \n",
    "        self.words = pickle.load(open(self.path_words, 'rb'))\n",
    "        self.word2idx = pickle.load(open(self.path_word2idx, 'rb'))\n",
    "        \n",
    "        self.glove_dict = {w: vectors[self.word2idx[w]] for w in self.words}\n",
    "        self.emb_matrix = torch.Tensor(vectors)\n",
    "    \n",
    "    def create_emb_layer(self, non_trainable=True):\n",
    "        self.emb_layer.load_state_dict({'weight': self.emb_matrix})\n",
    "        if non_trainable:\n",
    "            self.emb_layer.weight.requires_grad = False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n"
     ]
    }
   ],
   "source": [
    "myEmbedding = GloVe_embedding(dim_vect=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This Class Loads the Tweet Dataset, Cleans it. It also enables the loading for the training and testing. \n",
    "TODO : Loading for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetDataset(Dataset):\n",
    "    ''' \n",
    "    Pytorch Dataset for the Test set. \n",
    "    initialisation : - data : training pandas dataframe\n",
    "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
    "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
    "    '''\n",
    "    def __init__(self,data, subtask):\n",
    "        self.id = data.index.tolist()\n",
    "        self.token = data.token.tolist()\n",
    "        self.token_id = data.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "\n",
    "        \n",
    "    \n",
    "class TweetDataset(Dataset):\n",
    "    ''' \n",
    "    Pytorch Dataset for the Training set. \n",
    "    initialisation : - data : training pandas dataframe\n",
    "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
    "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
    "    '''\n",
    "    def __init__(self,data,subtask):        \n",
    "        # Save in lists the ids, labels, label_id, token, and token_id . \n",
    "        self.id = data.index.tolist()\n",
    "        self.label_id = data[subtask].tolist()\n",
    "        self.token = data.token.tolist()\n",
    "        self.token_id = data.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.label_id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandling(object):\n",
    "    def __init__(self, embedding, pValid):\n",
    "        print('-- Data Handling : ')\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.defineClasses()\n",
    "\n",
    "        # All the Text Data path\n",
    "        self.definePath()\n",
    "        \n",
    "        self.data = {}\n",
    "        \n",
    "        processed_ = True\n",
    "        for f in self.path:\n",
    "            processed_ = processed_ and isfile(self.path_clean[f])\n",
    "        \n",
    "        if  not(processed_) : \n",
    "            ### PROCESSING OF THE ORIGINAL DATASET\n",
    "            # Load, Clean and Tokenize the Datasets\n",
    "            print('---- Load, Clean and Tokensize Dataset : ',end='')\n",
    "            self.inital_dataload()\n",
    "            print('Done')\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            print('---- Finalize tokenized words and translation to id : ',end='')\n",
    "            self.compute_wordlist()\n",
    "            self.token2id()\n",
    "            print('Done')\n",
    "\n",
    "            # Add Embedding and correct clean the words not in embedding : \n",
    "            print('---- Adapt Dataset for Embedding : ',end='')\n",
    "            self.adaptDataset()\n",
    "            print('Done')\n",
    "\n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Saving all tokenized words : ',end='')\n",
    "            self.save_cleanDataset()\n",
    "            print('Done')\n",
    "        else : \n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Load the Clean Adapted Dataset : ',end='')\n",
    "            self.load_cleanDataset()\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            self.compute_wordlist()\n",
    "            print('Done')\n",
    "        \n",
    "        # Create Validation Set (split the test dataset) for every subtask\n",
    "        self.splitValidation(p=pValid)\n",
    "        self.prepareLabels()\n",
    "        \n",
    "        \n",
    "    def defineClasses(self):\n",
    "        ''' Function that defines the classes labels and id per subtask '''\n",
    "        self.classes_dict = {}\n",
    "        self.classes_dict['subtask_a'] = {'NOT' : 0 ,'OFF' : 1}\n",
    "        self.classes_dict['subtask_b'] = {'UNT' : 0 ,'TIN' : 1}\n",
    "        self.classes_dict['subtask_c'] = {'IND' : 0 ,'OTH' : 1, 'GRP' : 2}\n",
    "        \n",
    "    def definePath(self):\n",
    "        ''' Function that defines all the paths of the datasets. '''\n",
    "        self.path = {}\n",
    "        self.path_clean = {}\n",
    "        \n",
    "        self.path['train'] = join('..','data','start-kit','training-v1','offenseval-training-v1.tsv')\n",
    "        self.path_clean['train'] = join('..','data','start-kit','training-v1','clean-offenseval-training-v1.tsv')\n",
    "        \n",
    "        self.path['subtask_a'] = join('..','data','Test A Release','testset-taska.tsv')\n",
    "        self.path_clean['subtask_a'] = join('..','data','Test A Release','clean-testset-taska.tsv')\n",
    "        \n",
    "        self.path['subtask_b'] = join('..','data','Test B Release','testset-taskb.tsv')\n",
    "        self.path_clean['subtask_b'] = join('..','data','Test B Release','clean-testset-taskb.tsv')\n",
    "        \n",
    "        self.path['subtask_c'] = join('..','data','Test C Release','test_set_taskc.tsv')\n",
    "        self.path_clean['subtask_c'] = join('..','data','Test C Release','clean-test_set_taskc.tsv')\n",
    "        \n",
    "    def getDataset(self, dataT='train',subtask='subtask_a',balanced = True):\n",
    "        ''' Returns the pytorch Dataset\n",
    "            - file : {'train','test','validation'}\n",
    "            - subtask : {'subtask_a','subtask_b','subtask_c'} '''\n",
    "        \n",
    "            \n",
    "        if dataT == 'train':\n",
    "            if balanced : \n",
    "                data_train = self.balanceData(self.data[dataT][subtask],subtask)\n",
    "            else : \n",
    "                data_train = self.data[dataT][subtask]\n",
    "            dataset = TweetDataset(data_train, subtask)\n",
    "        elif dataT == 'validation':\n",
    "            dataset = TweetDataset(self.data[dataT][subtask], subtask)\n",
    "        elif dataT == 'test':\n",
    "            dataset = TestTweetDataset(self.data[subtask], subtask)\n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    def token2id(self):\n",
    "        ''' Function that translates the list of tokens into a list of token id of the embedding.\n",
    "            Adds a new 'token_id' column to the dataframe '''\n",
    "        for f in self.path : \n",
    "            def token2id_x(x):\n",
    "                \n",
    "                return [self.embedding.word2idx[k] for k in x if k in self.embedding.words]\n",
    "            self.data[f]['token_id'] = self.data[f]['token'].apply(lambda x : token2id_x(x))\n",
    "\n",
    "    def save_cleanDataset(self):\n",
    "        ''' Saves at the defined path the cleaned dataset '''\n",
    "        for f in self.path : \n",
    "            self.data[f].to_csv(self.path_clean[f])\n",
    "        \n",
    "    def load_cleanDataset(self):\n",
    "        ''' Loads at the defined path the cleaned dataset '''\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_csv(self.path_clean[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : ast.literal_eval(x))\n",
    "            self.data[f]['token_id'] = self.data[f]['token_id'].apply(lambda x : ast.literal_eval(x))\n",
    "           \n",
    "               \n",
    "    def adaptDataset(self):\n",
    "        ''' Function that finds all the words which are not in the embedding and tries to \n",
    "            correct them with the pattern.en package by taking the most probable replacement.\n",
    "            If the suggested word in very unlikely, the word is removed from the tweets. \n",
    "        '''\n",
    "        # Find all words wich are not in the Embedding :\n",
    "        missing_words = []\n",
    "        for i, word in enumerate(self.all_words) :\n",
    "            if self.embedding.word2idx.get(word) == None : \n",
    "                missing_words.append(word)\n",
    "        \n",
    "        # Correct if possible the missing_words : \n",
    "        ### We use theshold over which we correct the word. Under which we discard the word\n",
    "        t = 0.5 # threshold\n",
    "        rejected_words = []\n",
    "        corrected_words = {}\n",
    "        for word in tqdm(missing_words) : \n",
    "            suggestion, prob = spelling.suggest(word)[0]\n",
    "            if prob < t : \n",
    "                rejected_words.append(word)\n",
    "            else : \n",
    "                corrected_words[word] = suggestion\n",
    "        \n",
    "        # Modify the Original Datasets with those corrected_words : \n",
    "        for f in self.path : \n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [corrected_words.get(k,k) for k in x])\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [k for k in x if k not in rejected_words ])\n",
    "        nb_rejected = len(rejected_words)\n",
    "        nb_corrected = len(corrected_words)\n",
    "        nb_vocab = len(self.embedding.glove_dict)\n",
    "        p_rejected = 100* nb_rejected / nb_vocab\n",
    "        p_corrected = 100* nb_corrected / nb_vocab\n",
    "        print('---- Words removed   : {0:} / {1:.2f} - {2:} %'.format(nb_rejected,nb_vocab,p_rejected))\n",
    "        print('---- Words corrected : {0:} / {1:.2f} - {2:} %'.format(nb_corrected,nb_vocab,p_corrected))\n",
    "        \n",
    "    def inital_dataload(self):\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_table(self.path[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['tweet'].apply(lambda x : self.clean_tweet(x))\n",
    "            \n",
    "    def compute_wordlist(self):\n",
    "        self.all_words_freq = {}\n",
    "        self.all_words = []\n",
    "        \n",
    "        for f in self.data : \n",
    "            for i in range(len(self.data[f])):\n",
    "                for e in self.data[f].iloc[i].token:\n",
    "                    self.all_words_freq[e] = 1 + self.all_words_freq.get(e,0)\n",
    "        self.all_words = list(self.all_words_freq.keys())\n",
    "        \n",
    "    def splitValidation(self,p):\n",
    "        ''' Creates the validation set by  taking p % of the train dataset '''\n",
    "        data = self.data['train'].copy()\n",
    "        self.data['train'] = {}\n",
    "        self.data['validation'] = {}\n",
    "\n",
    "        for subtask in self.classes_dict: # per subtask\n",
    "            self.data['train'][subtask] = pd.DataFrame()\n",
    "            for label in self.classes_dict[subtask]: #per label in this subtask \n",
    "                data_label =  data[data[subtask]==label]\n",
    "                self.data['train'][subtask] = self.data['train'][subtask].append(data.loc[data_label.index])\n",
    "                nb_valid = int(len(data_label)*p)\n",
    "                # Select randmoly (without repetition) the indexes of the selected vaidation tweets\n",
    "                index_valid = np.random.choice(data_label.index,(nb_valid,),replace=False)\n",
    "                # Add the the selected validation tweets to the new dataframe\n",
    "                self.data['validation'][subtask] = self.data['train'][subtask].loc[index_valid,:]\n",
    "                # Drop the selected validation tweets from the training set\n",
    "                self.data['train'][subtask] = self.data['train'][subtask].drop(index = index_valid)\n",
    "                \n",
    "    def prepareLabels(self) : \n",
    "        ''' Transform the labels into classes id '''\n",
    "        for subtask in self.classes_dict: # per subtask\n",
    "            self.data['validation'][subtask][subtask] =self.data['validation'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
    "            self.data['train'][subtask][subtask] = self.data['train'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
    "\n",
    "    def balanceData(self,data,subtask):\n",
    "        ''' Augments the Data given in input in order to balance the dataset'''\n",
    "        class_size = {}\n",
    "        for label in self.classes_dict[subtask]:\n",
    "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
    "        largest_class = max(class_size, key=class_size.get)\n",
    "        print('---- Augmenting the Data : ')\n",
    "        print('Before Augmentation : ',class_size)\n",
    "\n",
    "        for label in self.classes_dict[subtask]:  \n",
    "            if label != largest_class:\n",
    "                id_list = data[data[subtask]==self.classes_dict[subtask][label]].index\n",
    "                nb_augmentation = class_size[largest_class] - class_size[label]\n",
    "                id_augmentation = np.random.choice(id_list, (nb_augmentation,))\n",
    "                data = data.append(data.loc[id_augmentation,:])\n",
    "        # Check if it went well\n",
    "        for label in self.classes_dict[subtask]:\n",
    "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
    "        \n",
    "        print('After Augmentation : ',class_size)\n",
    "        return data\n",
    "    \n",
    "    def clean_tweet(self,text):\n",
    "        ''' Function that is applied to every to tweet in the dataset '''\n",
    "        \n",
    "        # =========== TEXT ===========\n",
    "        # Replace @USER by <user>\n",
    "        text = re.compile(r'@USER').sub(r'<user>',text)\n",
    "\n",
    "        # Replace URL by <url>\n",
    "        text = re.compile(r'URL').sub(r'<url>',text)\n",
    "\n",
    "        # Remove numbers :\n",
    "        text = re.compile(r'[0-9]+').sub(r' ',text)\n",
    "\n",
    "        # Remove some special characters\n",
    "        text = re.compile(r'([\\xa0_\\{\\}\\[\\]¬•$,:;/@#|\\^*%().~`”\"“-])').sub(r' ',text) \n",
    "\n",
    "        # Space the special characters with white spaces\n",
    "        text = re.compile(r'([$&+,:;=?@#|\\'.^*()%!\"’“-])').sub(r' \\1 ',text)\n",
    "        \n",
    "        # Replace some special characters : \n",
    "        replace_dict = {r'&' : 'and' , \n",
    "                        r'\\+' : 'plus'}\n",
    "        for cha in replace_dict:\n",
    "            text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
    "            \n",
    "        # Handle Emoji : translate some and delete the others\n",
    "        text = self.handle_emoji(text)\n",
    "        \n",
    "        # Word delengthening : \n",
    "        text = re.compile(r'(.)\\1{3,}').sub(r'\\1\\1',text)\n",
    "\n",
    "        # Cut the words with caps in them : \n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
    "        # =========== TOKENS ===========\n",
    "        # TOKENIZE \n",
    "        text = text.split(' ')\n",
    "\n",
    "        # Remove white spaces tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
    "\n",
    "        # Remove empty tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != '']\n",
    "\n",
    "        # Remove repetition in tokens (!!! => !)\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
    "\n",
    "        #  Handle the ALL CAPS Tweets \n",
    "        ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
    "        caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
    "        if caps_r > 0.6 : \n",
    "            text.append('<allcaps>')\n",
    "\n",
    "        # Lower Case : \n",
    "        text = [text[i].lower() for i in range(len(text))]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def handle_emoji(self,text):\n",
    "        # Dictionnary of \"important\" emojis : \n",
    "        emoji_dict =  {'♥️': ' love ',\n",
    "                       '❤️' : ' love ',\n",
    "                       '❤' : ' love ',\n",
    "                       '😘' : ' kisses ',\n",
    "                      '😭' : ' cry ',\n",
    "                      '💪' : ' strong ',\n",
    "                      '🌍' : ' earth ',\n",
    "                      '💰' : ' money ',\n",
    "                      '👍' : ' ok ',\n",
    "                       '👌' : ' ok ',\n",
    "                      '😡' : ' angry ',\n",
    "                      '🍆' : ' dick ',\n",
    "                      '🤣' : ' haha ',\n",
    "                      '😂' : ' haha ',\n",
    "                      '🖕' : ' fuck you '}\n",
    "\n",
    "        for cha in emoji_dict:\n",
    "            text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
    "        # Remove ALL emojis\n",
    "        text = emoji.get_emoji_regexp().sub(r' ',text) \n",
    "        text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
    "        text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
    "\n",
    "        # Add Space between  the Emoji Expressions : \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Data Handling : \n",
      "---- Load the Clean Adapted Dataset : Done\n"
     ]
    }
   ],
   "source": [
    "mydata = DataHandling(myEmbedding, pValid=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Augmenting the Data : \n",
      "Before Augmentation :  {'NOT': 7514, 'OFF': 3740}\n",
      "After Augmentation :  {'NOT': 7514, 'OFF': 7514}\n"
     ]
    }
   ],
   "source": [
    "train = mydata.getDataset(dataT='train',subtask='subtask_a', balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier \n",
    "Set of Classes used as classifier for the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
    "        print('------ Creating FFNN : ',end='')\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "        \n",
    "        # Activation Layers\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.output = nn.Sigmoid()\n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # we average the embeddings of words in a sentence\n",
    "        \n",
    "        non_zero_nb = (x!=0).sum(1,keepdim=True)\n",
    "        #print(x.shape, non_zero_nb,embedded.sum(1).shape)\n",
    "        averaged = embedded.sum(1) / non_zero_nb.float()\n",
    "        #averaged = embedded.mean(1)\n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_tweet(batch):\n",
    "    '''\n",
    "    Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. \n",
    "    args: \n",
    "        batch - List of elements ( x , label )\n",
    "    return \n",
    "        batch - Padded ( list(x) , list(label))\n",
    "    \n",
    "    '''\n",
    "    batch = list(zip(*batch))\n",
    "    max_len = max([len(t) for t in batch[0]])\n",
    "    batch[0] = torch.stack([pad_tensor(vec=t, pad=max_len, dim=0) for t in batch[0]],dim=0)\n",
    "    batch[1] = torch.stack(batch[1])\n",
    "    return batch[0] , batch[1]\n",
    "\n",
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    return torch.cat([vec, torch.zeros(*pad_size,dtype=torch.long)], dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class : Trainer\n",
    "Main Class for the loading, training, testing etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffensiveClassifier(object):\n",
    "    ''' Main Class for the loading, training, testing etc ...'''\n",
    "    def __init__(self,subtask='subtask_a', dim_vect=25,cType='FFNN',pValid = 0.15):\n",
    "        \n",
    "        self.dim_vect = dim_vect\n",
    "        \n",
    "        self.subtask = subtask\n",
    "\n",
    "        # Loading the GloVe Embedding and Torch Formating of this Embedding\n",
    "        self.GloVe = GloVe_embedding(dim_vect= dim_vect )\n",
    "        self.embedding = self.GloVe.emb_layer\n",
    "        \n",
    "        # Loading the Data Handler : \n",
    "        self.dataHandler = DataHandling(self.GloVe,pValid=pValid)\n",
    "        \n",
    "        # Retrieving Training DataSet (pytorch)\n",
    "        self.train_set = self.dataHandler.getDataset('train',subtask,balanced=True)\n",
    "        \n",
    "        # Retrieving the Validation Set (pytorch)\n",
    "        self.valid_set = self.dataHandler.getDataset('validation',subtask)\n",
    "\n",
    "        # Retrieving Test DataSet (pytorch)\n",
    "        self.test_set = self.dataHandler.getDataset('test',subtask)\n",
    "        \n",
    "        # Classification : \n",
    "        if cType == 'FFNN':\n",
    "            # Creating the Neural Network\n",
    "            self.model = FFNN(self.embedding, 80, 1, self.dim_vect)\n",
    "        elif cType == 'RNN':\n",
    "            \n",
    "            pass\n",
    "        \n",
    "\n",
    "    def accuracy(self, output, target ):\n",
    "\n",
    "        output = torch.round(output)\n",
    "        correct = (output == target).float()\n",
    "        acc = correct.sum()/len(correct)\n",
    "        return acc, correct.sum()\n",
    "    \n",
    "    def train( self, nb_epochs, lr=0.001, batch_size = 1000 ):\n",
    "        \n",
    "        self.train_generator = DataLoader(self.train_set, batch_size=batch_size,collate_fn=padding_tweet, shuffle=True)\n",
    "\n",
    "        optimizer = optim.RMSprop(self.model.parameters(), lr=lr)#, weight_decay=0.005)\n",
    "        \n",
    "        loss_fn = self.model.loss_fn()\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            i_batch = 0\n",
    "            epoch_correct = 0\n",
    "            self.model.train() \n",
    "            \n",
    "            for tokens, target  in self.train_generator :\n",
    "                \n",
    "                i_batch += 1\n",
    "                target = target.float()\n",
    "                tokens = tokens.long()\n",
    "                #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "                #good practice to include even if do not use here\n",
    "                self.model.train()\n",
    "\n",
    "                #we zero the gradients as they are not removed automatically\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = self.model(tokens)\n",
    "                \n",
    "                loss = loss_fn(predictions, target)\n",
    "                acc, correct = self.accuracy(predictions, target)\n",
    "                \n",
    "                #calculate the gradient of each parameter\n",
    "                loss.backward()\n",
    "                \n",
    "                #update the parameters using the gradients and optimizer algorithm \n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss = loss.item()\n",
    "\n",
    "                if i_batch % 10  !=0:\n",
    "                    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {acc*100:.2f}%')\n",
    "            \n",
    "            self.validation(batch_size = 50)\n",
    "            \n",
    "    def validation(self, batch_size = 1000):\n",
    "        all_correct = 0\n",
    "        self.validation_generator = DataLoader(self.valid_set, batch_size=batch_size, collate_fn=padding_tweet, shuffle=True)\n",
    "        loss_fn = self.model.loss_fn()\n",
    "        nb_valid = len(taskAclassifier.valid_set)\n",
    "        self.model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad(): \n",
    "            for tokens, target  in self.validation_generator :\n",
    "\n",
    "                predictions = self.model(tokens)   \n",
    "                loss = loss_fn(predictions, target)\n",
    "                acc, correct = self.accuracy(predictions, target)\n",
    "                all_correct += correct\n",
    "        print(f'|+| Validation Accuracy : {100*all_correct/nb_valid:.2f} %')\n",
    "            \n",
    "            \n",
    "\n",
    "    def test(self):\n",
    "        ''' \n",
    "            Test Function : Tests the Network on the Test Data of the Subtask and Saves in a file\n",
    "        '''\n",
    "        self.test_generator = DataLoader(self.test_set,collate_fn= padding_tweet )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n",
      "-- Data Handling : \n",
      "---- Load the Clean Adapted Dataset : Done\n",
      "---- Augmenting the Data : \n",
      "Before Augmentation :  {'NOT': 7072, 'OFF': 3520}\n",
      "After Augmentation :  {'NOT': 7072, 'OFF': 7072}\n",
      "------ Creating FFNN : Done\n"
     ]
    }
   ],
   "source": [
    "taskAclassifier = OffensiveClassifier(subtask='subtask_a', dim_vect=200, cType='FFNN',pValid = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 00 | Train Loss: 0.699 | Train Acc: 48.00%\n",
      "| Epoch: 00 | Train Loss: 0.720 | Train Acc: 53.20%\n",
      "| Epoch: 00 | Train Loss: 0.665 | Train Acc: 59.20%\n",
      "| Epoch: 00 | Train Loss: 0.646 | Train Acc: 61.20%\n",
      "| Epoch: 00 | Train Loss: 0.644 | Train Acc: 63.40%\n",
      "| Epoch: 00 | Train Loss: 0.635 | Train Acc: 64.40%\n",
      "| Epoch: 00 | Train Loss: 0.618 | Train Acc: 67.20%\n",
      "| Epoch: 00 | Train Loss: 0.632 | Train Acc: 64.20%\n",
      "| Epoch: 00 | Train Loss: 0.628 | Train Acc: 65.20%\n",
      "| Epoch: 00 | Train Loss: 0.609 | Train Acc: 70.20%\n",
      "| Epoch: 00 | Train Loss: 0.593 | Train Acc: 68.00%\n",
      "| Epoch: 00 | Train Loss: 0.604 | Train Acc: 63.00%\n",
      "| Epoch: 00 | Train Loss: 0.603 | Train Acc: 66.80%\n",
      "| Epoch: 00 | Train Loss: 0.615 | Train Acc: 63.20%\n",
      "| Epoch: 00 | Train Loss: 0.613 | Train Acc: 65.20%\n",
      "| Epoch: 00 | Train Loss: 0.588 | Train Acc: 70.40%\n",
      "| Epoch: 00 | Train Loss: 0.579 | Train Acc: 69.40%\n",
      "| Epoch: 00 | Train Loss: 0.598 | Train Acc: 65.60%\n",
      "| Epoch: 00 | Train Loss: 0.584 | Train Acc: 71.00%\n",
      "| Epoch: 00 | Train Loss: 0.574 | Train Acc: 71.00%\n",
      "| Epoch: 00 | Train Loss: 0.572 | Train Acc: 70.80%\n",
      "| Epoch: 00 | Train Loss: 0.575 | Train Acc: 69.00%\n",
      "| Epoch: 00 | Train Loss: 0.589 | Train Acc: 66.60%\n",
      "| Epoch: 00 | Train Loss: 0.563 | Train Acc: 71.20%\n",
      "| Epoch: 00 | Train Loss: 0.557 | Train Acc: 74.00%\n",
      "| Epoch: 00 | Train Loss: 0.563 | Train Acc: 71.00%\n",
      "| Epoch: 00 | Train Loss: 0.610 | Train Acc: 65.28%\n",
      "|+| Validation Accuracy : 84.43 %\n",
      "| Epoch: 01 | Train Loss: 0.571 | Train Acc: 71.40%\n",
      "| Epoch: 01 | Train Loss: 0.564 | Train Acc: 70.40%\n",
      "| Epoch: 01 | Train Loss: 0.561 | Train Acc: 71.80%\n",
      "| Epoch: 01 | Train Loss: 0.570 | Train Acc: 69.40%\n",
      "| Epoch: 01 | Train Loss: 0.553 | Train Acc: 71.20%\n",
      "| Epoch: 01 | Train Loss: 0.600 | Train Acc: 63.20%\n",
      "| Epoch: 01 | Train Loss: 0.586 | Train Acc: 70.80%\n",
      "| Epoch: 01 | Train Loss: 0.557 | Train Acc: 72.60%\n",
      "| Epoch: 01 | Train Loss: 0.534 | Train Acc: 73.20%\n",
      "| Epoch: 01 | Train Loss: 0.567 | Train Acc: 70.60%\n",
      "| Epoch: 01 | Train Loss: 0.552 | Train Acc: 71.00%\n",
      "| Epoch: 01 | Train Loss: 0.593 | Train Acc: 67.60%\n",
      "| Epoch: 01 | Train Loss: 0.552 | Train Acc: 73.40%\n",
      "| Epoch: 01 | Train Loss: 0.560 | Train Acc: 71.40%\n",
      "| Epoch: 01 | Train Loss: 0.561 | Train Acc: 71.80%\n",
      "| Epoch: 01 | Train Loss: 0.590 | Train Acc: 67.40%\n",
      "| Epoch: 01 | Train Loss: 0.544 | Train Acc: 72.00%\n",
      "| Epoch: 01 | Train Loss: 0.551 | Train Acc: 71.40%\n",
      "| Epoch: 01 | Train Loss: 0.535 | Train Acc: 72.20%\n",
      "| Epoch: 01 | Train Loss: 0.559 | Train Acc: 70.00%\n",
      "| Epoch: 01 | Train Loss: 0.530 | Train Acc: 72.60%\n",
      "| Epoch: 01 | Train Loss: 0.540 | Train Acc: 70.20%\n",
      "| Epoch: 01 | Train Loss: 0.558 | Train Acc: 70.80%\n",
      "| Epoch: 01 | Train Loss: 0.538 | Train Acc: 72.40%\n",
      "| Epoch: 01 | Train Loss: 0.547 | Train Acc: 70.80%\n",
      "| Epoch: 01 | Train Loss: 0.562 | Train Acc: 69.60%\n",
      "| Epoch: 01 | Train Loss: 0.502 | Train Acc: 76.39%\n",
      "|+| Validation Accuracy : 83.98 %\n",
      "| Epoch: 02 | Train Loss: 0.566 | Train Acc: 70.00%\n",
      "| Epoch: 02 | Train Loss: 0.538 | Train Acc: 74.40%\n",
      "| Epoch: 02 | Train Loss: 0.559 | Train Acc: 71.60%\n",
      "| Epoch: 02 | Train Loss: 0.515 | Train Acc: 74.40%\n",
      "| Epoch: 02 | Train Loss: 0.539 | Train Acc: 73.00%\n",
      "| Epoch: 02 | Train Loss: 0.586 | Train Acc: 68.80%\n",
      "| Epoch: 02 | Train Loss: 0.540 | Train Acc: 72.20%\n",
      "| Epoch: 02 | Train Loss: 0.536 | Train Acc: 73.20%\n",
      "| Epoch: 02 | Train Loss: 0.542 | Train Acc: 73.00%\n",
      "| Epoch: 02 | Train Loss: 0.531 | Train Acc: 71.00%\n",
      "| Epoch: 02 | Train Loss: 0.574 | Train Acc: 70.20%\n",
      "| Epoch: 02 | Train Loss: 0.532 | Train Acc: 74.00%\n",
      "| Epoch: 02 | Train Loss: 0.564 | Train Acc: 70.80%\n",
      "| Epoch: 02 | Train Loss: 0.530 | Train Acc: 74.80%\n",
      "| Epoch: 02 | Train Loss: 0.555 | Train Acc: 69.20%\n",
      "| Epoch: 02 | Train Loss: 0.536 | Train Acc: 74.00%\n",
      "| Epoch: 02 | Train Loss: 0.560 | Train Acc: 69.60%\n",
      "| Epoch: 02 | Train Loss: 0.537 | Train Acc: 74.80%\n",
      "| Epoch: 02 | Train Loss: 0.547 | Train Acc: 71.60%\n",
      "| Epoch: 02 | Train Loss: 0.523 | Train Acc: 74.00%\n",
      "| Epoch: 02 | Train Loss: 0.523 | Train Acc: 73.40%\n",
      "| Epoch: 02 | Train Loss: 0.535 | Train Acc: 70.00%\n",
      "| Epoch: 02 | Train Loss: 0.558 | Train Acc: 70.60%\n",
      "| Epoch: 02 | Train Loss: 0.532 | Train Acc: 73.80%\n",
      "| Epoch: 02 | Train Loss: 0.527 | Train Acc: 72.60%\n",
      "| Epoch: 02 | Train Loss: 0.519 | Train Acc: 74.40%\n",
      "| Epoch: 02 | Train Loss: 0.529 | Train Acc: 73.61%\n",
      "|+| Validation Accuracy : 80.00 %\n",
      "| Epoch: 03 | Train Loss: 0.541 | Train Acc: 73.00%\n",
      "| Epoch: 03 | Train Loss: 0.543 | Train Acc: 73.00%\n",
      "| Epoch: 03 | Train Loss: 0.523 | Train Acc: 72.60%\n",
      "| Epoch: 03 | Train Loss: 0.530 | Train Acc: 72.00%\n",
      "| Epoch: 03 | Train Loss: 0.524 | Train Acc: 72.60%\n",
      "| Epoch: 03 | Train Loss: 0.545 | Train Acc: 71.00%\n",
      "| Epoch: 03 | Train Loss: 0.542 | Train Acc: 72.20%\n",
      "| Epoch: 03 | Train Loss: 0.539 | Train Acc: 72.20%\n",
      "| Epoch: 03 | Train Loss: 0.561 | Train Acc: 70.20%\n",
      "| Epoch: 03 | Train Loss: 0.540 | Train Acc: 72.20%\n",
      "| Epoch: 03 | Train Loss: 0.559 | Train Acc: 70.00%\n",
      "| Epoch: 03 | Train Loss: 0.535 | Train Acc: 72.40%\n",
      "| Epoch: 03 | Train Loss: 0.525 | Train Acc: 74.20%\n",
      "| Epoch: 03 | Train Loss: 0.532 | Train Acc: 71.00%\n",
      "| Epoch: 03 | Train Loss: 0.516 | Train Acc: 74.00%\n",
      "| Epoch: 03 | Train Loss: 0.517 | Train Acc: 75.20%\n",
      "| Epoch: 03 | Train Loss: 0.513 | Train Acc: 72.40%\n",
      "| Epoch: 03 | Train Loss: 0.554 | Train Acc: 72.00%\n",
      "| Epoch: 03 | Train Loss: 0.515 | Train Acc: 71.60%\n",
      "| Epoch: 03 | Train Loss: 0.518 | Train Acc: 75.60%\n",
      "| Epoch: 03 | Train Loss: 0.517 | Train Acc: 75.20%\n",
      "| Epoch: 03 | Train Loss: 0.519 | Train Acc: 74.60%\n",
      "| Epoch: 03 | Train Loss: 0.512 | Train Acc: 76.40%\n",
      "| Epoch: 03 | Train Loss: 0.563 | Train Acc: 71.80%\n",
      "| Epoch: 03 | Train Loss: 0.544 | Train Acc: 71.40%\n",
      "| Epoch: 03 | Train Loss: 0.554 | Train Acc: 70.20%\n",
      "| Epoch: 03 | Train Loss: 0.491 | Train Acc: 77.08%\n",
      "|+| Validation Accuracy : 80.80 %\n",
      "| Epoch: 04 | Train Loss: 0.554 | Train Acc: 69.80%\n",
      "| Epoch: 04 | Train Loss: 0.542 | Train Acc: 72.80%\n",
      "| Epoch: 04 | Train Loss: 0.540 | Train Acc: 72.80%\n",
      "| Epoch: 04 | Train Loss: 0.518 | Train Acc: 72.00%\n",
      "| Epoch: 04 | Train Loss: 0.524 | Train Acc: 73.60%\n",
      "| Epoch: 04 | Train Loss: 0.553 | Train Acc: 69.60%\n",
      "| Epoch: 04 | Train Loss: 0.533 | Train Acc: 72.20%\n",
      "| Epoch: 04 | Train Loss: 0.554 | Train Acc: 70.80%\n",
      "| Epoch: 04 | Train Loss: 0.539 | Train Acc: 72.80%\n",
      "| Epoch: 04 | Train Loss: 0.525 | Train Acc: 75.20%\n",
      "| Epoch: 04 | Train Loss: 0.515 | Train Acc: 74.80%\n",
      "| Epoch: 04 | Train Loss: 0.524 | Train Acc: 75.20%\n",
      "| Epoch: 04 | Train Loss: 0.510 | Train Acc: 73.60%\n",
      "| Epoch: 04 | Train Loss: 0.544 | Train Acc: 72.40%\n",
      "| Epoch: 04 | Train Loss: 0.518 | Train Acc: 73.00%\n",
      "| Epoch: 04 | Train Loss: 0.516 | Train Acc: 74.20%\n",
      "| Epoch: 04 | Train Loss: 0.559 | Train Acc: 72.20%\n",
      "| Epoch: 04 | Train Loss: 0.518 | Train Acc: 73.00%\n",
      "| Epoch: 04 | Train Loss: 0.526 | Train Acc: 73.00%\n",
      "| Epoch: 04 | Train Loss: 0.525 | Train Acc: 72.40%\n",
      "| Epoch: 04 | Train Loss: 0.513 | Train Acc: 73.40%\n",
      "| Epoch: 04 | Train Loss: 0.515 | Train Acc: 75.00%\n",
      "| Epoch: 04 | Train Loss: 0.508 | Train Acc: 73.80%\n",
      "| Epoch: 04 | Train Loss: 0.516 | Train Acc: 74.60%\n",
      "| Epoch: 04 | Train Loss: 0.537 | Train Acc: 73.00%\n",
      "| Epoch: 04 | Train Loss: 0.539 | Train Acc: 72.00%\n",
      "| Epoch: 04 | Train Loss: 0.528 | Train Acc: 74.31%\n",
      "|+| Validation Accuracy : 72.50 %\n",
      "| Epoch: 05 | Train Loss: 0.512 | Train Acc: 75.20%\n",
      "| Epoch: 05 | Train Loss: 0.560 | Train Acc: 70.60%\n",
      "| Epoch: 05 | Train Loss: 0.529 | Train Acc: 73.80%\n",
      "| Epoch: 05 | Train Loss: 0.527 | Train Acc: 77.20%\n",
      "| Epoch: 05 | Train Loss: 0.528 | Train Acc: 74.20%\n",
      "| Epoch: 05 | Train Loss: 0.502 | Train Acc: 77.20%\n",
      "| Epoch: 05 | Train Loss: 0.541 | Train Acc: 71.60%\n",
      "| Epoch: 05 | Train Loss: 0.518 | Train Acc: 74.00%\n",
      "| Epoch: 05 | Train Loss: 0.562 | Train Acc: 70.40%\n",
      "| Epoch: 05 | Train Loss: 0.501 | Train Acc: 74.40%\n",
      "| Epoch: 05 | Train Loss: 0.512 | Train Acc: 74.80%\n",
      "| Epoch: 05 | Train Loss: 0.509 | Train Acc: 75.40%\n",
      "| Epoch: 05 | Train Loss: 0.522 | Train Acc: 72.20%\n",
      "| Epoch: 05 | Train Loss: 0.551 | Train Acc: 72.60%\n",
      "| Epoch: 05 | Train Loss: 0.491 | Train Acc: 75.40%\n",
      "| Epoch: 05 | Train Loss: 0.545 | Train Acc: 72.40%\n",
      "| Epoch: 05 | Train Loss: 0.502 | Train Acc: 75.20%\n",
      "| Epoch: 05 | Train Loss: 0.521 | Train Acc: 72.80%\n",
      "| Epoch: 05 | Train Loss: 0.522 | Train Acc: 75.40%\n",
      "| Epoch: 05 | Train Loss: 0.521 | Train Acc: 73.20%\n",
      "| Epoch: 05 | Train Loss: 0.516 | Train Acc: 72.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 05 | Train Loss: 0.504 | Train Acc: 74.40%\n",
      "| Epoch: 05 | Train Loss: 0.543 | Train Acc: 70.80%\n",
      "| Epoch: 05 | Train Loss: 0.490 | Train Acc: 76.80%\n",
      "| Epoch: 05 | Train Loss: 0.513 | Train Acc: 71.80%\n",
      "| Epoch: 05 | Train Loss: 0.575 | Train Acc: 72.00%\n",
      "| Epoch: 05 | Train Loss: 0.524 | Train Acc: 74.31%\n",
      "|+| Validation Accuracy : 67.73 %\n",
      "| Epoch: 06 | Train Loss: 0.499 | Train Acc: 79.00%\n",
      "| Epoch: 06 | Train Loss: 0.525 | Train Acc: 72.60%\n",
      "| Epoch: 06 | Train Loss: 0.534 | Train Acc: 71.60%\n",
      "| Epoch: 06 | Train Loss: 0.519 | Train Acc: 74.80%\n",
      "| Epoch: 06 | Train Loss: 0.528 | Train Acc: 73.00%\n",
      "| Epoch: 06 | Train Loss: 0.524 | Train Acc: 76.20%\n",
      "| Epoch: 06 | Train Loss: 0.496 | Train Acc: 72.80%\n",
      "| Epoch: 06 | Train Loss: 0.526 | Train Acc: 72.60%\n",
      "| Epoch: 06 | Train Loss: 0.492 | Train Acc: 76.80%\n",
      "| Epoch: 06 | Train Loss: 0.528 | Train Acc: 73.20%\n",
      "| Epoch: 06 | Train Loss: 0.521 | Train Acc: 73.60%\n",
      "| Epoch: 06 | Train Loss: 0.538 | Train Acc: 72.40%\n",
      "| Epoch: 06 | Train Loss: 0.496 | Train Acc: 75.20%\n",
      "| Epoch: 06 | Train Loss: 0.511 | Train Acc: 73.60%\n",
      "| Epoch: 06 | Train Loss: 0.538 | Train Acc: 72.80%\n",
      "| Epoch: 06 | Train Loss: 0.545 | Train Acc: 70.80%\n",
      "| Epoch: 06 | Train Loss: 0.490 | Train Acc: 77.00%\n",
      "| Epoch: 06 | Train Loss: 0.509 | Train Acc: 76.20%\n",
      "| Epoch: 06 | Train Loss: 0.544 | Train Acc: 71.40%\n",
      "| Epoch: 06 | Train Loss: 0.527 | Train Acc: 72.20%\n",
      "| Epoch: 06 | Train Loss: 0.503 | Train Acc: 76.60%\n",
      "| Epoch: 06 | Train Loss: 0.509 | Train Acc: 76.20%\n",
      "| Epoch: 06 | Train Loss: 0.552 | Train Acc: 70.20%\n",
      "| Epoch: 06 | Train Loss: 0.546 | Train Acc: 72.60%\n",
      "| Epoch: 06 | Train Loss: 0.516 | Train Acc: 74.40%\n",
      "| Epoch: 06 | Train Loss: 0.520 | Train Acc: 73.60%\n",
      "| Epoch: 06 | Train Loss: 0.484 | Train Acc: 76.39%\n",
      "|+| Validation Accuracy : 67.73 %\n",
      "| Epoch: 07 | Train Loss: 0.511 | Train Acc: 74.80%\n",
      "| Epoch: 07 | Train Loss: 0.503 | Train Acc: 73.60%\n",
      "| Epoch: 07 | Train Loss: 0.517 | Train Acc: 75.00%\n",
      "| Epoch: 07 | Train Loss: 0.505 | Train Acc: 76.00%\n",
      "| Epoch: 07 | Train Loss: 0.491 | Train Acc: 75.80%\n",
      "| Epoch: 07 | Train Loss: 0.487 | Train Acc: 79.00%\n",
      "| Epoch: 07 | Train Loss: 0.504 | Train Acc: 74.40%\n",
      "| Epoch: 07 | Train Loss: 0.547 | Train Acc: 71.20%\n",
      "| Epoch: 07 | Train Loss: 0.500 | Train Acc: 73.20%\n",
      "| Epoch: 07 | Train Loss: 0.504 | Train Acc: 76.20%\n",
      "| Epoch: 07 | Train Loss: 0.508 | Train Acc: 75.20%\n",
      "| Epoch: 07 | Train Loss: 0.491 | Train Acc: 76.60%\n",
      "| Epoch: 07 | Train Loss: 0.502 | Train Acc: 75.80%\n",
      "| Epoch: 07 | Train Loss: 0.504 | Train Acc: 72.60%\n",
      "| Epoch: 07 | Train Loss: 0.535 | Train Acc: 71.80%\n",
      "| Epoch: 07 | Train Loss: 0.519 | Train Acc: 73.20%\n",
      "| Epoch: 07 | Train Loss: 0.560 | Train Acc: 69.60%\n",
      "| Epoch: 07 | Train Loss: 0.505 | Train Acc: 74.40%\n",
      "| Epoch: 07 | Train Loss: 0.552 | Train Acc: 72.40%\n",
      "| Epoch: 07 | Train Loss: 0.524 | Train Acc: 73.00%\n",
      "| Epoch: 07 | Train Loss: 0.518 | Train Acc: 73.80%\n",
      "| Epoch: 07 | Train Loss: 0.516 | Train Acc: 72.00%\n",
      "| Epoch: 07 | Train Loss: 0.537 | Train Acc: 73.20%\n",
      "| Epoch: 07 | Train Loss: 0.506 | Train Acc: 75.00%\n",
      "| Epoch: 07 | Train Loss: 0.502 | Train Acc: 74.20%\n",
      "| Epoch: 07 | Train Loss: 0.565 | Train Acc: 72.00%\n",
      "| Epoch: 07 | Train Loss: 0.498 | Train Acc: 74.31%\n",
      "|+| Validation Accuracy : 73.41 %\n",
      "| Epoch: 08 | Train Loss: 0.500 | Train Acc: 74.60%\n",
      "| Epoch: 08 | Train Loss: 0.491 | Train Acc: 74.80%\n",
      "| Epoch: 08 | Train Loss: 0.552 | Train Acc: 70.80%\n",
      "| Epoch: 08 | Train Loss: 0.536 | Train Acc: 73.40%\n",
      "| Epoch: 08 | Train Loss: 0.553 | Train Acc: 72.60%\n",
      "| Epoch: 08 | Train Loss: 0.541 | Train Acc: 71.80%\n",
      "| Epoch: 08 | Train Loss: 0.515 | Train Acc: 76.60%\n",
      "| Epoch: 08 | Train Loss: 0.520 | Train Acc: 76.80%\n",
      "| Epoch: 08 | Train Loss: 0.528 | Train Acc: 72.40%\n",
      "| Epoch: 08 | Train Loss: 0.539 | Train Acc: 72.20%\n",
      "| Epoch: 08 | Train Loss: 0.525 | Train Acc: 75.00%\n",
      "| Epoch: 08 | Train Loss: 0.559 | Train Acc: 72.00%\n",
      "| Epoch: 08 | Train Loss: 0.507 | Train Acc: 76.00%\n",
      "| Epoch: 08 | Train Loss: 0.504 | Train Acc: 74.60%\n",
      "| Epoch: 08 | Train Loss: 0.503 | Train Acc: 76.00%\n",
      "| Epoch: 08 | Train Loss: 0.539 | Train Acc: 70.20%\n",
      "| Epoch: 08 | Train Loss: 0.512 | Train Acc: 73.80%\n",
      "| Epoch: 08 | Train Loss: 0.512 | Train Acc: 74.40%\n",
      "| Epoch: 08 | Train Loss: 0.503 | Train Acc: 75.20%\n",
      "| Epoch: 08 | Train Loss: 0.494 | Train Acc: 71.80%\n",
      "| Epoch: 08 | Train Loss: 0.501 | Train Acc: 75.00%\n",
      "| Epoch: 08 | Train Loss: 0.529 | Train Acc: 71.80%\n",
      "| Epoch: 08 | Train Loss: 0.503 | Train Acc: 74.40%\n",
      "| Epoch: 08 | Train Loss: 0.508 | Train Acc: 73.60%\n",
      "| Epoch: 08 | Train Loss: 0.512 | Train Acc: 74.00%\n",
      "| Epoch: 08 | Train Loss: 0.468 | Train Acc: 78.00%\n",
      "| Epoch: 08 | Train Loss: 0.468 | Train Acc: 77.78%\n",
      "|+| Validation Accuracy : 64.09 %\n",
      "| Epoch: 09 | Train Loss: 0.457 | Train Acc: 78.40%\n",
      "| Epoch: 09 | Train Loss: 0.525 | Train Acc: 74.40%\n",
      "| Epoch: 09 | Train Loss: 0.508 | Train Acc: 75.60%\n",
      "| Epoch: 09 | Train Loss: 0.498 | Train Acc: 75.20%\n",
      "| Epoch: 09 | Train Loss: 0.553 | Train Acc: 73.20%\n",
      "| Epoch: 09 | Train Loss: 0.529 | Train Acc: 74.40%\n",
      "| Epoch: 09 | Train Loss: 0.512 | Train Acc: 75.20%\n",
      "| Epoch: 09 | Train Loss: 0.503 | Train Acc: 74.00%\n",
      "| Epoch: 09 | Train Loss: 0.523 | Train Acc: 71.80%\n",
      "| Epoch: 09 | Train Loss: 0.522 | Train Acc: 72.40%\n",
      "| Epoch: 09 | Train Loss: 0.512 | Train Acc: 73.80%\n",
      "| Epoch: 09 | Train Loss: 0.514 | Train Acc: 73.80%\n",
      "| Epoch: 09 | Train Loss: 0.508 | Train Acc: 75.40%\n",
      "| Epoch: 09 | Train Loss: 0.507 | Train Acc: 74.40%\n",
      "| Epoch: 09 | Train Loss: 0.499 | Train Acc: 75.20%\n",
      "| Epoch: 09 | Train Loss: 0.522 | Train Acc: 73.80%\n",
      "| Epoch: 09 | Train Loss: 0.500 | Train Acc: 76.40%\n",
      "| Epoch: 09 | Train Loss: 0.543 | Train Acc: 71.00%\n",
      "| Epoch: 09 | Train Loss: 0.504 | Train Acc: 75.80%\n",
      "| Epoch: 09 | Train Loss: 0.535 | Train Acc: 72.40%\n",
      "| Epoch: 09 | Train Loss: 0.518 | Train Acc: 75.00%\n",
      "| Epoch: 09 | Train Loss: 0.540 | Train Acc: 74.00%\n",
      "| Epoch: 09 | Train Loss: 0.524 | Train Acc: 72.40%\n",
      "| Epoch: 09 | Train Loss: 0.482 | Train Acc: 77.40%\n",
      "| Epoch: 09 | Train Loss: 0.497 | Train Acc: 75.20%\n",
      "| Epoch: 09 | Train Loss: 0.507 | Train Acc: 75.40%\n",
      "| Epoch: 09 | Train Loss: 0.492 | Train Acc: 72.22%\n",
      "|+| Validation Accuracy : 64.55 %\n",
      "| Epoch: 10 | Train Loss: 0.561 | Train Acc: 72.00%\n",
      "| Epoch: 10 | Train Loss: 0.512 | Train Acc: 73.40%\n",
      "| Epoch: 10 | Train Loss: 0.487 | Train Acc: 74.80%\n",
      "| Epoch: 10 | Train Loss: 0.484 | Train Acc: 76.80%\n",
      "| Epoch: 10 | Train Loss: 0.500 | Train Acc: 76.80%\n",
      "| Epoch: 10 | Train Loss: 0.522 | Train Acc: 71.00%\n",
      "| Epoch: 10 | Train Loss: 0.503 | Train Acc: 77.00%\n",
      "| Epoch: 10 | Train Loss: 0.533 | Train Acc: 73.00%\n",
      "| Epoch: 10 | Train Loss: 0.500 | Train Acc: 73.40%\n",
      "| Epoch: 10 | Train Loss: 0.490 | Train Acc: 76.20%\n",
      "| Epoch: 10 | Train Loss: 0.508 | Train Acc: 75.40%\n",
      "| Epoch: 10 | Train Loss: 0.489 | Train Acc: 78.40%\n",
      "| Epoch: 10 | Train Loss: 0.496 | Train Acc: 76.40%\n",
      "| Epoch: 10 | Train Loss: 0.517 | Train Acc: 71.20%\n",
      "| Epoch: 10 | Train Loss: 0.490 | Train Acc: 75.20%\n",
      "| Epoch: 10 | Train Loss: 0.537 | Train Acc: 69.20%\n",
      "| Epoch: 10 | Train Loss: 0.531 | Train Acc: 72.40%\n",
      "| Epoch: 10 | Train Loss: 0.509 | Train Acc: 74.20%\n",
      "| Epoch: 10 | Train Loss: 0.527 | Train Acc: 76.00%\n",
      "| Epoch: 10 | Train Loss: 0.505 | Train Acc: 75.20%\n",
      "| Epoch: 10 | Train Loss: 0.489 | Train Acc: 74.60%\n",
      "| Epoch: 10 | Train Loss: 0.540 | Train Acc: 72.20%\n",
      "| Epoch: 10 | Train Loss: 0.521 | Train Acc: 74.80%\n",
      "| Epoch: 10 | Train Loss: 0.482 | Train Acc: 76.20%\n",
      "| Epoch: 10 | Train Loss: 0.513 | Train Acc: 74.80%\n",
      "| Epoch: 10 | Train Loss: 0.549 | Train Acc: 71.20%\n",
      "| Epoch: 10 | Train Loss: 0.496 | Train Acc: 73.61%\n",
      "|+| Validation Accuracy : 74.20 %\n",
      "| Epoch: 11 | Train Loss: 0.499 | Train Acc: 74.60%\n",
      "| Epoch: 11 | Train Loss: 0.499 | Train Acc: 76.00%\n",
      "| Epoch: 11 | Train Loss: 0.531 | Train Acc: 74.80%\n",
      "| Epoch: 11 | Train Loss: 0.535 | Train Acc: 72.80%\n",
      "| Epoch: 11 | Train Loss: 0.521 | Train Acc: 70.20%\n",
      "| Epoch: 11 | Train Loss: 0.506 | Train Acc: 76.80%\n",
      "| Epoch: 11 | Train Loss: 0.492 | Train Acc: 74.80%\n",
      "| Epoch: 11 | Train Loss: 0.515 | Train Acc: 72.40%\n",
      "| Epoch: 11 | Train Loss: 0.526 | Train Acc: 73.60%\n",
      "| Epoch: 11 | Train Loss: 0.476 | Train Acc: 76.00%\n",
      "| Epoch: 11 | Train Loss: 0.467 | Train Acc: 77.60%\n",
      "| Epoch: 11 | Train Loss: 0.502 | Train Acc: 76.60%\n",
      "| Epoch: 11 | Train Loss: 0.497 | Train Acc: 75.20%\n",
      "| Epoch: 11 | Train Loss: 0.522 | Train Acc: 75.00%\n",
      "| Epoch: 11 | Train Loss: 0.572 | Train Acc: 69.00%\n",
      "| Epoch: 11 | Train Loss: 0.524 | Train Acc: 70.80%\n",
      "| Epoch: 11 | Train Loss: 0.483 | Train Acc: 76.60%\n",
      "| Epoch: 11 | Train Loss: 0.508 | Train Acc: 77.00%\n",
      "| Epoch: 11 | Train Loss: 0.511 | Train Acc: 73.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 11 | Train Loss: 0.487 | Train Acc: 76.40%\n",
      "| Epoch: 11 | Train Loss: 0.550 | Train Acc: 72.20%\n",
      "| Epoch: 11 | Train Loss: 0.481 | Train Acc: 77.40%\n",
      "| Epoch: 11 | Train Loss: 0.512 | Train Acc: 74.40%\n",
      "| Epoch: 11 | Train Loss: 0.451 | Train Acc: 78.80%\n",
      "| Epoch: 11 | Train Loss: 0.534 | Train Acc: 72.40%\n",
      "| Epoch: 11 | Train Loss: 0.484 | Train Acc: 76.20%\n",
      "| Epoch: 11 | Train Loss: 0.516 | Train Acc: 73.61%\n",
      "|+| Validation Accuracy : 74.55 %\n",
      "| Epoch: 12 | Train Loss: 0.492 | Train Acc: 76.80%\n",
      "| Epoch: 12 | Train Loss: 0.503 | Train Acc: 73.40%\n",
      "| Epoch: 12 | Train Loss: 0.524 | Train Acc: 75.40%\n",
      "| Epoch: 12 | Train Loss: 0.489 | Train Acc: 76.20%\n",
      "| Epoch: 12 | Train Loss: 0.522 | Train Acc: 73.00%\n",
      "| Epoch: 12 | Train Loss: 0.471 | Train Acc: 78.40%\n",
      "| Epoch: 12 | Train Loss: 0.506 | Train Acc: 74.60%\n",
      "| Epoch: 12 | Train Loss: 0.528 | Train Acc: 74.80%\n",
      "| Epoch: 12 | Train Loss: 0.518 | Train Acc: 73.00%\n",
      "| Epoch: 12 | Train Loss: 0.523 | Train Acc: 74.20%\n",
      "| Epoch: 12 | Train Loss: 0.523 | Train Acc: 72.80%\n",
      "| Epoch: 12 | Train Loss: 0.523 | Train Acc: 73.60%\n",
      "| Epoch: 12 | Train Loss: 0.510 | Train Acc: 75.20%\n",
      "| Epoch: 12 | Train Loss: 0.486 | Train Acc: 75.20%\n",
      "| Epoch: 12 | Train Loss: 0.519 | Train Acc: 72.00%\n",
      "| Epoch: 12 | Train Loss: 0.505 | Train Acc: 72.80%\n",
      "| Epoch: 12 | Train Loss: 0.486 | Train Acc: 76.20%\n",
      "| Epoch: 12 | Train Loss: 0.527 | Train Acc: 72.80%\n",
      "| Epoch: 12 | Train Loss: 0.471 | Train Acc: 77.60%\n",
      "| Epoch: 12 | Train Loss: 0.493 | Train Acc: 76.80%\n",
      "| Epoch: 12 | Train Loss: 0.503 | Train Acc: 75.00%\n",
      "| Epoch: 12 | Train Loss: 0.486 | Train Acc: 75.40%\n",
      "| Epoch: 12 | Train Loss: 0.496 | Train Acc: 73.80%\n",
      "| Epoch: 12 | Train Loss: 0.498 | Train Acc: 75.40%\n",
      "| Epoch: 12 | Train Loss: 0.508 | Train Acc: 74.60%\n",
      "| Epoch: 12 | Train Loss: 0.518 | Train Acc: 70.40%\n",
      "| Epoch: 12 | Train Loss: 0.439 | Train Acc: 78.47%\n",
      "|+| Validation Accuracy : 75.80 %\n",
      "| Epoch: 13 | Train Loss: 0.500 | Train Acc: 75.20%\n",
      "| Epoch: 13 | Train Loss: 0.490 | Train Acc: 73.80%\n",
      "| Epoch: 13 | Train Loss: 0.503 | Train Acc: 75.20%\n",
      "| Epoch: 13 | Train Loss: 0.513 | Train Acc: 74.40%\n",
      "| Epoch: 13 | Train Loss: 0.545 | Train Acc: 71.80%\n",
      "| Epoch: 13 | Train Loss: 0.531 | Train Acc: 72.60%\n",
      "| Epoch: 13 | Train Loss: 0.525 | Train Acc: 72.20%\n",
      "| Epoch: 13 | Train Loss: 0.521 | Train Acc: 75.40%\n",
      "| Epoch: 13 | Train Loss: 0.485 | Train Acc: 74.40%\n",
      "| Epoch: 13 | Train Loss: 0.499 | Train Acc: 74.40%\n",
      "| Epoch: 13 | Train Loss: 0.519 | Train Acc: 76.00%\n",
      "| Epoch: 13 | Train Loss: 0.527 | Train Acc: 73.80%\n",
      "| Epoch: 13 | Train Loss: 0.491 | Train Acc: 74.20%\n",
      "| Epoch: 13 | Train Loss: 0.472 | Train Acc: 78.40%\n",
      "| Epoch: 13 | Train Loss: 0.504 | Train Acc: 74.20%\n",
      "| Epoch: 13 | Train Loss: 0.485 | Train Acc: 77.60%\n",
      "| Epoch: 13 | Train Loss: 0.499 | Train Acc: 75.80%\n",
      "| Epoch: 13 | Train Loss: 0.489 | Train Acc: 75.20%\n",
      "| Epoch: 13 | Train Loss: 0.495 | Train Acc: 78.60%\n",
      "| Epoch: 13 | Train Loss: 0.485 | Train Acc: 77.80%\n",
      "| Epoch: 13 | Train Loss: 0.489 | Train Acc: 74.80%\n",
      "| Epoch: 13 | Train Loss: 0.486 | Train Acc: 75.40%\n",
      "| Epoch: 13 | Train Loss: 0.519 | Train Acc: 75.60%\n",
      "| Epoch: 13 | Train Loss: 0.485 | Train Acc: 76.40%\n",
      "| Epoch: 13 | Train Loss: 0.516 | Train Acc: 72.80%\n",
      "| Epoch: 13 | Train Loss: 0.515 | Train Acc: 75.80%\n",
      "| Epoch: 13 | Train Loss: 0.545 | Train Acc: 68.75%\n",
      "|+| Validation Accuracy : 78.18 %\n",
      "| Epoch: 14 | Train Loss: 0.515 | Train Acc: 72.20%\n",
      "| Epoch: 14 | Train Loss: 0.528 | Train Acc: 72.60%\n",
      "| Epoch: 14 | Train Loss: 0.522 | Train Acc: 74.00%\n",
      "| Epoch: 14 | Train Loss: 0.495 | Train Acc: 75.00%\n",
      "| Epoch: 14 | Train Loss: 0.488 | Train Acc: 74.00%\n",
      "| Epoch: 14 | Train Loss: 0.501 | Train Acc: 73.60%\n",
      "| Epoch: 14 | Train Loss: 0.538 | Train Acc: 71.20%\n",
      "| Epoch: 14 | Train Loss: 0.479 | Train Acc: 78.00%\n",
      "| Epoch: 14 | Train Loss: 0.485 | Train Acc: 76.00%\n",
      "| Epoch: 14 | Train Loss: 0.509 | Train Acc: 73.40%\n",
      "| Epoch: 14 | Train Loss: 0.502 | Train Acc: 73.40%\n",
      "| Epoch: 14 | Train Loss: 0.495 | Train Acc: 75.00%\n",
      "| Epoch: 14 | Train Loss: 0.515 | Train Acc: 73.80%\n",
      "| Epoch: 14 | Train Loss: 0.491 | Train Acc: 78.40%\n",
      "| Epoch: 14 | Train Loss: 0.499 | Train Acc: 75.00%\n",
      "| Epoch: 14 | Train Loss: 0.495 | Train Acc: 74.00%\n",
      "| Epoch: 14 | Train Loss: 0.488 | Train Acc: 73.60%\n",
      "| Epoch: 14 | Train Loss: 0.503 | Train Acc: 75.40%\n",
      "| Epoch: 14 | Train Loss: 0.491 | Train Acc: 76.60%\n",
      "| Epoch: 14 | Train Loss: 0.508 | Train Acc: 74.60%\n",
      "| Epoch: 14 | Train Loss: 0.521 | Train Acc: 76.40%\n",
      "| Epoch: 14 | Train Loss: 0.481 | Train Acc: 76.20%\n",
      "| Epoch: 14 | Train Loss: 0.491 | Train Acc: 75.00%\n",
      "| Epoch: 14 | Train Loss: 0.474 | Train Acc: 76.40%\n",
      "| Epoch: 14 | Train Loss: 0.493 | Train Acc: 77.40%\n",
      "| Epoch: 14 | Train Loss: 0.493 | Train Acc: 74.60%\n",
      "| Epoch: 14 | Train Loss: 0.538 | Train Acc: 72.92%\n",
      "|+| Validation Accuracy : 77.39 %\n",
      "| Epoch: 15 | Train Loss: 0.507 | Train Acc: 74.00%\n",
      "| Epoch: 15 | Train Loss: 0.489 | Train Acc: 75.80%\n",
      "| Epoch: 15 | Train Loss: 0.524 | Train Acc: 73.80%\n",
      "| Epoch: 15 | Train Loss: 0.483 | Train Acc: 76.80%\n",
      "| Epoch: 15 | Train Loss: 0.482 | Train Acc: 77.60%\n",
      "| Epoch: 15 | Train Loss: 0.531 | Train Acc: 71.20%\n",
      "| Epoch: 15 | Train Loss: 0.456 | Train Acc: 79.60%\n",
      "| Epoch: 15 | Train Loss: 0.482 | Train Acc: 76.80%\n",
      "| Epoch: 15 | Train Loss: 0.536 | Train Acc: 73.60%\n",
      "| Epoch: 15 | Train Loss: 0.494 | Train Acc: 76.00%\n",
      "| Epoch: 15 | Train Loss: 0.492 | Train Acc: 76.20%\n",
      "| Epoch: 15 | Train Loss: 0.457 | Train Acc: 77.20%\n",
      "| Epoch: 15 | Train Loss: 0.502 | Train Acc: 76.60%\n",
      "| Epoch: 15 | Train Loss: 0.461 | Train Acc: 76.40%\n",
      "| Epoch: 15 | Train Loss: 0.536 | Train Acc: 73.80%\n",
      "| Epoch: 15 | Train Loss: 0.477 | Train Acc: 75.20%\n",
      "| Epoch: 15 | Train Loss: 0.543 | Train Acc: 71.20%\n",
      "| Epoch: 15 | Train Loss: 0.518 | Train Acc: 73.40%\n",
      "| Epoch: 15 | Train Loss: 0.493 | Train Acc: 77.00%\n",
      "| Epoch: 15 | Train Loss: 0.458 | Train Acc: 76.60%\n",
      "| Epoch: 15 | Train Loss: 0.483 | Train Acc: 79.20%\n",
      "| Epoch: 15 | Train Loss: 0.519 | Train Acc: 74.80%\n",
      "| Epoch: 15 | Train Loss: 0.496 | Train Acc: 75.00%\n",
      "| Epoch: 15 | Train Loss: 0.517 | Train Acc: 75.20%\n",
      "| Epoch: 15 | Train Loss: 0.512 | Train Acc: 73.80%\n",
      "| Epoch: 15 | Train Loss: 0.530 | Train Acc: 70.80%\n",
      "| Epoch: 15 | Train Loss: 0.513 | Train Acc: 74.31%\n",
      "|+| Validation Accuracy : 68.64 %\n",
      "| Epoch: 16 | Train Loss: 0.522 | Train Acc: 72.20%\n",
      "| Epoch: 16 | Train Loss: 0.492 | Train Acc: 74.40%\n",
      "| Epoch: 16 | Train Loss: 0.487 | Train Acc: 74.20%\n",
      "| Epoch: 16 | Train Loss: 0.504 | Train Acc: 76.00%\n",
      "| Epoch: 16 | Train Loss: 0.477 | Train Acc: 78.00%\n",
      "| Epoch: 16 | Train Loss: 0.487 | Train Acc: 75.60%\n",
      "| Epoch: 16 | Train Loss: 0.524 | Train Acc: 73.80%\n",
      "| Epoch: 16 | Train Loss: 0.508 | Train Acc: 73.40%\n",
      "| Epoch: 16 | Train Loss: 0.464 | Train Acc: 78.20%\n",
      "| Epoch: 16 | Train Loss: 0.492 | Train Acc: 76.60%\n",
      "| Epoch: 16 | Train Loss: 0.466 | Train Acc: 76.40%\n",
      "| Epoch: 16 | Train Loss: 0.497 | Train Acc: 74.60%\n",
      "| Epoch: 16 | Train Loss: 0.475 | Train Acc: 77.20%\n",
      "| Epoch: 16 | Train Loss: 0.478 | Train Acc: 75.20%\n",
      "| Epoch: 16 | Train Loss: 0.485 | Train Acc: 76.40%\n",
      "| Epoch: 16 | Train Loss: 0.498 | Train Acc: 73.20%\n",
      "| Epoch: 16 | Train Loss: 0.472 | Train Acc: 77.40%\n",
      "| Epoch: 16 | Train Loss: 0.510 | Train Acc: 76.60%\n",
      "| Epoch: 16 | Train Loss: 0.496 | Train Acc: 74.40%\n",
      "| Epoch: 16 | Train Loss: 0.487 | Train Acc: 73.60%\n",
      "| Epoch: 16 | Train Loss: 0.503 | Train Acc: 75.20%\n",
      "| Epoch: 16 | Train Loss: 0.535 | Train Acc: 72.20%\n",
      "| Epoch: 16 | Train Loss: 0.528 | Train Acc: 72.00%\n",
      "| Epoch: 16 | Train Loss: 0.492 | Train Acc: 77.20%\n",
      "| Epoch: 16 | Train Loss: 0.467 | Train Acc: 78.40%\n",
      "| Epoch: 16 | Train Loss: 0.529 | Train Acc: 73.20%\n",
      "| Epoch: 16 | Train Loss: 0.487 | Train Acc: 75.00%\n",
      "|+| Validation Accuracy : 76.82 %\n",
      "| Epoch: 17 | Train Loss: 0.469 | Train Acc: 75.20%\n",
      "| Epoch: 17 | Train Loss: 0.510 | Train Acc: 74.00%\n",
      "| Epoch: 17 | Train Loss: 0.468 | Train Acc: 77.00%\n",
      "| Epoch: 17 | Train Loss: 0.474 | Train Acc: 75.60%\n",
      "| Epoch: 17 | Train Loss: 0.496 | Train Acc: 74.60%\n",
      "| Epoch: 17 | Train Loss: 0.521 | Train Acc: 73.60%\n",
      "| Epoch: 17 | Train Loss: 0.527 | Train Acc: 70.60%\n",
      "| Epoch: 17 | Train Loss: 0.514 | Train Acc: 72.60%\n",
      "| Epoch: 17 | Train Loss: 0.493 | Train Acc: 73.20%\n",
      "| Epoch: 17 | Train Loss: 0.477 | Train Acc: 76.60%\n",
      "| Epoch: 17 | Train Loss: 0.501 | Train Acc: 75.00%\n",
      "| Epoch: 17 | Train Loss: 0.464 | Train Acc: 80.20%\n",
      "| Epoch: 17 | Train Loss: 0.495 | Train Acc: 72.60%\n",
      "| Epoch: 17 | Train Loss: 0.462 | Train Acc: 77.60%\n",
      "| Epoch: 17 | Train Loss: 0.497 | Train Acc: 76.20%\n",
      "| Epoch: 17 | Train Loss: 0.484 | Train Acc: 76.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 17 | Train Loss: 0.481 | Train Acc: 74.20%\n",
      "| Epoch: 17 | Train Loss: 0.481 | Train Acc: 75.80%\n",
      "| Epoch: 17 | Train Loss: 0.527 | Train Acc: 74.00%\n",
      "| Epoch: 17 | Train Loss: 0.504 | Train Acc: 75.20%\n",
      "| Epoch: 17 | Train Loss: 0.504 | Train Acc: 75.80%\n",
      "| Epoch: 17 | Train Loss: 0.560 | Train Acc: 71.20%\n",
      "| Epoch: 17 | Train Loss: 0.478 | Train Acc: 77.80%\n",
      "| Epoch: 17 | Train Loss: 0.497 | Train Acc: 77.20%\n",
      "| Epoch: 17 | Train Loss: 0.476 | Train Acc: 75.60%\n",
      "| Epoch: 17 | Train Loss: 0.490 | Train Acc: 76.00%\n",
      "| Epoch: 17 | Train Loss: 0.492 | Train Acc: 72.92%\n",
      "|+| Validation Accuracy : 67.39 %\n",
      "| Epoch: 18 | Train Loss: 0.495 | Train Acc: 74.40%\n",
      "| Epoch: 18 | Train Loss: 0.488 | Train Acc: 76.80%\n",
      "| Epoch: 18 | Train Loss: 0.528 | Train Acc: 72.20%\n",
      "| Epoch: 18 | Train Loss: 0.474 | Train Acc: 74.20%\n",
      "| Epoch: 18 | Train Loss: 0.481 | Train Acc: 77.20%\n",
      "| Epoch: 18 | Train Loss: 0.462 | Train Acc: 78.20%\n",
      "| Epoch: 18 | Train Loss: 0.489 | Train Acc: 75.60%\n",
      "| Epoch: 18 | Train Loss: 0.507 | Train Acc: 74.40%\n",
      "| Epoch: 18 | Train Loss: 0.471 | Train Acc: 76.40%\n",
      "| Epoch: 18 | Train Loss: 0.499 | Train Acc: 74.20%\n",
      "| Epoch: 18 | Train Loss: 0.507 | Train Acc: 73.40%\n",
      "| Epoch: 18 | Train Loss: 0.511 | Train Acc: 76.00%\n",
      "| Epoch: 18 | Train Loss: 0.498 | Train Acc: 74.60%\n",
      "| Epoch: 18 | Train Loss: 0.511 | Train Acc: 73.80%\n",
      "| Epoch: 18 | Train Loss: 0.475 | Train Acc: 76.20%\n",
      "| Epoch: 18 | Train Loss: 0.481 | Train Acc: 78.60%\n",
      "| Epoch: 18 | Train Loss: 0.481 | Train Acc: 77.60%\n",
      "| Epoch: 18 | Train Loss: 0.487 | Train Acc: 76.20%\n",
      "| Epoch: 18 | Train Loss: 0.503 | Train Acc: 73.60%\n",
      "| Epoch: 18 | Train Loss: 0.480 | Train Acc: 77.20%\n",
      "| Epoch: 18 | Train Loss: 0.497 | Train Acc: 75.60%\n",
      "| Epoch: 18 | Train Loss: 0.468 | Train Acc: 76.40%\n",
      "| Epoch: 18 | Train Loss: 0.493 | Train Acc: 76.40%\n",
      "| Epoch: 18 | Train Loss: 0.478 | Train Acc: 76.20%\n",
      "| Epoch: 18 | Train Loss: 0.493 | Train Acc: 77.60%\n",
      "| Epoch: 18 | Train Loss: 0.499 | Train Acc: 73.80%\n",
      "| Epoch: 18 | Train Loss: 0.526 | Train Acc: 72.22%\n",
      "|+| Validation Accuracy : 78.41 %\n",
      "| Epoch: 19 | Train Loss: 0.452 | Train Acc: 79.80%\n",
      "| Epoch: 19 | Train Loss: 0.525 | Train Acc: 72.60%\n",
      "| Epoch: 19 | Train Loss: 0.483 | Train Acc: 76.60%\n",
      "| Epoch: 19 | Train Loss: 0.497 | Train Acc: 75.80%\n",
      "| Epoch: 19 | Train Loss: 0.462 | Train Acc: 75.80%\n",
      "| Epoch: 19 | Train Loss: 0.451 | Train Acc: 79.80%\n",
      "| Epoch: 19 | Train Loss: 0.459 | Train Acc: 77.80%\n",
      "| Epoch: 19 | Train Loss: 0.547 | Train Acc: 70.40%\n",
      "| Epoch: 19 | Train Loss: 0.451 | Train Acc: 77.00%\n",
      "| Epoch: 19 | Train Loss: 0.505 | Train Acc: 76.80%\n",
      "| Epoch: 19 | Train Loss: 0.502 | Train Acc: 76.00%\n",
      "| Epoch: 19 | Train Loss: 0.478 | Train Acc: 76.00%\n",
      "| Epoch: 19 | Train Loss: 0.471 | Train Acc: 77.40%\n",
      "| Epoch: 19 | Train Loss: 0.507 | Train Acc: 75.20%\n",
      "| Epoch: 19 | Train Loss: 0.504 | Train Acc: 75.40%\n",
      "| Epoch: 19 | Train Loss: 0.505 | Train Acc: 74.00%\n",
      "| Epoch: 19 | Train Loss: 0.477 | Train Acc: 74.40%\n",
      "| Epoch: 19 | Train Loss: 0.511 | Train Acc: 73.00%\n",
      "| Epoch: 19 | Train Loss: 0.495 | Train Acc: 74.20%\n",
      "| Epoch: 19 | Train Loss: 0.516 | Train Acc: 71.80%\n",
      "| Epoch: 19 | Train Loss: 0.466 | Train Acc: 79.40%\n",
      "| Epoch: 19 | Train Loss: 0.511 | Train Acc: 72.80%\n",
      "| Epoch: 19 | Train Loss: 0.486 | Train Acc: 75.20%\n",
      "| Epoch: 19 | Train Loss: 0.486 | Train Acc: 78.00%\n",
      "| Epoch: 19 | Train Loss: 0.500 | Train Acc: 75.80%\n",
      "| Epoch: 19 | Train Loss: 0.493 | Train Acc: 72.00%\n",
      "| Epoch: 19 | Train Loss: 0.490 | Train Acc: 75.00%\n",
      "|+| Validation Accuracy : 73.86 %\n",
      "| Epoch: 20 | Train Loss: 0.505 | Train Acc: 73.80%\n",
      "| Epoch: 20 | Train Loss: 0.482 | Train Acc: 74.80%\n",
      "| Epoch: 20 | Train Loss: 0.505 | Train Acc: 73.40%\n",
      "| Epoch: 20 | Train Loss: 0.488 | Train Acc: 75.60%\n",
      "| Epoch: 20 | Train Loss: 0.476 | Train Acc: 76.00%\n",
      "| Epoch: 20 | Train Loss: 0.450 | Train Acc: 78.20%\n",
      "| Epoch: 20 | Train Loss: 0.472 | Train Acc: 77.20%\n",
      "| Epoch: 20 | Train Loss: 0.496 | Train Acc: 76.00%\n",
      "| Epoch: 20 | Train Loss: 0.493 | Train Acc: 76.20%\n",
      "| Epoch: 20 | Train Loss: 0.495 | Train Acc: 75.20%\n",
      "| Epoch: 20 | Train Loss: 0.484 | Train Acc: 76.00%\n",
      "| Epoch: 20 | Train Loss: 0.507 | Train Acc: 72.60%\n",
      "| Epoch: 20 | Train Loss: 0.458 | Train Acc: 78.40%\n",
      "| Epoch: 20 | Train Loss: 0.503 | Train Acc: 75.80%\n",
      "| Epoch: 20 | Train Loss: 0.456 | Train Acc: 79.80%\n",
      "| Epoch: 20 | Train Loss: 0.487 | Train Acc: 74.80%\n",
      "| Epoch: 20 | Train Loss: 0.469 | Train Acc: 75.20%\n",
      "| Epoch: 20 | Train Loss: 0.501 | Train Acc: 77.40%\n",
      "| Epoch: 20 | Train Loss: 0.484 | Train Acc: 73.80%\n",
      "| Epoch: 20 | Train Loss: 0.480 | Train Acc: 75.80%\n",
      "| Epoch: 20 | Train Loss: 0.476 | Train Acc: 75.40%\n",
      "| Epoch: 20 | Train Loss: 0.492 | Train Acc: 75.40%\n",
      "| Epoch: 20 | Train Loss: 0.474 | Train Acc: 78.60%\n",
      "| Epoch: 20 | Train Loss: 0.507 | Train Acc: 75.80%\n",
      "| Epoch: 20 | Train Loss: 0.521 | Train Acc: 73.20%\n",
      "| Epoch: 20 | Train Loss: 0.500 | Train Acc: 76.60%\n",
      "| Epoch: 20 | Train Loss: 0.458 | Train Acc: 79.86%\n",
      "|+| Validation Accuracy : 67.05 %\n",
      "| Epoch: 21 | Train Loss: 0.492 | Train Acc: 74.00%\n",
      "| Epoch: 21 | Train Loss: 0.487 | Train Acc: 76.80%\n",
      "| Epoch: 21 | Train Loss: 0.454 | Train Acc: 76.60%\n",
      "| Epoch: 21 | Train Loss: 0.465 | Train Acc: 76.20%\n",
      "| Epoch: 21 | Train Loss: 0.499 | Train Acc: 74.40%\n",
      "| Epoch: 21 | Train Loss: 0.480 | Train Acc: 77.00%\n",
      "| Epoch: 21 | Train Loss: 0.466 | Train Acc: 76.80%\n",
      "| Epoch: 21 | Train Loss: 0.471 | Train Acc: 77.60%\n",
      "| Epoch: 21 | Train Loss: 0.507 | Train Acc: 74.40%\n",
      "| Epoch: 21 | Train Loss: 0.478 | Train Acc: 75.80%\n",
      "| Epoch: 21 | Train Loss: 0.505 | Train Acc: 73.60%\n",
      "| Epoch: 21 | Train Loss: 0.500 | Train Acc: 75.40%\n",
      "| Epoch: 21 | Train Loss: 0.516 | Train Acc: 75.00%\n",
      "| Epoch: 21 | Train Loss: 0.484 | Train Acc: 73.60%\n",
      "| Epoch: 21 | Train Loss: 0.469 | Train Acc: 76.80%\n",
      "| Epoch: 21 | Train Loss: 0.485 | Train Acc: 74.80%\n",
      "| Epoch: 21 | Train Loss: 0.465 | Train Acc: 78.40%\n",
      "| Epoch: 21 | Train Loss: 0.526 | Train Acc: 71.40%\n",
      "| Epoch: 21 | Train Loss: 0.476 | Train Acc: 77.60%\n",
      "| Epoch: 21 | Train Loss: 0.490 | Train Acc: 75.60%\n",
      "| Epoch: 21 | Train Loss: 0.500 | Train Acc: 74.00%\n",
      "| Epoch: 21 | Train Loss: 0.472 | Train Acc: 75.60%\n",
      "| Epoch: 21 | Train Loss: 0.482 | Train Acc: 76.00%\n",
      "| Epoch: 21 | Train Loss: 0.504 | Train Acc: 73.80%\n",
      "| Epoch: 21 | Train Loss: 0.493 | Train Acc: 76.00%\n",
      "| Epoch: 21 | Train Loss: 0.449 | Train Acc: 80.60%\n",
      "| Epoch: 21 | Train Loss: 0.458 | Train Acc: 78.47%\n",
      "|+| Validation Accuracy : 80.00 %\n",
      "| Epoch: 22 | Train Loss: 0.530 | Train Acc: 71.60%\n",
      "| Epoch: 22 | Train Loss: 0.511 | Train Acc: 72.00%\n",
      "| Epoch: 22 | Train Loss: 0.468 | Train Acc: 76.60%\n",
      "| Epoch: 22 | Train Loss: 0.468 | Train Acc: 77.40%\n",
      "| Epoch: 22 | Train Loss: 0.463 | Train Acc: 76.20%\n",
      "| Epoch: 22 | Train Loss: 0.523 | Train Acc: 74.40%\n",
      "| Epoch: 22 | Train Loss: 0.466 | Train Acc: 76.20%\n",
      "| Epoch: 22 | Train Loss: 0.509 | Train Acc: 76.00%\n",
      "| Epoch: 22 | Train Loss: 0.497 | Train Acc: 75.20%\n",
      "| Epoch: 22 | Train Loss: 0.513 | Train Acc: 73.40%\n",
      "| Epoch: 22 | Train Loss: 0.465 | Train Acc: 77.40%\n",
      "| Epoch: 22 | Train Loss: 0.451 | Train Acc: 78.80%\n",
      "| Epoch: 22 | Train Loss: 0.511 | Train Acc: 74.60%\n",
      "| Epoch: 22 | Train Loss: 0.477 | Train Acc: 76.40%\n",
      "| Epoch: 22 | Train Loss: 0.513 | Train Acc: 73.60%\n",
      "| Epoch: 22 | Train Loss: 0.459 | Train Acc: 77.00%\n",
      "| Epoch: 22 | Train Loss: 0.468 | Train Acc: 77.40%\n",
      "| Epoch: 22 | Train Loss: 0.498 | Train Acc: 78.20%\n",
      "| Epoch: 22 | Train Loss: 0.507 | Train Acc: 73.80%\n",
      "| Epoch: 22 | Train Loss: 0.498 | Train Acc: 75.00%\n",
      "| Epoch: 22 | Train Loss: 0.533 | Train Acc: 72.20%\n",
      "| Epoch: 22 | Train Loss: 0.511 | Train Acc: 73.60%\n",
      "| Epoch: 22 | Train Loss: 0.446 | Train Acc: 79.60%\n",
      "| Epoch: 22 | Train Loss: 0.475 | Train Acc: 76.20%\n",
      "| Epoch: 22 | Train Loss: 0.446 | Train Acc: 78.20%\n",
      "| Epoch: 22 | Train Loss: 0.459 | Train Acc: 77.60%\n",
      "| Epoch: 22 | Train Loss: 0.484 | Train Acc: 77.08%\n",
      "|+| Validation Accuracy : 74.09 %\n",
      "| Epoch: 23 | Train Loss: 0.492 | Train Acc: 75.20%\n",
      "| Epoch: 23 | Train Loss: 0.527 | Train Acc: 74.20%\n",
      "| Epoch: 23 | Train Loss: 0.483 | Train Acc: 77.40%\n",
      "| Epoch: 23 | Train Loss: 0.477 | Train Acc: 76.20%\n",
      "| Epoch: 23 | Train Loss: 0.464 | Train Acc: 78.40%\n",
      "| Epoch: 23 | Train Loss: 0.480 | Train Acc: 74.80%\n",
      "| Epoch: 23 | Train Loss: 0.458 | Train Acc: 78.20%\n",
      "| Epoch: 23 | Train Loss: 0.513 | Train Acc: 73.40%\n",
      "| Epoch: 23 | Train Loss: 0.452 | Train Acc: 77.80%\n",
      "| Epoch: 23 | Train Loss: 0.483 | Train Acc: 77.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 23 | Train Loss: 0.437 | Train Acc: 79.60%\n",
      "| Epoch: 23 | Train Loss: 0.453 | Train Acc: 78.20%\n",
      "| Epoch: 23 | Train Loss: 0.477 | Train Acc: 76.40%\n",
      "| Epoch: 23 | Train Loss: 0.479 | Train Acc: 76.00%\n",
      "| Epoch: 23 | Train Loss: 0.487 | Train Acc: 75.40%\n",
      "| Epoch: 23 | Train Loss: 0.498 | Train Acc: 74.80%\n",
      "| Epoch: 23 | Train Loss: 0.494 | Train Acc: 75.00%\n",
      "| Epoch: 23 | Train Loss: 0.509 | Train Acc: 73.40%\n",
      "| Epoch: 23 | Train Loss: 0.500 | Train Acc: 75.40%\n",
      "| Epoch: 23 | Train Loss: 0.475 | Train Acc: 78.20%\n",
      "| Epoch: 23 | Train Loss: 0.511 | Train Acc: 74.00%\n",
      "| Epoch: 23 | Train Loss: 0.477 | Train Acc: 77.60%\n",
      "| Epoch: 23 | Train Loss: 0.484 | Train Acc: 76.80%\n",
      "| Epoch: 23 | Train Loss: 0.483 | Train Acc: 76.00%\n",
      "| Epoch: 23 | Train Loss: 0.464 | Train Acc: 75.60%\n",
      "| Epoch: 23 | Train Loss: 0.514 | Train Acc: 72.80%\n",
      "| Epoch: 23 | Train Loss: 0.516 | Train Acc: 74.31%\n",
      "|+| Validation Accuracy : 68.41 %\n",
      "| Epoch: 24 | Train Loss: 0.476 | Train Acc: 76.20%\n",
      "| Epoch: 24 | Train Loss: 0.453 | Train Acc: 78.20%\n",
      "| Epoch: 24 | Train Loss: 0.483 | Train Acc: 77.00%\n",
      "| Epoch: 24 | Train Loss: 0.506 | Train Acc: 75.20%\n",
      "| Epoch: 24 | Train Loss: 0.483 | Train Acc: 76.00%\n",
      "| Epoch: 24 | Train Loss: 0.471 | Train Acc: 79.00%\n",
      "| Epoch: 24 | Train Loss: 0.469 | Train Acc: 75.60%\n",
      "| Epoch: 24 | Train Loss: 0.494 | Train Acc: 75.60%\n",
      "| Epoch: 24 | Train Loss: 0.494 | Train Acc: 74.40%\n",
      "| Epoch: 24 | Train Loss: 0.484 | Train Acc: 76.60%\n",
      "| Epoch: 24 | Train Loss: 0.509 | Train Acc: 74.20%\n",
      "| Epoch: 24 | Train Loss: 0.449 | Train Acc: 80.00%\n",
      "| Epoch: 24 | Train Loss: 0.480 | Train Acc: 76.00%\n",
      "| Epoch: 24 | Train Loss: 0.501 | Train Acc: 74.80%\n",
      "| Epoch: 24 | Train Loss: 0.486 | Train Acc: 77.80%\n",
      "| Epoch: 24 | Train Loss: 0.485 | Train Acc: 75.80%\n",
      "| Epoch: 24 | Train Loss: 0.462 | Train Acc: 78.60%\n",
      "| Epoch: 24 | Train Loss: 0.472 | Train Acc: 76.60%\n",
      "| Epoch: 24 | Train Loss: 0.523 | Train Acc: 71.20%\n",
      "| Epoch: 24 | Train Loss: 0.498 | Train Acc: 74.20%\n",
      "| Epoch: 24 | Train Loss: 0.466 | Train Acc: 78.00%\n",
      "| Epoch: 24 | Train Loss: 0.453 | Train Acc: 80.00%\n",
      "| Epoch: 24 | Train Loss: 0.484 | Train Acc: 77.00%\n",
      "| Epoch: 24 | Train Loss: 0.479 | Train Acc: 74.80%\n",
      "| Epoch: 24 | Train Loss: 0.471 | Train Acc: 77.20%\n",
      "| Epoch: 24 | Train Loss: 0.488 | Train Acc: 76.00%\n",
      "| Epoch: 24 | Train Loss: 0.463 | Train Acc: 77.08%\n",
      "|+| Validation Accuracy : 65.11 %\n",
      "| Epoch: 25 | Train Loss: 0.426 | Train Acc: 81.20%\n",
      "| Epoch: 25 | Train Loss: 0.496 | Train Acc: 74.80%\n",
      "| Epoch: 25 | Train Loss: 0.515 | Train Acc: 74.20%\n",
      "| Epoch: 25 | Train Loss: 0.513 | Train Acc: 75.40%\n",
      "| Epoch: 25 | Train Loss: 0.485 | Train Acc: 77.40%\n",
      "| Epoch: 25 | Train Loss: 0.463 | Train Acc: 77.20%\n",
      "| Epoch: 25 | Train Loss: 0.470 | Train Acc: 76.00%\n",
      "| Epoch: 25 | Train Loss: 0.484 | Train Acc: 77.00%\n",
      "| Epoch: 25 | Train Loss: 0.458 | Train Acc: 78.40%\n",
      "| Epoch: 25 | Train Loss: 0.437 | Train Acc: 78.60%\n",
      "| Epoch: 25 | Train Loss: 0.463 | Train Acc: 78.40%\n",
      "| Epoch: 25 | Train Loss: 0.473 | Train Acc: 76.00%\n",
      "| Epoch: 25 | Train Loss: 0.478 | Train Acc: 75.60%\n",
      "| Epoch: 25 | Train Loss: 0.485 | Train Acc: 76.80%\n",
      "| Epoch: 25 | Train Loss: 0.445 | Train Acc: 80.00%\n",
      "| Epoch: 25 | Train Loss: 0.477 | Train Acc: 78.20%\n",
      "| Epoch: 25 | Train Loss: 0.477 | Train Acc: 76.00%\n",
      "| Epoch: 25 | Train Loss: 0.482 | Train Acc: 75.60%\n",
      "| Epoch: 25 | Train Loss: 0.503 | Train Acc: 74.20%\n",
      "| Epoch: 25 | Train Loss: 0.496 | Train Acc: 75.20%\n",
      "| Epoch: 25 | Train Loss: 0.480 | Train Acc: 76.40%\n",
      "| Epoch: 25 | Train Loss: 0.469 | Train Acc: 78.60%\n",
      "| Epoch: 25 | Train Loss: 0.444 | Train Acc: 79.60%\n",
      "| Epoch: 25 | Train Loss: 0.508 | Train Acc: 72.80%\n",
      "| Epoch: 25 | Train Loss: 0.505 | Train Acc: 75.20%\n",
      "| Epoch: 25 | Train Loss: 0.508 | Train Acc: 75.00%\n",
      "| Epoch: 25 | Train Loss: 0.414 | Train Acc: 79.17%\n",
      "|+| Validation Accuracy : 77.16 %\n",
      "| Epoch: 26 | Train Loss: 0.493 | Train Acc: 73.40%\n",
      "| Epoch: 26 | Train Loss: 0.466 | Train Acc: 78.00%\n",
      "| Epoch: 26 | Train Loss: 0.464 | Train Acc: 76.20%\n",
      "| Epoch: 26 | Train Loss: 0.443 | Train Acc: 81.00%\n",
      "| Epoch: 26 | Train Loss: 0.479 | Train Acc: 76.80%\n",
      "| Epoch: 26 | Train Loss: 0.487 | Train Acc: 75.40%\n",
      "| Epoch: 26 | Train Loss: 0.490 | Train Acc: 75.00%\n",
      "| Epoch: 26 | Train Loss: 0.526 | Train Acc: 73.40%\n",
      "| Epoch: 26 | Train Loss: 0.469 | Train Acc: 74.80%\n",
      "| Epoch: 26 | Train Loss: 0.468 | Train Acc: 78.60%\n",
      "| Epoch: 26 | Train Loss: 0.491 | Train Acc: 76.20%\n",
      "| Epoch: 26 | Train Loss: 0.495 | Train Acc: 75.60%\n",
      "| Epoch: 26 | Train Loss: 0.430 | Train Acc: 80.20%\n",
      "| Epoch: 26 | Train Loss: 0.508 | Train Acc: 76.60%\n",
      "| Epoch: 26 | Train Loss: 0.478 | Train Acc: 76.60%\n",
      "| Epoch: 26 | Train Loss: 0.470 | Train Acc: 77.00%\n",
      "| Epoch: 26 | Train Loss: 0.466 | Train Acc: 75.80%\n",
      "| Epoch: 26 | Train Loss: 0.484 | Train Acc: 76.00%\n",
      "| Epoch: 26 | Train Loss: 0.430 | Train Acc: 79.60%\n",
      "| Epoch: 26 | Train Loss: 0.509 | Train Acc: 72.80%\n",
      "| Epoch: 26 | Train Loss: 0.498 | Train Acc: 75.60%\n",
      "| Epoch: 26 | Train Loss: 0.454 | Train Acc: 80.20%\n",
      "| Epoch: 26 | Train Loss: 0.531 | Train Acc: 72.20%\n",
      "| Epoch: 26 | Train Loss: 0.435 | Train Acc: 79.40%\n",
      "| Epoch: 26 | Train Loss: 0.443 | Train Acc: 79.60%\n",
      "| Epoch: 26 | Train Loss: 0.470 | Train Acc: 75.80%\n",
      "| Epoch: 26 | Train Loss: 0.469 | Train Acc: 75.69%\n",
      "|+| Validation Accuracy : 75.91 %\n",
      "| Epoch: 27 | Train Loss: 0.500 | Train Acc: 73.60%\n",
      "| Epoch: 27 | Train Loss: 0.449 | Train Acc: 80.00%\n",
      "| Epoch: 27 | Train Loss: 0.462 | Train Acc: 77.40%\n",
      "| Epoch: 27 | Train Loss: 0.502 | Train Acc: 76.00%\n",
      "| Epoch: 27 | Train Loss: 0.459 | Train Acc: 79.40%\n",
      "| Epoch: 27 | Train Loss: 0.471 | Train Acc: 75.00%\n",
      "| Epoch: 27 | Train Loss: 0.508 | Train Acc: 76.20%\n",
      "| Epoch: 27 | Train Loss: 0.472 | Train Acc: 76.20%\n",
      "| Epoch: 27 | Train Loss: 0.432 | Train Acc: 81.20%\n",
      "| Epoch: 27 | Train Loss: 0.481 | Train Acc: 75.60%\n",
      "| Epoch: 27 | Train Loss: 0.458 | Train Acc: 75.00%\n",
      "| Epoch: 27 | Train Loss: 0.458 | Train Acc: 78.60%\n",
      "| Epoch: 27 | Train Loss: 0.481 | Train Acc: 77.80%\n",
      "| Epoch: 27 | Train Loss: 0.467 | Train Acc: 79.40%\n",
      "| Epoch: 27 | Train Loss: 0.448 | Train Acc: 78.00%\n",
      "| Epoch: 27 | Train Loss: 0.484 | Train Acc: 77.20%\n",
      "| Epoch: 27 | Train Loss: 0.485 | Train Acc: 75.20%\n",
      "| Epoch: 27 | Train Loss: 0.504 | Train Acc: 73.80%\n",
      "| Epoch: 27 | Train Loss: 0.450 | Train Acc: 79.40%\n",
      "| Epoch: 27 | Train Loss: 0.471 | Train Acc: 76.20%\n",
      "| Epoch: 27 | Train Loss: 0.483 | Train Acc: 76.40%\n",
      "| Epoch: 27 | Train Loss: 0.485 | Train Acc: 76.20%\n",
      "| Epoch: 27 | Train Loss: 0.493 | Train Acc: 72.40%\n",
      "| Epoch: 27 | Train Loss: 0.451 | Train Acc: 79.20%\n",
      "| Epoch: 27 | Train Loss: 0.439 | Train Acc: 78.60%\n",
      "| Epoch: 27 | Train Loss: 0.526 | Train Acc: 74.20%\n",
      "| Epoch: 27 | Train Loss: 0.550 | Train Acc: 68.06%\n",
      "|+| Validation Accuracy : 72.05 %\n",
      "| Epoch: 28 | Train Loss: 0.495 | Train Acc: 72.00%\n",
      "| Epoch: 28 | Train Loss: 0.470 | Train Acc: 77.60%\n",
      "| Epoch: 28 | Train Loss: 0.478 | Train Acc: 79.40%\n",
      "| Epoch: 28 | Train Loss: 0.487 | Train Acc: 75.40%\n",
      "| Epoch: 28 | Train Loss: 0.478 | Train Acc: 76.40%\n",
      "| Epoch: 28 | Train Loss: 0.492 | Train Acc: 75.80%\n",
      "| Epoch: 28 | Train Loss: 0.480 | Train Acc: 75.00%\n",
      "| Epoch: 28 | Train Loss: 0.460 | Train Acc: 79.40%\n",
      "| Epoch: 28 | Train Loss: 0.442 | Train Acc: 79.00%\n",
      "| Epoch: 28 | Train Loss: 0.474 | Train Acc: 76.60%\n",
      "| Epoch: 28 | Train Loss: 0.451 | Train Acc: 79.20%\n",
      "| Epoch: 28 | Train Loss: 0.445 | Train Acc: 78.80%\n",
      "| Epoch: 28 | Train Loss: 0.450 | Train Acc: 79.00%\n",
      "| Epoch: 28 | Train Loss: 0.477 | Train Acc: 76.00%\n",
      "| Epoch: 28 | Train Loss: 0.493 | Train Acc: 76.00%\n",
      "| Epoch: 28 | Train Loss: 0.450 | Train Acc: 78.20%\n",
      "| Epoch: 28 | Train Loss: 0.488 | Train Acc: 78.40%\n",
      "| Epoch: 28 | Train Loss: 0.473 | Train Acc: 76.20%\n",
      "| Epoch: 28 | Train Loss: 0.484 | Train Acc: 76.60%\n",
      "| Epoch: 28 | Train Loss: 0.459 | Train Acc: 79.40%\n",
      "| Epoch: 28 | Train Loss: 0.468 | Train Acc: 76.00%\n",
      "| Epoch: 28 | Train Loss: 0.485 | Train Acc: 75.20%\n",
      "| Epoch: 28 | Train Loss: 0.468 | Train Acc: 77.40%\n",
      "| Epoch: 28 | Train Loss: 0.452 | Train Acc: 78.60%\n",
      "| Epoch: 28 | Train Loss: 0.520 | Train Acc: 72.40%\n",
      "| Epoch: 28 | Train Loss: 0.472 | Train Acc: 77.00%\n",
      "| Epoch: 28 | Train Loss: 0.479 | Train Acc: 78.47%\n",
      "|+| Validation Accuracy : 75.11 %\n",
      "| Epoch: 29 | Train Loss: 0.452 | Train Acc: 77.60%\n",
      "| Epoch: 29 | Train Loss: 0.476 | Train Acc: 77.00%\n",
      "| Epoch: 29 | Train Loss: 0.443 | Train Acc: 77.00%\n",
      "| Epoch: 29 | Train Loss: 0.480 | Train Acc: 78.80%\n",
      "| Epoch: 29 | Train Loss: 0.471 | Train Acc: 76.80%\n",
      "| Epoch: 29 | Train Loss: 0.469 | Train Acc: 74.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 29 | Train Loss: 0.432 | Train Acc: 79.60%\n",
      "| Epoch: 29 | Train Loss: 0.464 | Train Acc: 77.40%\n",
      "| Epoch: 29 | Train Loss: 0.471 | Train Acc: 76.40%\n",
      "| Epoch: 29 | Train Loss: 0.501 | Train Acc: 74.60%\n",
      "| Epoch: 29 | Train Loss: 0.467 | Train Acc: 77.80%\n",
      "| Epoch: 29 | Train Loss: 0.497 | Train Acc: 74.80%\n",
      "| Epoch: 29 | Train Loss: 0.462 | Train Acc: 75.80%\n",
      "| Epoch: 29 | Train Loss: 0.497 | Train Acc: 71.20%\n",
      "| Epoch: 29 | Train Loss: 0.496 | Train Acc: 75.40%\n",
      "| Epoch: 29 | Train Loss: 0.507 | Train Acc: 74.80%\n",
      "| Epoch: 29 | Train Loss: 0.515 | Train Acc: 74.20%\n",
      "| Epoch: 29 | Train Loss: 0.436 | Train Acc: 80.80%\n",
      "| Epoch: 29 | Train Loss: 0.463 | Train Acc: 78.80%\n",
      "| Epoch: 29 | Train Loss: 0.472 | Train Acc: 79.00%\n",
      "| Epoch: 29 | Train Loss: 0.469 | Train Acc: 76.60%\n",
      "| Epoch: 29 | Train Loss: 0.472 | Train Acc: 76.20%\n",
      "| Epoch: 29 | Train Loss: 0.474 | Train Acc: 77.40%\n",
      "| Epoch: 29 | Train Loss: 0.458 | Train Acc: 78.40%\n",
      "| Epoch: 29 | Train Loss: 0.451 | Train Acc: 78.40%\n",
      "| Epoch: 29 | Train Loss: 0.468 | Train Acc: 77.20%\n",
      "| Epoch: 29 | Train Loss: 0.483 | Train Acc: 77.08%\n",
      "|+| Validation Accuracy : 56.93 %\n",
      "| Epoch: 30 | Train Loss: 0.480 | Train Acc: 75.60%\n",
      "| Epoch: 30 | Train Loss: 0.431 | Train Acc: 79.40%\n",
      "| Epoch: 30 | Train Loss: 0.445 | Train Acc: 78.60%\n",
      "| Epoch: 30 | Train Loss: 0.475 | Train Acc: 77.40%\n",
      "| Epoch: 30 | Train Loss: 0.466 | Train Acc: 75.60%\n",
      "| Epoch: 30 | Train Loss: 0.459 | Train Acc: 78.20%\n",
      "| Epoch: 30 | Train Loss: 0.497 | Train Acc: 75.00%\n",
      "| Epoch: 30 | Train Loss: 0.448 | Train Acc: 80.60%\n",
      "| Epoch: 30 | Train Loss: 0.486 | Train Acc: 74.60%\n",
      "| Epoch: 30 | Train Loss: 0.490 | Train Acc: 72.80%\n",
      "| Epoch: 30 | Train Loss: 0.473 | Train Acc: 78.40%\n",
      "| Epoch: 30 | Train Loss: 0.448 | Train Acc: 78.00%\n",
      "| Epoch: 30 | Train Loss: 0.500 | Train Acc: 72.80%\n",
      "| Epoch: 30 | Train Loss: 0.476 | Train Acc: 78.60%\n",
      "| Epoch: 30 | Train Loss: 0.455 | Train Acc: 80.40%\n",
      "| Epoch: 30 | Train Loss: 0.485 | Train Acc: 76.80%\n",
      "| Epoch: 30 | Train Loss: 0.436 | Train Acc: 80.80%\n",
      "| Epoch: 30 | Train Loss: 0.465 | Train Acc: 79.00%\n",
      "| Epoch: 30 | Train Loss: 0.482 | Train Acc: 76.00%\n",
      "| Epoch: 30 | Train Loss: 0.479 | Train Acc: 76.00%\n",
      "| Epoch: 30 | Train Loss: 0.469 | Train Acc: 76.40%\n",
      "| Epoch: 30 | Train Loss: 0.508 | Train Acc: 73.60%\n",
      "| Epoch: 30 | Train Loss: 0.475 | Train Acc: 76.80%\n",
      "| Epoch: 30 | Train Loss: 0.487 | Train Acc: 73.80%\n",
      "| Epoch: 30 | Train Loss: 0.446 | Train Acc: 78.40%\n",
      "| Epoch: 30 | Train Loss: 0.451 | Train Acc: 78.20%\n",
      "| Epoch: 30 | Train Loss: 0.476 | Train Acc: 77.78%\n",
      "|+| Validation Accuracy : 74.20 %\n",
      "| Epoch: 31 | Train Loss: 0.473 | Train Acc: 75.40%\n",
      "| Epoch: 31 | Train Loss: 0.490 | Train Acc: 78.20%\n",
      "| Epoch: 31 | Train Loss: 0.429 | Train Acc: 79.80%\n",
      "| Epoch: 31 | Train Loss: 0.492 | Train Acc: 74.20%\n",
      "| Epoch: 31 | Train Loss: 0.465 | Train Acc: 79.00%\n",
      "| Epoch: 31 | Train Loss: 0.435 | Train Acc: 81.20%\n",
      "| Epoch: 31 | Train Loss: 0.448 | Train Acc: 78.20%\n",
      "| Epoch: 31 | Train Loss: 0.487 | Train Acc: 76.20%\n",
      "| Epoch: 31 | Train Loss: 0.453 | Train Acc: 77.40%\n",
      "| Epoch: 31 | Train Loss: 0.481 | Train Acc: 75.40%\n",
      "| Epoch: 31 | Train Loss: 0.465 | Train Acc: 78.40%\n",
      "| Epoch: 31 | Train Loss: 0.460 | Train Acc: 77.20%\n",
      "| Epoch: 31 | Train Loss: 0.445 | Train Acc: 79.60%\n",
      "| Epoch: 31 | Train Loss: 0.468 | Train Acc: 77.20%\n",
      "| Epoch: 31 | Train Loss: 0.464 | Train Acc: 78.80%\n",
      "| Epoch: 31 | Train Loss: 0.472 | Train Acc: 76.80%\n",
      "| Epoch: 31 | Train Loss: 0.481 | Train Acc: 74.00%\n",
      "| Epoch: 31 | Train Loss: 0.447 | Train Acc: 79.60%\n",
      "| Epoch: 31 | Train Loss: 0.470 | Train Acc: 76.60%\n",
      "| Epoch: 31 | Train Loss: 0.465 | Train Acc: 77.60%\n",
      "| Epoch: 31 | Train Loss: 0.480 | Train Acc: 76.80%\n",
      "| Epoch: 31 | Train Loss: 0.472 | Train Acc: 79.00%\n",
      "| Epoch: 31 | Train Loss: 0.464 | Train Acc: 78.00%\n",
      "| Epoch: 31 | Train Loss: 0.472 | Train Acc: 76.60%\n",
      "| Epoch: 31 | Train Loss: 0.484 | Train Acc: 73.20%\n",
      "| Epoch: 31 | Train Loss: 0.466 | Train Acc: 77.20%\n",
      "| Epoch: 31 | Train Loss: 0.544 | Train Acc: 71.53%\n",
      "|+| Validation Accuracy : 61.93 %\n",
      "| Epoch: 32 | Train Loss: 0.455 | Train Acc: 77.20%\n",
      "| Epoch: 32 | Train Loss: 0.447 | Train Acc: 78.20%\n",
      "| Epoch: 32 | Train Loss: 0.438 | Train Acc: 79.60%\n",
      "| Epoch: 32 | Train Loss: 0.469 | Train Acc: 77.00%\n",
      "| Epoch: 32 | Train Loss: 0.487 | Train Acc: 74.80%\n"
     ]
    }
   ],
   "source": [
    "taskAclassifier.train(500,lr=0.001,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
