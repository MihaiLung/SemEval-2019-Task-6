{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import mkdir\n",
    "from os.path import join, isfile, isdir, exists\n",
    "import bcolz\n",
    "import pickle \n",
    "import emoji\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This Class Loads the Tweet Dataset, Cleans it. It also enables the loading for the training and testing. \n",
    "TODO : Loading for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self):\n",
    "        # All the Text Data path\n",
    "        self.path = {}\n",
    "        self.path['train'] = join('..','data','start-kit','training-v1','offenseval-training-v1.tsv')\n",
    "        #self.path['trial'] = join('..','data','start-kit','trial-data','offenseval-trial.txt')\n",
    "        self.path['testA'] = join('..','data','Test A Release','testset-taska.tsv')\n",
    "        self.path['testB'] = join('..','data','Test B Release','testset-taskb.tsv')\n",
    "        self.path['testC'] = join('..','data','Test C Release','test_set_taskc.tsv')\n",
    "        \n",
    "        # Load, Clean and Tokenize the Datasets\n",
    "        print('---- Load, Clean and Tokensize Dataset : ',end='')\n",
    "        self.data = {}\n",
    "        self.inital_dataload()\n",
    "        print('Done')\n",
    "        \n",
    "        # Compute List of All words in the datasets\n",
    "        print('---- List all tokenized words : ',end='')\n",
    "        self.all_words_freq = {}\n",
    "        self.all_words = []\n",
    "        self.compute_wordlist()\n",
    "        print('Done')\n",
    "\n",
    "\n",
    "    def inital_dataload(self):\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_table(self.path[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['tweet'].apply(lambda x : self.clean_tweet(x))\n",
    "            \n",
    "    def compute_wordlist(self):\n",
    "        \n",
    "        for f in self.data : \n",
    "            for i in range(len(self.data[f])):\n",
    "                for e in self.data[f].iloc[i].token:\n",
    "                    self.all_words_freq[e] = 1 + self.all_words_freq.get(e,0)\n",
    "        self.all_words = list(self.all_words_freq.keys())\n",
    "    def clean_tweet(self,text):\n",
    "        ''' Function that is applied to every to tweet in the dataset '''\n",
    "        \n",
    "        # =========== TEXT ===========\n",
    "        # Replace @USER by <user>\n",
    "        text = re.compile(r'@USER').sub(r'<user>',text)\n",
    "        \n",
    "        # Remove  Hashtags (#)\n",
    "        text = re.compile(r'#').sub(r'',text)\n",
    "\n",
    "        # Replace URL by <url>\n",
    "        text = re.compile(r'URL').sub(r'<url>',text)\n",
    "\n",
    "        # Remove numbers :\n",
    "        text = re.compile(r'[0-9]+').sub(r' ',text)\n",
    "\n",
    "        # Remove some special characters\n",
    "        text = re.compile(r'([_\\{\\}\\[\\]¬¨‚Ä¢$,:;/@#|\\^*%().‚Äù\"‚Äú-])').sub(r' ',text) \n",
    "\n",
    "        # Space the special characters with white spaces\n",
    "        text = re.compile(r'([$&+,:;=?@#|\\'.^*()%!\"‚Äô‚Äú-])').sub(r' \\1 ',text) \n",
    "\n",
    "        # Replace some special characters : \n",
    "        replace_dict = {r'&' : 'and' , \n",
    "                        r'\\+' : 'plus'}\n",
    "        for cha in replace_dict:\n",
    "            text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
    "            \n",
    "        # Handle Emoji : translate some and delete the others\n",
    "        text = self.handle_emoji(text)\n",
    "\n",
    "        # Cut the words with caps in them : \n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
    "        # =========== TOKENS ===========\n",
    "        # TOKENIZE \n",
    "        text = text.split(' ')\n",
    "\n",
    "        # Remove white spaces tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
    "\n",
    "        # Remove empty tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != '']\n",
    "\n",
    "        # Remove repetition in tokens (!!! => !)\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
    "\n",
    "        #  Handle the ALL CAPS Tweets \n",
    "        ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
    "        caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
    "        if caps_r > 0.6 : \n",
    "            text.append('<allcaps>')\n",
    "\n",
    "        # Lower Case : \n",
    "        text = [text[i].lower() for i in range(len(text))]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def handle_emoji(self,text):\n",
    "        # Dictionnary of \"important\" emojis : \n",
    "        emoji_dict =  {'‚ô•Ô∏è': ' love ',\n",
    "                       '‚ù§Ô∏è' : ' love ',\n",
    "                       '‚ù§' : ' love ',\n",
    "                       'üòò' : ' kisses ',\n",
    "                      'üò≠' : ' cry ',\n",
    "                      'üí™' : ' strong ',\n",
    "                      'üåç' : ' earth ',\n",
    "                      'üí∞' : ' money ',\n",
    "                      'üëç' : ' ok ',\n",
    "                       'üëå' : ' ok ',\n",
    "                      'üò°' : ' angry ',\n",
    "                      'üçÜ' : ' dick ',\n",
    "                      'ü§£' : ' haha ',\n",
    "                      'üòÇ' : ' haha ',\n",
    "                      'üñï' : ' fuck you '}\n",
    "\n",
    "        for cha in emoji_dict:\n",
    "            text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
    "        # Remove ALL emojis\n",
    "        text = emoji.get_emoji_regexp().sub(r' ',text) \n",
    "        text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
    "        text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
    "\n",
    "        # Add Space between  the Emoji Expressions : \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load, Clean and Tokensize Dataset : Done\n",
      "---- List all tokenized words : Done\n"
     ]
    }
   ],
   "source": [
    "mydata = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " 'the',\n",
       " 'is',\n",
       " 'to',\n",
       " 'and',\n",
       " 'a',\n",
       " 'you',\n",
       " 'of',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'are',\n",
       " '!',\n",
       " 'he',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'she',\n",
       " '?',\n",
       " 's',\n",
       " 'for',\n",
       " '<url>',\n",
       " 't',\n",
       " 'this',\n",
       " 'on',\n",
       " 'they',\n",
       " 'not',\n",
       " 'with',\n",
       " 'have',\n",
       " 'liberals',\n",
       " 'be',\n",
       " 'gun',\n",
       " 'all',\n",
       " 'so',\n",
       " 'control',\n",
       " 'antifa',\n",
       " 'your',\n",
       " 'what',\n",
       " 'we',\n",
       " 'but',\n",
       " 'like',\n",
       " 'as',\n",
       " 'just',\n",
       " 'maga',\n",
       " 'about',\n",
       " 'her',\n",
       " 'was',\n",
       " 'conservatives',\n",
       " 'if',\n",
       " 'will',\n",
       " 'who']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(list(mydata.all_words_freq.keys()))\n",
    "list(words[list(np.argsort(list(mydata.all_words_freq.values())))[::-1]][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86426</th>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, she, should, ask, a, few, native, ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90194</th>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>[&lt;user&gt;, go, home, you, re, drunk, !, &lt;user&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16820</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62688</th>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, someone, should, ', ve, taken, this, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43605</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, obama, wanted, liberals, and, amp, il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97670</th>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>[&lt;user&gt;, liberals, are, all, kookoo, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77444</th>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, oh, noes, !, tough, shit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52415</th>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "      <td>[&lt;user&gt;, was, literally, just, talking, about,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45157</th>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, buy, more, icecream, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13384</th>\n",
       "      <td>@USER Canada doesn‚Äôt need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>[&lt;user&gt;, canada, doesn, t, need, another, cuck...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet subtask_a subtask_b  \\\n",
       "id                                                                             \n",
       "86426  @USER She should ask a few native Americans wh...       OFF       UNT   \n",
       "90194  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...       OFF       TIN   \n",
       "16820  Amazon is investigating Chinese employees who ...       NOT       NaN   \n",
       "62688  @USER Someone should'veTaken\" this piece of sh...       OFF       UNT   \n",
       "43605  @USER @USER Obama wanted liberals &amp; illega...       NOT       NaN   \n",
       "97670                  @USER Liberals are all Kookoo !!!       OFF       TIN   \n",
       "77444                   @USER @USER Oh noes! Tough shit.       OFF       UNT   \n",
       "52415  @USER was literally just talking about this lo...       OFF       TIN   \n",
       "45157                         @USER Buy more icecream!!!       NOT       NaN   \n",
       "13384  @USER Canada doesn‚Äôt need another CUCK! We alr...       OFF       TIN   \n",
       "\n",
       "      subtask_c                                              token  \n",
       "id                                                                  \n",
       "86426       NaN  [<user>, she, should, ask, a, few, native, ame...  \n",
       "90194       IND  [<user>, go, home, you, re, drunk, !, <user>, ...  \n",
       "16820       NaN  [amazon, is, investigating, chinese, employees...  \n",
       "62688       NaN  [<user>, someone, should, ', ve, taken, this, ...  \n",
       "43605       NaN  [<user>, obama, wanted, liberals, and, amp, il...  \n",
       "97670       OTH            [<user>, liberals, are, all, kookoo, !]  \n",
       "77444       NaN                 [<user>, oh, noes, !, tough, shit]  \n",
       "52415       GRP  [<user>, was, literally, just, talking, about,...  \n",
       "45157       NaN                   [<user>, buy, more, icecream, !]  \n",
       "13384       IND  [<user>, canada, doesn, t, need, another, cuck...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.data['train'].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings : GloVe\n",
    "This Class Loads the GloVe Embeding, processes it, and create a word embedding given the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe_embedding(object):\n",
    "    def __init__(self, dataLoader,dim_vect = 25 ):\n",
    "        ########## VARIABLES ##########\n",
    "        \n",
    "        # Defining variables for GloVe: \n",
    "        self.words = []\n",
    "        self.word2idx = {}\n",
    "        self.glove_dict = {}\n",
    "        \n",
    "        # Defining variables for our Embedding:\n",
    "        self.dim_vect = dim_vect\n",
    "        self.size_vocab = len(dataLoader.all_words)\n",
    "        self.emb_vocab = dataLoader.all_words\n",
    "        self.emb_matrix = torch.zeros((self.size_vocab,self.dim_vect))\n",
    "        \n",
    "        ########## LOADING GLOVE DATA ##########\n",
    "        \n",
    "        # Defining path for GloVe Data : \n",
    "        self.path = join('..','data','glove') # Path of glove\n",
    "        self.path_glove = join(self.path,'glove.twitter.27B.'+str(dim_vect))\n",
    "        if not(isdir(self.path_glove)):\n",
    "            mkdir(self.path_glove)\n",
    "        self.path_vec_original = join(self.path,'glove.twitter.27B.'+str(dim_vect)+'d.txt') # Path of glove original vectors\n",
    "        self.path_vec_save = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.vectors.dat')  # Path of glove saved vectors\n",
    "        self.path_words = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.words.pkl')\n",
    "        self.path_word2idx = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.word2idx.pkl')\n",
    "                \n",
    "        if not(isdir(self.path_vec_save) and isfile(self.path_words) and isfile(self.path_word2idx)) : \n",
    "            # If files are allready processed, just load them\n",
    "            print('---- Processing the GloVe files : ',end='')\n",
    "            self.process_GloVe()\n",
    "            print('Done')\n",
    "            \n",
    "        # Load the wordvec files\n",
    "        print('---- Loading the processed GloVe files : ',end='')\n",
    "        self.load_GloVe()\n",
    "        print('Done')\n",
    "        \n",
    "        # Adding the unknown words to the embedding\n",
    "        print('---- Processing the Missing Vocabulary  : ',end='')\n",
    "        nb_added = self.add_wordsData()\n",
    "        print('Done')\n",
    "        print('---- Words added : {0:} - Words in total : {1:}  -  Unreferenced Vocabulary : {2:.2f}%'.format(nb_added,self.size_vocab,100*nb_added/self.size_vocab))\n",
    "        \n",
    "        ########## TORCH EMBEDDING ##########\n",
    "        # Creating the Pytorch Embedding Layer : \n",
    "        print('---- Creating the Pytorch Embedding Layer  : ',end='')\n",
    "        self.emb_layer = nn.Embedding(self.size_vocab, self.dim_vect)\n",
    "        self.create_emb_layer(non_trainable=True)\n",
    "        print('Done')\n",
    "\n",
    "               \n",
    "    def process_GloVe(self):\n",
    "        ''' Processes the GloVe Dataset - Saves files'''\n",
    "        words = []\n",
    "        word2idx = {}\n",
    "        \n",
    "        vectors = bcolz.carray(np.zeros(1), rootdir=self.path_vec_save, mode='w') # defining vector saved\n",
    "\n",
    "        idx = 0\n",
    "        with open(self.path_vec_original, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "\n",
    "        vectors = bcolz.carray(vectors[:].reshape((-1, self.dim_vect)), rootdir=self.path_vec_save, mode='w')\n",
    "\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(self.path_words, 'wb'))\n",
    "        pickle.dump(word2idx, open(self.path_word2idx, 'wb'))\n",
    "        \n",
    "    def load_GloVe(self):\n",
    "        ''' Loads previously processed dataset'''\n",
    "        \n",
    "        vectors = bcolz.open(self.path_vec_save)[:]\n",
    "\n",
    "        self.words = pickle.load(open(self.path_words, 'rb'))\n",
    "        self.word2idx = pickle.load(open(self.path_word2idx, 'rb'))\n",
    "        \n",
    "        self.glove_dict = {w: vectors[self.word2idx[w]] for w in self.words}\n",
    "    \n",
    "    def add_wordsData(self):\n",
    "        nb_added = 0\n",
    "        for i, word in enumerate(self.emb_vocab) :\n",
    "            try: \n",
    "                self.emb_matrix[i,:] = torch.Tensor(self.glove_dict[word])\n",
    "            except KeyError:\n",
    "                nb_added +=1\n",
    "                self.emb_matrix[i,:] = torch.Tensor(np.random.normal(scale=0.6, size=(self.dim_vect,)))\n",
    "        return nb_added\n",
    "    \n",
    "    def create_emb_layer(self, non_trainable=True):\n",
    "        self.emb_layer.load_state_dict({'weight': self.emb_matrix})\n",
    "        if non_trainable:\n",
    "            self.emb_layer.weight.requires_grad = False\n",
    "            \n",
    "    def word2vec_(self,word):\n",
    "        return self.vectors[self.word2idx[word]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Processing the Missing Vocabulary  : Done\n",
      "---- Words added : 2203 - Words in total : 19258  -  Unreferenced Vocabulary : 11.44%\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n"
     ]
    }
   ],
   "source": [
    "myEmbedding = GloVe_embedding(mydata,dim_vect=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier \n",
    "Set of Classes used as classifier for the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim, vocab_size, max_len, num_classes):\n",
    "        print('----Creating FFNN : ',end='')\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        #embedding (lookup layer) layer\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        #hidden layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        #activation\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #output layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)  \n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # we average the embeddings of words in a sentence\n",
    "        averaged = embedded.mean(1)\n",
    "        \n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class : Trainer\n",
    "Main Class for the loading, training, testing etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffensiveClassifier(object):\n",
    "    ''' Main Class for the loading, training, testing etc ...'''\n",
    "    def __init__(self, dim_vect=25,cType='FFNN'):\n",
    "        \n",
    "        self.dim_vect = dim_vect\n",
    "        \n",
    "        # Loading the Tweet Data : \n",
    "        self.dataloader = DataLoader()\n",
    "        \n",
    "\n",
    "        # Loading the GloVe Embedding \n",
    "        self.embedding = GloVe_embedding(self.dataloader,dim_vect=dim_vect)\n",
    "        \n",
    "        # Classification : \n",
    "        if cType == 'FFNN':\n",
    "            # Creating the Neuronal Network\n",
    "            self.model = FFNN()\n",
    "        if cType == 'logistic':\n",
    "            pass\n",
    "\n",
    "    def accuracy(output, target):\n",
    "\n",
    "        output = torch.round(torch.sigmoid(output))\n",
    "        correct = (output == target).float()\n",
    "        acc = correct.sum()/len(correct)\n",
    "        return acc\n",
    "    \n",
    "    def train(self,nb_epochs):\n",
    "        # we will train for N epochs (N times the model will see all the data)\n",
    "        epochs=20\n",
    "\n",
    "        # the input dimension is the vocabulary size\n",
    "        INPUT_DIM = len(word2idx)\n",
    "\n",
    "        # we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "        EMBEDDING_DIM = 100\n",
    "\n",
    "        # dimensionality of the output of the second hidden layer\n",
    "        HIDDEN_DIM = 50\n",
    "\n",
    "        #the outut dimension is the number of classes, 1 for binary classification\n",
    "        OUTPUT_DIM = 1\n",
    "\n",
    "\n",
    "        # recall input parameters to our model\n",
    "        #embedding_dim, hidden_dim, vocab_size, max_len, num_classes\n",
    "        # max_len is the maximum length of the input sentences as we defined during padding\n",
    "\n",
    "        model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), max_len, OUTPUT_DIM)\n",
    "\n",
    "        # we use the stochastic gradient descent (SGD) optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "        # we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
    "        #Recall we did not apply any activation to our output layer, we need to make our outputs look like probality.\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        feature = train_sent_tensor\n",
    "        target = train_label_tensor\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "            #good practice to include even if do not use here\n",
    "            model.train()\n",
    "\n",
    "            #we zero the gradients as they are not removed automatically\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # queeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
    "            predictions = model(feature).squeeze(1)\n",
    "            loss = loss_fn(predictions, target)\n",
    "            acc = accuracy(predictions, target)\n",
    "            #calculate the gradient of each parameter\n",
    "            loss.backward()\n",
    "            #update the parameters using the gradients and optimizer algorithm \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss = loss.item()\n",
    "            epoch_acc = acc\n",
    "\n",
    "\n",
    "\n",
    "            print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {epoch_acc*100:.2f}%')\n",
    "            #print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
