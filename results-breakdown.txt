This file contains details for all of your submissions.

Your best runs were:
	* Sub-task A, adrianlwn CodaLab #549184
	* Sub-task B, adrianlwn CodaLab #549189
	* Sub-task C, adrianlwn CodaLab #549197

You will find detailed results for all your submissions below.

====

Sub-task A, adrianlwn CodaLab #549183

# SUBMITTED FILE: subtask_a_2.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  CNN 1
DESCRIPTION : Implementation of the CNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.73968879846
Overall Accuracy: 0.782558139535

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8676    0.8242    0.8453       620
        OFF     0.5978    0.6750    0.6341       240

avg / total     0.7923    0.7826    0.7864       860


====

Sub-task A, adrianlwn CodaLab #549184

# SUBMITTED FILE: subtask_a_1.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  CNN 3
DESCRIPTION : Implementation of the CNN Classifier with 3 layers.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
w

# RESULTS:

Macro-F1: 0.748752208714
Overall Accuracy: 0.806976744186

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8471    0.8935    0.8697       620
        OFF     0.6796    0.5833    0.6278       240

avg / total     0.8004    0.8070    0.8022       860


====

Sub-task A, adrianlwn CodaLab #549185

# SUBMITTED FILE: subtask_a_1.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  FFNN
DESCRIPTION : Implementation of the FFNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
w

# RESULTS:

Macro-F1: 0.6722323049
Overall Accuracy: 0.706976744186

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8538    0.7161    0.7789       620
        OFF     0.4824    0.6833    0.5655       240

avg / total     0.7502    0.7070    0.7194       860


====

Sub-task A, adrianlwn CodaLab #549186

# SUBMITTED FILE: subtask_a.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  GRU
DESCRIPTION : Implementation of the GRU Classifier.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.73095638028
Overall Accuracy: 0.773255813953

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8657    0.8113    0.8376       620
        OFF     0.5806    0.6750    0.6243       240

avg / total     0.7862    0.7733    0.7781       860


====

Sub-task A, adrianlwn CodaLab #549187

# SUBMITTED FILE: subtask_a_6.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  LSTM
DESCRIPTION : Implementation of the LSTM Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
www

# RESULTS:

Macro-F1: 0.698233627287
Overall Accuracy: 0.753488372093

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8355    0.8194    0.8274       620
        OFF     0.5556    0.5833    0.5691       240

avg / total     0.7574    0.7535    0.7553       860


====

Sub-task A, adrianlwn CodaLab #549188

# SUBMITTED FILE: subtask_a_1.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  RNN
DESCRIPTION : Implementation of the RNN Classifier.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.698233627287
Overall Accuracy: 0.753488372093

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8355    0.8194    0.8274       620
        OFF     0.5556    0.5833    0.5691       240

avg / total     0.7574    0.7535    0.7553       860


====

Sub-task A, adrianlwn CodaLab #549206

# SUBMITTED FILE: subtask_a_1.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : A
ARCHITECTURE :  FFNN
DESCRIPTION : Implementation of the FFNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
w

# RESULTS:

Macro-F1: 0.6722323049
Overall Accuracy: 0.706976744186

Per-class performance:
             precision    recall  f1-score   samples

        NOT     0.8538    0.7161    0.7789       620
        OFF     0.4824    0.6833    0.5655       240

avg / total     0.7502    0.7070    0.7194       860


====

Sub-task B, adrianlwn CodaLab #549189

# SUBMITTED FILE: subtask_b.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  CNN 1
DESCRIPTION : Implementation of the CNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
w

# RESULTS:

Macro-F1: 0.715679442509
Overall Accuracy: 0.858333333333

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9543    0.8826    0.9171       213
        UNT     0.4186    0.6667    0.5143        27

avg / total     0.8940    0.8583    0.8718       240


====

Sub-task B, adrianlwn CodaLab #549190

# SUBMITTED FILE: subtask_b.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  CNN 3
DESCRIPTION : Implementation of the CNN Classifier with 3 layers.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.690128721781
Overall Accuracy: 0.8625

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9369    0.9061    0.9212       213
        UNT     0.4118    0.5185    0.4590        27

avg / total     0.8778    0.8625    0.8692       240


====

Sub-task B, adrianlwn CodaLab #549191

# SUBMITTED FILE: subtask_b_1.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  FFNN
DESCRIPTION : Implementation of the FFNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.606850335071
Overall Accuracy: 0.770833333333

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9341    0.7981    0.8608       213
        UNT     0.2586    0.5556    0.3529        27

avg / total     0.8581    0.7708    0.8036       240


====

Sub-task B, adrianlwn CodaLab #549192

# SUBMITTED FILE: subtask_b.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  GRU
DESCRIPTION : Implementation of the GRU Classifier.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.571373899732
Overall Accuracy: 0.766666666667

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9153    0.8122    0.8607       213
        UNT     0.2157    0.4074    0.2821        27

avg / total     0.8366    0.7667    0.7956       240


====

Sub-task B, adrianlwn CodaLab #549193

# SUBMITTED FILE: subtask_b.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  LSTM
DESCRIPTION : Implementation of the LSTM Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.615331010453
Overall Accuracy: 0.808333333333

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9239    0.8545    0.8878       213
        UNT     0.2791    0.4444    0.3429        27

avg / total     0.8513    0.8083    0.8265       240


====

Sub-task B, adrianlwn CodaLab #549194

# SUBMITTED FILE: subtask_b.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  RNN
DESCRIPTION : Implementation of the RNN Classifier.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
ww

# RESULTS:

Macro-F1: 0.567220997796
Overall Accuracy: 0.85

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9005    0.9343    0.9171       213
        UNT     0.2632    0.1852    0.2174        27

avg / total     0.8288    0.8500    0.8383       240


====

Sub-task B, adrianlwn CodaLab #549203

# SUBMITTED FILE: subtask_b.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : B
ARCHITECTURE :  CNN 1
DESCRIPTION : Implementation of the CNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.
w

# RESULTS:

Macro-F1: 0.715679442509
Overall Accuracy: 0.858333333333

Per-class performance:
             precision    recall  f1-score   samples

        TIN     0.9543    0.8826    0.9171       213
        UNT     0.4186    0.6667    0.5143        27

avg / total     0.8940    0.8583    0.8718       240


====

Sub-task C, adrianlwn CodaLab #549196

# SUBMITTED FILE: subtask_c.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : C
ARCHITECTURE :  CNN 1
DESCRIPTION : Implementation of the CNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.559990877136
Overall Accuracy: 0.647887323944

Per-class performance:
             precision    recall  f1-score   samples

        GRP     0.6395    0.7051    0.6707        78
        IND     0.7353    0.7500    0.7426       100
        OTH     0.3200    0.2286    0.2667        35

avg / total     0.6320    0.6479    0.6381       213


====

Sub-task C, adrianlwn CodaLab #549197

# SUBMITTED FILE: subtask_c.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : C
ARCHITECTURE :  CNN 3
DESCRIPTION : Implementation of the CNN Classifier with 3 layers.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.580794252523
Overall Accuracy: 0.694835680751

Per-class performance:
             precision    recall  f1-score   samples

        GRP     0.6517    0.7436    0.6946        78
        IND     0.7500    0.8400    0.7925       100
        OTH     0.5000    0.1714    0.2553        35

avg / total     0.6729    0.6948    0.6684       213


====

Sub-task C, adrianlwn CodaLab #549198

# SUBMITTED FILE: subtask_c.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : C
ARCHITECTURE :  FFNN
DESCRIPTION : Implementation of the FFNN Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.499750658698
Overall Accuracy: 0.539906103286

Per-class performance:
             precision    recall  f1-score   samples

        GRP     0.5513    0.5513    0.5513        78
        IND     0.7867    0.5900    0.6743       100
        OTH     0.2167    0.3714    0.2737        35

avg / total     0.6068    0.5399    0.5634       213


====

Sub-task C, adrianlwn CodaLab #549199

# SUBMITTED FILE: subtask_c_1.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : C
ARCHITECTURE :  GRU
DESCRIPTION : Implementation of the GRU Classifier.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.57530674895
Overall Accuracy: 0.657276995305

Per-class performance:
             precision    recall  f1-score   samples

        GRP     0.6383    0.7692    0.6977        78
        IND     0.7978    0.7100    0.7513       100
        OTH     0.3000    0.2571    0.2769        35

avg / total     0.6576    0.6573    0.6537       213


====

Sub-task C, adrianlwn CodaLab #549200

# SUBMITTED FILE: subtask_c.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : C
ARCHITECTURE :  LSTM
DESCRIPTION : Implementation of the LSTM Classifier with 1 layer.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.577737730039
Overall Accuracy: 0.647887323944

Per-class performance:
             precision    recall  f1-score   samples

        GRP     0.7213    0.5641    0.6331        78
        IND     0.6949    0.8200    0.7523       100
        OTH     0.3529    0.3429    0.3478        35

avg / total     0.6484    0.6479    0.6422       213


====

Sub-task C, adrianlwn CodaLab #549201

# SUBMITTED FILE: subtask_c.csv

# DESCRIPTION:
Codalab Competition :
SUBTASK : C
ARCHITECTURE :  RNN
DESCRIPTION : Implementation of the RNN Classifier.

In order to have a numerical representation of the words in the tweets, a word embedding was used forthis project.  An embedding allows a high dimensional representation of the words and their meaning intheir usual context.  The choice was made to use the Global Vectors embedding (GloVe) (1).  GLoVEwas designed to capture a maximum of information inferrable by the juxtaposition of two words.A GloVe embedding which was pre-trained on a twitter dataset is available in several dimensions.  Weuse the 200 dimensions vector for this project.

# RESULTS:

Macro-F1: 0.506025009473
Overall Accuracy: 0.577464788732

Per-class performance:
             precision    recall  f1-score   samples

        GRP     0.5312    0.6538    0.5862        78
        IND     0.7805    0.6400    0.7033       100
        OTH     0.2286    0.2286    0.2286        35

avg / total     0.5985    0.5775    0.5824       213


