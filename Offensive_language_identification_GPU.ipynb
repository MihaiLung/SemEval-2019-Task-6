{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Offensive language identification_GPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "2TUHe_v24LOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c1220d33-2a00-41f7-aafb-d4a75329b3a7"
      },
      "cell_type": "code",
      "source": [
        "! pip install bcolz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.14.6)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CYritSMt3kJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from utils import *\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from os import mkdir\n",
        "from os.path import join, isfile, isdir, exists\n",
        "import bcolz\n",
        "import pickle \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "#from pattern.en import spelling\n",
        "from tqdm import tqdm\n",
        "import ast"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgD6haCe3kJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCvE01y26NKu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XaHuGbPm3kJP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word Embedings : GloVe\n",
        "This Class Loads the GloVe Embeding, processes it, and create a word embedding given the DataLoader."
      ]
    },
    {
      "metadata": {
        "id": "MJbPeSci3kJR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GloVe_embedding(object):\n",
        "    def __init__(self,dim_vect = 25 ):\n",
        "        ########## VARIABLES ##########\n",
        "        self.dim_vect = dim_vect\n",
        "\n",
        "        # Defining variables for GloVe: \n",
        "        self.words = []\n",
        "        self.word2idx = {}\n",
        "        self.glove_dict = {}\n",
        "        \n",
        "        ########## LOADING GLOVE DATA ##########\n",
        "        \n",
        "        # Defining path for GloVe Data : \n",
        "        self.path = join('..','data','glove') # Path of glove\n",
        "        self.path_glove = join(self.path,'glove.twitter.27B.'+str(dim_vect))\n",
        "        if not(isdir(self.path_glove)):\n",
        "            mkdir(self.path_glove)\n",
        "        self.path_vec_original = join(self.path,'glove.twitter.27B.'+str(dim_vect)+'d.txt') # Path of glove original vectors\n",
        "        self.path_vec_save = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.vectors.dat')  # Path of glove saved vectors\n",
        "        self.path_words = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.words.pkl')\n",
        "        self.path_word2idx = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.word2idx.pkl')\n",
        "                \n",
        "        if not(isdir(self.path_vec_save) and isfile(self.path_words) and isfile(self.path_word2idx)) : \n",
        "            # If files are allready processed, just load them\n",
        "            print('---- Processing the GloVe files : ',end='')\n",
        "            self.process_GloVe()\n",
        "            print('Done')\n",
        "            \n",
        "        # Load the wordvec files\n",
        "        print('---- Loading the processed GloVe files : ',end='')\n",
        "        self.load_GloVe()\n",
        "        print('Done')\n",
        "        \n",
        "        ########## TORCH EMBEDDING ##########\n",
        "        \n",
        "        # Defining variables for our Embedding:\n",
        "        self.size_vocab = len(self.words)\n",
        "        \n",
        "        # Creating the Pytorch Embedding Layer : \n",
        "        print('---- Creating the Pytorch Embedding Layer  : ',end='')\n",
        "        self.emb_layer = nn.Embedding(self.size_vocab, self.dim_vect)\n",
        "        self.create_emb_layer(non_trainable=True)\n",
        "        print('Done')\n",
        "\n",
        "               \n",
        "    def process_GloVe(self):\n",
        "        ''' Processes the GloVe Dataset - Saves files'''\n",
        "        words = []\n",
        "        word2idx = {}\n",
        "        \n",
        "        vectors = bcolz.carray(np.zeros(1) , rootdir=self.path_vec_save , mode='w' ) # defining vector saved\n",
        "        \n",
        "        # Adding Padding vector : \n",
        "        word2idx['<pad>'] = 0\n",
        "        words.append('<pad>')\n",
        "        #vect = np.random.normal(scale=0.6, size=(self.dim_vect , )) # random padding vect\n",
        "        vect = np.zeros((self.dim_vect , )) # 0's padding vect. \n",
        "        vectors.append(vect)\n",
        "        \n",
        "        idx = 1\n",
        "        with open(self.path_vec_original, 'rb') as f:\n",
        "            for l in f:\n",
        "                line = l.decode().split()\n",
        "                word = line[0]\n",
        "                words.append(word)\n",
        "                word2idx[word] = idx\n",
        "                idx += 1\n",
        "                vect = np.array(line[1:]).astype(np.float)\n",
        "                vectors.append(vect)\n",
        "                \n",
        "\n",
        "        vectors = bcolz.carray(vectors[:].reshape((-1, self.dim_vect)), rootdir=self.path_vec_save, mode='w')\n",
        "\n",
        "        vectors.flush()\n",
        "        pickle.dump(words, open(self.path_words, 'wb'))\n",
        "        pickle.dump(word2idx, open(self.path_word2idx, 'wb'))\n",
        "        \n",
        "    def load_GloVe(self):\n",
        "        ''' Loads previously processed dataset'''\n",
        "        \n",
        "        vectors = bcolz.open(self.path_vec_save)[:]\n",
        "        \n",
        "        self.words = pickle.load(open(self.path_words, 'rb'))\n",
        "        self.word2idx = pickle.load(open(self.path_word2idx, 'rb'))\n",
        "        \n",
        "        self.glove_dict = {w: vectors[self.word2idx[w]] for w in self.words}\n",
        "        self.emb_matrix = torch.Tensor(vectors)\n",
        "    \n",
        "    def create_emb_layer(self, non_trainable=True):\n",
        "        self.emb_layer.load_state_dict({'weight': self.emb_matrix})\n",
        "        if non_trainable:\n",
        "            self.emb_layer.weight.requires_grad = False\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BjUfvDiG3kJU",
        "colab_type": "code",
        "outputId": "b4b636f1-6228-461e-b4b8-0c0f69ab6724",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "myEmbedding = GloVe_embedding(dim_vect=25)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Loading the processed GloVe files : Done\n",
            "---- Creating the Pytorch Embedding Layer  : Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JwEkfDbk3kJZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loader\n",
        "This Class Loads the Tweet Dataset, Cleans it. It also enables the loading for the training and testing. \n",
        "TODO : Loading for training and testing"
      ]
    },
    {
      "metadata": {
        "id": "SAZpfe4u3kJb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TestTweetDataset(Dataset):\n",
        "    ''' \n",
        "    Pytorch Dataset for the Test set. \n",
        "    initialisation : - data : training pandas dataframe\n",
        "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
        "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
        "    '''\n",
        "    def __init__(self,data, subtask):\n",
        "        self.id = data.index.tolist()\n",
        "        self.token = data.token.tolist()\n",
        "        self.token_id = data.token_id.tolist()\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.id[index]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token) \n",
        "\n",
        "        \n",
        "    \n",
        "class TweetDataset(Dataset):\n",
        "    ''' \n",
        "    Pytorch Dataset for the Training set. \n",
        "    initialisation : - data : training pandas dataframe\n",
        "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
        "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
        "    '''\n",
        "    def __init__(self,data,subtask):        \n",
        "        # Save in lists the ids, labels, label_id, token, and token_id . \n",
        "        self.id = data.index.tolist()\n",
        "        self.label_id = data[subtask].tolist()\n",
        "        self.token = data.token.tolist()\n",
        "        self.token_id = data.token_id.tolist()\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.label_id[index]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token) \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CudHeQVp3kJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataHandling(object):\n",
        "    def __init__(self, embedding, pValid):\n",
        "        print('-- Data Handling : ')\n",
        "        \n",
        "        self.embedding = embedding\n",
        "        \n",
        "        self.defineClasses()\n",
        "\n",
        "        # All the Text Data path\n",
        "        self.definePath()\n",
        "        \n",
        "        self.data = {}\n",
        "        \n",
        "        processed_ = True\n",
        "        for f in self.path:\n",
        "            processed_ = processed_ and isfile(self.path_clean[f])\n",
        "        \n",
        "        if  not(processed_) : \n",
        "            ### PROCESSING OF THE ORIGINAL DATASET\n",
        "            # Load, Clean and Tokenize the Datasets\n",
        "            print('---- Load, Clean and Tokensize Dataset : ',end='')\n",
        "            self.inital_dataload()\n",
        "            print('Done')\n",
        "            \n",
        "            # Compute List of All words in the datasets\n",
        "            print('---- Finalize tokenized words and translation to id : ',end='')\n",
        "            self.compute_wordlist()\n",
        "            self.token2id()\n",
        "            print('Done')\n",
        "\n",
        "            # Add Embedding and correct clean the words not in embedding : \n",
        "            print('---- Adapt Dataset for Embedding : ',end='')\n",
        "            self.adaptDataset()\n",
        "            print('Done')\n",
        "\n",
        "            # Save the Cleaned Datasets\n",
        "            print('---- Saving all tokenized words : ',end='')\n",
        "            self.save_cleanDataset()\n",
        "            print('Done')\n",
        "        else : \n",
        "            # Save the Cleaned Datasets\n",
        "            print('---- Load the Clean Adapted Dataset : ',end='')\n",
        "            self.load_cleanDataset()\n",
        "            \n",
        "            # Compute List of All words in the datasets\n",
        "            self.compute_wordlist()\n",
        "            print('Done')\n",
        "        \n",
        "        # Create Validation Set (split the test dataset) for every subtask\n",
        "        self.splitValidation(p=pValid)\n",
        "        self.prepareLabels()\n",
        "        \n",
        "        \n",
        "    def defineClasses(self):\n",
        "        ''' Function that defines the classes labels and id per subtask '''\n",
        "        self.classes_dict = {}\n",
        "        self.classes_dict['subtask_a'] = {'NOT' : 0 ,'OFF' : 1}\n",
        "        self.classes_dict['subtask_b'] = {'UNT' : 0 ,'TIN' : 1}\n",
        "        self.classes_dict['subtask_c'] = {'IND' : 0 ,'OTH' : 1, 'GRP' : 2}\n",
        "        \n",
        "    def definePath(self):\n",
        "        ''' Function that defines all the paths of the datasets. '''\n",
        "        self.path = {}\n",
        "        self.path_clean = {}\n",
        "        \n",
        "        self.path['train'] = join('..','data','start-kit','training-v1','offenseval-training-v1.tsv')\n",
        "        self.path_clean['train'] = join('..','data','start-kit','training-v1','clean-offenseval-training-v1.tsv')\n",
        "        \n",
        "        self.path['subtask_a'] = join('..','data','Test A Release','testset-taska.tsv')\n",
        "        self.path_clean['subtask_a'] = join('..','data','Test A Release','clean-testset-taska.tsv')\n",
        "        \n",
        "        self.path['subtask_b'] = join('..','data','Test B Release','testset-taskb.tsv')\n",
        "        self.path_clean['subtask_b'] = join('..','data','Test B Release','clean-testset-taskb.tsv')\n",
        "        \n",
        "        self.path['subtask_c'] = join('..','data','Test C Release','test_set_taskc.tsv')\n",
        "        self.path_clean['subtask_c'] = join('..','data','Test C Release','clean-test_set_taskc.tsv')\n",
        "        \n",
        "    def getDataset(self, dataT='train',subtask='subtask_a',balanced = True):\n",
        "        ''' Returns the pytorch Dataset\n",
        "            - file : {'train','test','validation'}\n",
        "            - subtask : {'subtask_a','subtask_b','subtask_c'} '''\n",
        "        \n",
        "            \n",
        "        if dataT == 'train':\n",
        "            if balanced : \n",
        "                data_train = self.balanceData(self.data[dataT][subtask],subtask)\n",
        "            else : \n",
        "                data_train = self.data[dataT][subtask]\n",
        "            dataset = TweetDataset(data_train, subtask)\n",
        "        elif dataT == 'validation':\n",
        "            dataset = TweetDataset(self.data[dataT][subtask], subtask)\n",
        "        elif dataT == 'test':\n",
        "            dataset = TestTweetDataset(self.data[subtask], subtask)\n",
        "            \n",
        "        return dataset\n",
        "    \n",
        "    def token2id(self):\n",
        "        ''' Function that translates the list of tokens into a list of token id of the embedding.\n",
        "            Adds a new 'token_id' column to the dataframe '''\n",
        "        for f in self.path : \n",
        "            def token2id_x(x):\n",
        "                \n",
        "                return [self.embedding.word2idx[k] for k in x if k in self.embedding.words]\n",
        "            self.data[f]['token_id'] = self.data[f]['token'].apply(lambda x : token2id_x(x))\n",
        "\n",
        "    def save_cleanDataset(self):\n",
        "        ''' Saves at the defined path the cleaned dataset '''\n",
        "        for f in self.path : \n",
        "            self.data[f].to_csv(self.path_clean[f])\n",
        "        \n",
        "    def load_cleanDataset(self):\n",
        "        ''' Loads at the defined path the cleaned dataset '''\n",
        "        for f in self.path : \n",
        "            self.data[f] = pd.read_csv(self.path_clean[f],index_col='id')\n",
        "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : ast.literal_eval(x))\n",
        "            self.data[f]['token_id'] = self.data[f]['token_id'].apply(lambda x : ast.literal_eval(x))\n",
        "           \n",
        "               \n",
        "    def adaptDataset(self):\n",
        "        ''' Function that finds all the words which are not in the embedding and tries to \n",
        "            correct them with the pattern.en package by taking the most probable replacement.\n",
        "            If the suggested word in very unlikely, the word is removed from the tweets. \n",
        "        '''\n",
        "        # Find all words wich are not in the Embedding :\n",
        "        missing_words = []\n",
        "        for i, word in enumerate(self.all_words) :\n",
        "            if self.embedding.word2idx.get(word) == None : \n",
        "                missing_words.append(word)\n",
        "        \n",
        "        # Correct if possible the missing_words : \n",
        "        ### We use theshold over which we correct the word. Under which we discard the word\n",
        "        t = 0.5 # threshold\n",
        "        rejected_words = []\n",
        "        corrected_words = {}\n",
        "        for word in tqdm(missing_words) : \n",
        "            suggestion, prob = spelling.suggest(word)[0]\n",
        "            if prob < t : \n",
        "                rejected_words.append(word)\n",
        "            else : \n",
        "                corrected_words[word] = suggestion\n",
        "        \n",
        "        # Modify the Original Datasets with those corrected_words : \n",
        "        for f in self.path : \n",
        "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [corrected_words.get(k,k) for k in x])\n",
        "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [k for k in x if k not in rejected_words ])\n",
        "        nb_rejected = len(rejected_words)\n",
        "        nb_corrected = len(corrected_words)\n",
        "        nb_vocab = len(self.embedding.glove_dict)\n",
        "        p_rejected = 100* nb_rejected / nb_vocab\n",
        "        p_corrected = 100* nb_corrected / nb_vocab\n",
        "        print('---- Words removed   : {0:} / {1:.2f} - {2:} %'.format(nb_rejected,nb_vocab,p_rejected))\n",
        "        print('---- Words corrected : {0:} / {1:.2f} - {2:} %'.format(nb_corrected,nb_vocab,p_corrected))\n",
        "        \n",
        "    def inital_dataload(self):\n",
        "        for f in self.path : \n",
        "            self.data[f] = pd.read_table(self.path[f],index_col='id')\n",
        "            self.data[f]['token'] = self.data[f]['tweet'].apply(lambda x : self.clean_tweet(x))\n",
        "            \n",
        "    def compute_wordlist(self):\n",
        "        self.all_words_freq = {}\n",
        "        self.all_words = []\n",
        "        \n",
        "        for f in self.data : \n",
        "            for i in range(len(self.data[f])):\n",
        "                for e in self.data[f].iloc[i].token:\n",
        "                    self.all_words_freq[e] = 1 + self.all_words_freq.get(e,0)\n",
        "        self.all_words = list(self.all_words_freq.keys())\n",
        "        \n",
        "    def splitValidation(self,p):\n",
        "        ''' Creates the validation set by  taking p % of the train dataset '''\n",
        "        data = self.data['train'].copy()\n",
        "        self.data['train'] = {}\n",
        "        self.data['validation'] = {}\n",
        "\n",
        "        for subtask in self.classes_dict: # per subtask\n",
        "            self.data['train'][subtask] = pd.DataFrame()\n",
        "            self.data['validation'][subtask]= pd.DataFrame()\n",
        "            for label in self.classes_dict[subtask]: #per label in this subtask \n",
        "                data_label =  data[data[subtask]==label]\n",
        "                self.data['train'][subtask] = self.data['train'][subtask].append(data.loc[data_label.index])\n",
        "                nb_valid = int(len(data_label)*p)\n",
        "                # Select randmoly (without repetition) the indexes of the selected vaidation tweets\n",
        "                index_valid = np.random.choice(data_label.index,(nb_valid,),replace=False)\n",
        "                # Add the the selected validation tweets to the new dataframe\n",
        "                self.data['validation'][subtask] = self.data['validation'][subtask].append(self.data['train'][subtask].loc[index_valid,:])\n",
        "                # Drop the selected validation tweets from the training set\n",
        "                self.data['train'][subtask] = self.data['train'][subtask].drop(index = index_valid)\n",
        "                \n",
        "    def prepareLabels(self) : \n",
        "        ''' Transform the labels into classes id '''\n",
        "        for subtask in self.classes_dict: # per subtask\n",
        "            self.data['validation'][subtask][subtask] =self.data['validation'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
        "            self.data['train'][subtask][subtask] = self.data['train'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
        "\n",
        "    def balanceData(self,data,subtask):\n",
        "        ''' Augments the Data given in input in order to balance the dataset'''\n",
        "        class_size = {}\n",
        "        for label in self.classes_dict[subtask]:\n",
        "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
        "        largest_class = max(class_size, key=class_size.get)\n",
        "        print('---- Augmenting the Data : ')\n",
        "        print('Before Augmentation : ',class_size)\n",
        "\n",
        "        for label in self.classes_dict[subtask]:  \n",
        "            if label != largest_class:\n",
        "                id_list = data[data[subtask]==self.classes_dict[subtask][label]].index\n",
        "                nb_augmentation = class_size[largest_class] - class_size[label]\n",
        "                id_augmentation = np.random.choice(id_list, (nb_augmentation,))\n",
        "                data = data.append(data.loc[id_augmentation,:])\n",
        "        # Check if it went well\n",
        "        for label in self.classes_dict[subtask]:\n",
        "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
        "        \n",
        "        print('After Augmentation : ',class_size)\n",
        "        return data\n",
        "    \n",
        "    def clean_tweet(self,text):\n",
        "        ''' Function that is applied to every to tweet in the dataset '''\n",
        "        \n",
        "        # =========== TEXT ===========\n",
        "        # Replace @USER by <user>\n",
        "        text = re.compile(r'@USER').sub(r'<user>',text)\n",
        "\n",
        "        # Replace URL by <url>\n",
        "        text = re.compile(r'URL').sub(r'<url>',text)\n",
        "\n",
        "        # Remove numbers :\n",
        "        text = re.compile(r'[0-9]+').sub(r' ',text)\n",
        "\n",
        "        # Remove some special characters\n",
        "        text = re.compile(r'([\\xa0_\\{\\}\\[\\]¬•$,:;/@#|\\^*%().~`”\"“-])').sub(r' ',text) \n",
        "\n",
        "        # Space the special characters with white spaces\n",
        "        text = re.compile(r'([$&+,:;=?@#|\\'.^*()%!\"’“-])').sub(r' \\1 ',text)\n",
        "        \n",
        "        # Replace some special characters : \n",
        "        replace_dict = {r'&' : 'and' , \n",
        "                        r'\\+' : 'plus'}\n",
        "        for cha in replace_dict:\n",
        "            text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
        "            \n",
        "        # Handle Emoji : translate some and delete the others\n",
        "        text = self.handle_emoji(text)\n",
        "        \n",
        "        # Word delengthening : \n",
        "        text = re.compile(r'(.)\\1{3,}').sub(r'\\1\\1',text)\n",
        "\n",
        "        # Cut the words with caps in them : \n",
        "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
        "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
        "        # =========== TOKENS ===========\n",
        "        # TOKENIZE \n",
        "        text = text.split(' ')\n",
        "\n",
        "        # Remove white spaces tokens\n",
        "        text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
        "\n",
        "        # Remove empty tokens\n",
        "        text = [text[i] for i in range(len(text)) if text[i] != '']\n",
        "\n",
        "        # Remove repetition in tokens (!!! => !)\n",
        "        text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
        "\n",
        "        #  Handle the ALL CAPS Tweets \n",
        "        ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
        "        caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
        "        if caps_r > 0.6 : \n",
        "            text.append('<allcaps>')\n",
        "\n",
        "        # Lower Case : \n",
        "        text = [text[i].lower() for i in range(len(text))]\n",
        "\n",
        "        return text\n",
        "\n",
        "    def handle_emoji(self,text):\n",
        "        # Dictionnary of \"important\" emojis : \n",
        "        emoji_dict =  {'♥️': ' love ',\n",
        "                       '❤️' : ' love ',\n",
        "                       '❤' : ' love ',\n",
        "                       '😘' : ' kisses ',\n",
        "                      '😭' : ' cry ',\n",
        "                      '💪' : ' strong ',\n",
        "                      '🌍' : ' earth ',\n",
        "                      '💰' : ' money ',\n",
        "                      '👍' : ' ok ',\n",
        "                       '👌' : ' ok ',\n",
        "                      '😡' : ' angry ',\n",
        "                      '🍆' : ' dick ',\n",
        "                      '🤣' : ' haha ',\n",
        "                      '😂' : ' haha ',\n",
        "                      '🖕' : ' fuck you '}\n",
        "\n",
        "        for cha in emoji_dict:\n",
        "            text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
        "        # Remove ALL emojis\n",
        "        text = emoji.get_emoji_regexp().sub(r' ',text) \n",
        "        text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
        "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
        "        text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
        "\n",
        "        # Add Space between  the Emoji Expressions : \n",
        "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0f2pJn13kJh",
        "colab_type": "code",
        "outputId": "7ffa0820-4b56-4a5c-aa53-b4130370a140",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mydata = DataHandling(myEmbedding, pValid=0.15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Data Handling : \n",
            "---- Load the Clean Adapted Dataset : Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nCYdTDX03kJl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = mydata.getDataset(dataT='validation',subtask='subtask_c')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qxvQf9AH3kJp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IkPjJ7Kx3kJr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier \n",
        "Set of Classes used as classifier for the tweets. "
      ]
    },
    {
      "metadata": {
        "id": "NHs2iTwV3kJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Classification NN : \n",
        "class FFNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
        "        print('------ Creating FFNN : ',end='')\n",
        "        \n",
        "        super(FFNN, self).__init__()\n",
        "        \n",
        "        # Embedding\n",
        "        self.embedding = embedding\n",
        "        \n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "        # Activation Layers\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.output = nn.LogSoftmax(dim=1)\n",
        "        print('Done')\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "        # we average the embeddings of words in a sentence\n",
        "        \n",
        "        non_zero_nb = (x!=0).sum(1,keepdim=True)\n",
        "        #print(x.shape, non_zero_nb,embedded.sum(1).shape)\n",
        "        averaged = embedded.sum(1) / non_zero_nb.float()\n",
        "        #averaged = embedded.mean(1)\n",
        "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
        "\n",
        "        out = self.fc1(averaged)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.output(out)\n",
        "        return out\n",
        "    \n",
        "    def loss_fn(self):\n",
        "        ''' Returns the loss function best associated with the model'''\n",
        "        return nn.NLLLoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fjriTHog3kJw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN : \n",
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, embedding, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.embedding = embedding\n",
        "        \n",
        "        #in_channels -- 1 text channel\n",
        "        #out_channels -- the number of output channels\n",
        "        #kernel_size is (window size x embedding dim)\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
        "        \n",
        "        #the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "        #the output layer\n",
        "        self.fc = nn.Linear(out_channels, output_dim)\n",
        "        \n",
        "        self.out =  nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "                \n",
        "        #(batch size, max sent length)\n",
        "        \n",
        "        embedded = self.embedding(x)\n",
        "        #print('embedded',embedded.shape)\n",
        "                \n",
        "        #(batch size, max sent length, embedding dim)\n",
        "        \n",
        "        #images have 3 RGB channels \n",
        "        #for the text we add 1 channel\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        #print('embedded_unsqueeze',embedded.shape)\n",
        "        \n",
        "        #(batch size, 1, max sent length, embedding dim)\n",
        "        \n",
        "        feature_maps = self.conv(embedded)\n",
        "        #print('feature_maps',feature_maps.shape)\n",
        "\n",
        "        #??? what is the shape of the convolution output\n",
        "        \n",
        "        #(batch size, n filters, max input length - window size +1, 1)\n",
        "        \n",
        "        feature_maps = feature_maps.squeeze(3)\n",
        "        #print('feature_maps_unsqueeze',feature_maps.shape)\n",
        "\n",
        "        #??? why do we reduce 1 dimention here\n",
        "        \n",
        "        # we do need the 1 channel anymore\n",
        "                \n",
        "        feature_maps = F.relu(feature_maps)\n",
        "        #print('feature_maps_relu',feature_maps.shape)\n",
        "\n",
        "  \n",
        "        #the max pooling layer\n",
        "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
        "        #print('pooled',pooled.shape)\n",
        "\n",
        "        pooled = pooled.squeeze(2)\n",
        "        #print('pooled_squeeze',pooled.shape)\n",
        "\n",
        "        #??? what is the shape of the pooling output\n",
        "        #(batch size, n_filters)\n",
        "        \n",
        "        dropped = self.dropout(pooled)\n",
        "        #print('dropped',dropped.shape)\n",
        "\n",
        "        preds = self.fc(dropped)\n",
        "        #print('preds', preds.shape)\n",
        "        out = self.out(preds)\n",
        "        #print('out',out.shape)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def loss_fn(self):\n",
        "        return nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFKLQ_2L3kJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RNN, GRU and LSTM\n",
        "class RNN_base(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
        "        super(RNN_base, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        \n",
        "        if output_layer==None:\n",
        "            self.i2o = nn.Linear(hidden_size, output_size)\n",
        "        else:\n",
        "            self.i2o = output_layer\n",
        "        self.final_activation = nn.LogSoftmax(dim=1)\n",
        "        self.final_activation = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        inputs_embedded = self.embedding(inputs).float()\n",
        " \n",
        "        sentence_lengths = (inputs_embedded.sum(dim=2,keepdim=False)!=0).sum(dim=1)\n",
        "        \n",
        "        output, hidden = self.rnn(inputs_embedded)\n",
        "        \n",
        "        output_final = torch.empty(inputs.shape[0],self.hidden_size).float()\n",
        "        output_final = output[torch.arange(inputs.shape[0]),sentence_lengths-1,:]\n",
        "\n",
        "        output = self.i2o(output_final)\n",
        "        output = self.final_activation(output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def loss_fn(self):\n",
        "        ''' Returns the loss function best associated with the model'''\n",
        "        return nn.NLLLoss()\n",
        "    \n",
        "class RNN(RNN_base):\n",
        "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
        "        RNN_base.__init__(self, \n",
        "                          embedding=embedding, \n",
        "                          embedding_dim=embedding_dim, \n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=output_size, \n",
        "                          num_layers=num_layers, \n",
        "                          output_layer=output_layer)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size = embedding_dim, \n",
        "            hidden_size = hidden_size,\n",
        "            num_layers = num_layers,\n",
        "            nonlinearity = \"relu\",\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "class LSTM(RNN_base):\n",
        "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
        "        RNN_base.__init__(self, \n",
        "                          embedding=embedding, \n",
        "                          embedding_dim=embedding_dim, \n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=output_size, \n",
        "                          num_layers=num_layers, \n",
        "                          output_layer=output_layer)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size = embedding_dim, \n",
        "            hidden_size = hidden_size,\n",
        "            num_layers = num_layers,\n",
        "            batch_first = True\n",
        "        )  \n",
        "        \n",
        "class GRU(RNN_base):\n",
        "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
        "        RNN_base.__init__(self, \n",
        "                          embedding=embedding, \n",
        "                          embedding_dim=embedding_dim, \n",
        "                          hidden_size=hidden_size, \n",
        "                          output_size=output_size, \n",
        "                          num_layers=num_layers, \n",
        "                          output_layer=output_layer)\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size = embedding_dim, \n",
        "            hidden_size = hidden_size,\n",
        "            num_layers = num_layers,\n",
        "            batch_first = True\n",
        "        ) \n",
        "        \n",
        "def output_layer(input_size,output_size,middle_layers=[]):\n",
        "    \"Input and output size - scalars. Middle layers - list (ignore if just one layer wanted)\"\n",
        "    \n",
        "    #For current implementation, the input_size should be equal to the size of the hidden layer for the RNNs.\n",
        "    activation = nn.ReLU()\n",
        "\n",
        "    component_layers=[]\n",
        "    if middle_layers!=[]:\n",
        "\n",
        "        component_layers.append(\n",
        "            nn.Linear(\n",
        "                in_features  = input_size,\n",
        "                out_features  = middle_layers[0]))\n",
        "        component_layers.append(activation)\n",
        "\n",
        "\n",
        "        if len(middle_layers)>1:\n",
        "            for layer in range(1,len(middle_layers)):\n",
        "                component_layers.append(\n",
        "                    nn.Linear(in_features  = middle_layers[layer-1],\n",
        "                              out_features  = middle_layers[layer]))\n",
        "                component_layers.append(activation)\n",
        "\n",
        "        component_layers.append(\n",
        "            nn.Linear(in_features  = middle_layers[-1],\n",
        "                      out_features  = output_size))\n",
        "\n",
        "    else:\n",
        "        component_layers.append(\n",
        "            nn.Linear(in_features  = input_size,\n",
        "                      out_features  = output_size))\n",
        "\n",
        "    i2o = nn.Sequential(*component_layers)\n",
        "    \n",
        "    return i2o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dEYQKa13kJ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. "
      ]
    },
    {
      "metadata": {
        "id": "FArD3qIF3kJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class padding_tweet:\n",
        "    '''\n",
        "    Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. \n",
        "    args: \n",
        "        batch - List of elements ( x , label )\n",
        "    return \n",
        "        batch - Padded ( list(x) , list(label))\n",
        "    \n",
        "    '''\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "    def __call__(self,batch):\n",
        "        \n",
        "        batch = list(zip(*batch))\n",
        "       \n",
        "        batch[0] = torch.stack([self.pad_tensor(vec=t, pad=self.max_len, dim=0) for t in batch[0]],dim=0)\n",
        "        batch[1] = torch.stack(batch[1])\n",
        "        return batch[0] , batch[1]\n",
        "\n",
        "    def pad_tensor(self,vec, pad, dim):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            vec - tensor to pad\n",
        "            pad - the size to pad to\n",
        "            dim - dimension to pad\n",
        "\n",
        "        return:\n",
        "            a new tensor padded to 'pad' in dimension 'dim'\n",
        "        \"\"\"\n",
        "        pad_size = list(vec.shape)\n",
        "        pad_size[dim] = pad - vec.size(dim)\n",
        "        return torch.cat([vec, torch.zeros(*pad_size,dtype=torch.long)], dim=dim)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ql4Aim1m3kJ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Main Class : Trainer\n",
        "Main Class for the loading, training, testing etc ..."
      ]
    },
    {
      "metadata": {
        "id": "0Q50IDC93kJ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class OffensiveClassifier(object):\n",
        "    ''' Main Class for the loading, training, testing etc ...'''\n",
        "    def __init__(self,subtask='subtask_a', dim_vect=25,pValid = 0.15):\n",
        "        \n",
        "        self.dim_vect = dim_vect\n",
        "        \n",
        "        self.subtask = subtask\n",
        "        \n",
        "\n",
        "        # Loading the GloVe Embedding and Torch Formating of this Embedding\n",
        "        self.GloVe = GloVe_embedding(dim_vect= dim_vect )\n",
        "        self.embedding = self.GloVe.emb_layer\n",
        "        \n",
        "        # Loading the Data Handler : \n",
        "        self.dataHandler = DataHandling(self.GloVe,pValid=pValid)\n",
        "        \n",
        "        # Retrieving Training DataSet (pytorch)\n",
        "        self.train_set = self.dataHandler.getDataset('train',subtask,balanced=True)\n",
        "        \n",
        "        # Retrieving the Validation Set (pytorch)\n",
        "        self.valid_set = self.dataHandler.getDataset('validation',subtask)\n",
        "\n",
        "        # Retrieving Test DataSet (pytorch)\n",
        "        self.test_set = self.dataHandler.getDataset('test',subtask)\n",
        "        \n",
        "        # Usefull info on classes : \n",
        "        self.class2id  = self.dataHandler.classes_dict[self.subtask]\n",
        "        self.id2class = dict(zip(self.class2id.values(),self.class2id.keys()))\n",
        "        self.nb_class = len(self.class2id)\n",
        "        self.max_len  = max([ max([len(t) for t in self.train_set.token]),\n",
        "                              max([len(t) for t in self.valid_set.token]),\n",
        "                              max([len(t) for t in self.test_set.token])])\n",
        "            \n",
        "        \n",
        "    \n",
        "    def train( self, nb_epochs, optimizer , model , batch_size = 1000 ,  ):\n",
        "        \n",
        "        self.train_generator = DataLoader(self.train_set, batch_size=batch_size,collate_fn=padding_tweet(self.max_len), shuffle=True)\n",
        "\n",
        "        self.model = model.to(device)\n",
        "        loss_fn = self.model.loss_fn()\n",
        "\n",
        "        for epoch in range(nb_epochs):\n",
        "            i_batch = 0\n",
        "            epoch_correct = 0\n",
        "            self.model.train() \n",
        "            \n",
        "            for tokens, target  in self.train_generator :\n",
        "                i_batch += 1\n",
        "                target = target.long().view((-1,)).to(device)\n",
        "                tokens = tokens.long().to(device)\n",
        "                #to ensure the dropout (exlained later) is \"turned on\" while training\n",
        "                #good practice to include even if do not use here\n",
        "                self.model.train()\n",
        "\n",
        "                #we zero the gradients as they are not removed automatically\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output = self.model(tokens)\n",
        "                #print(output.shape)\n",
        "\n",
        "                predictions = torch.argmax(output,dim=1).float()\n",
        "                loss = loss_fn(output, target)\n",
        "                acc, correct = self.accuracy(predictions, target)\n",
        "                \n",
        "                #calculate the gradient of each parameter\n",
        "                loss.backward()\n",
        "                \n",
        "                #update the parameters using the gradients and optimizer algorithm \n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss = loss.item()\n",
        "\n",
        "                if i_batch % (nb_epochs -1 )  ==0:\n",
        "                    pass\n",
        "            print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {acc*100:.2f}%')\n",
        "            self.validation(batch_size = 50)\n",
        "            print('=============================================================================')\n",
        "            print()\n",
        "            \n",
        "    def validation(self, batch_size = 1000):\n",
        "        all_correct = 0\n",
        "        self.validation_generator = DataLoader(self.valid_set, batch_size=batch_size, collate_fn=padding_tweet(self.max_len), shuffle=True)\n",
        "        loss_fn = self.model.loss_fn()\n",
        "        nb_valid = len(taskAclassifier.valid_set)\n",
        "        self.model.eval()  # set model to evaluation mode\n",
        "        all_prediction = []\n",
        "        all_target = []\n",
        "        \n",
        "        with torch.no_grad(): \n",
        "            for tokens, target  in self.validation_generator :\n",
        "                target = target.long().view((-1,))\n",
        "                tokens = tokens.long()\n",
        "                \n",
        "                output = self.model(tokens)\n",
        "                predictions = torch.argmax(output,dim=1)\n",
        "                \n",
        "                loss = loss_fn(output, target)\n",
        "                \n",
        "                all_prediction.extend(predictions.view(-1,).tolist())\n",
        "                all_target.extend(target.view(-1,).tolist())\n",
        "                \n",
        "        accuracy, CM, stats_df = self.results(all_prediction,all_target)\n",
        "        \n",
        "        print(f'| Validation Accuracy : {100*accuracy:.2f} %')\n",
        "        print(f'| Stats on the Validation : ')\n",
        "        print(stats_df)\n",
        "        print()\n",
        "        print(f'| CM on the Validation : ')\n",
        "        print(CM)\n",
        "        print()\n",
        "        \n",
        "    def test(self):\n",
        "        ''' \n",
        "            Test Function : Tests the Network on the Test Data of the Subtask and Saves in a file\n",
        "        '''\n",
        "        test_result = pd.DataFrame()\n",
        "        self.test_generator = DataLoader(self.test_set,collate_fn= padding_tweet(self.max_len) )\n",
        "        nb_valid = len(taskAclassifier.valid_set)\n",
        "        self.model.eval()  # set model to evaluation mode\n",
        "        all_prediction = []\n",
        "        all_id = []\n",
        "        with torch.no_grad(): \n",
        "            for tokens, id_tweet  in self.test_generator :\n",
        "                id_tweet = id_tweet.long().view((-1,))\n",
        "                tokens = tokens.long()\n",
        "                \n",
        "                output = self.model(tokens)\n",
        "                predictions = torch.argmax(output,dim=1)\n",
        "                \n",
        "                \n",
        "                all_prediction.extend(predictions.view(-1,).tolist())\n",
        "                all_id.extend(id_tweet.view(-1,).tolist())\n",
        "        results_df = pd.DataFrame(all_prediction,index=all_id)\n",
        "        save_path = join('..','data','results')\n",
        "        mkdir(save_path)\n",
        "        results_df.to_csv(join(save_path,self.subtask + '.csv'), header=False)\n",
        "        \n",
        "\n",
        "    def accuracy(self, output, target):\n",
        "        target = np.array(target.tolist()).astype(int)\n",
        "        output = np.array(torch.round(output).tolist()).astype(int)\n",
        "        correct = (output == target)\n",
        "        accuracy = correct.sum()/len(correct)\n",
        "        return accuracy, correct\n",
        "        \n",
        "    def results(self, output, target ):\n",
        "        target = np.array(target).astype(int)\n",
        "        output = np.round(output).astype(int)\n",
        "        correct = (output == target)\n",
        "        accuracy = correct.sum()/len(correct)\n",
        "        \n",
        "        CM = np.zeros((self.nb_class,self.nb_class))\n",
        "        for i in range(len(output)): \n",
        "            CM[target[i], output[i]] += 1\n",
        "        \n",
        "        # compute the interesting stats on the classification : \n",
        "        stats_df = self.stats(CM)\n",
        "        stats_df.columns = map(lambda x : self.id2class[x] ,stats_df.columns)\n",
        "        return (accuracy, CM, stats_df)\n",
        "\n",
        "    def stats(self,CM):   \n",
        "        n_class = self.nb_class\n",
        "    \n",
        "        stats = {}\n",
        "        stats['precision'] = {} \n",
        "        stats['recall'] = {} \n",
        "        stats['accuracy'] = {} \n",
        "        stats['f1-measure'] = {} \n",
        "\n",
        "        for t in range(self.nb_class):\n",
        "            tp = CM[t,t]\n",
        "            tn = np.sum([CM[i,i] for i in range(self.nb_class) if i != t])\n",
        "\n",
        "            fp = np.sum([CM[i,t] for i in range(self.nb_class) if i != t])\n",
        "            fn = np.sum([CM[t,i] for i in range(self.nb_class) if i != t])\n",
        "\n",
        "            # compute accuracy per class :\n",
        "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "            # compute recall per class\n",
        "            recall = tp /(tp + fn)\n",
        "\n",
        "            # compute precision per class \n",
        "            precision = tp / (tp + fp)\n",
        "\n",
        "            # compute F1-measure per class \n",
        "            f1 = 2*tp / (2* tp + fp + fn)\n",
        "\n",
        "            # saving stats : \n",
        "\n",
        "            stats['precision'][t] = precision\n",
        "            stats['recall'][t] = recall\n",
        "            stats['accuracy'][t] = accuracy\n",
        "            stats['f1-measure'][t] = f1\n",
        "\n",
        "        # Compute Average classification rate\n",
        "        stats = pd.DataFrame(stats).transpose()\n",
        "        return stats\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkfM8PQD3kKB",
        "colab_type": "code",
        "outputId": "cc15ddea-6609-4e12-e9f8-6d65745198ed",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1st step : Define Classifier for specific task\n",
        "taskAclassifier = OffensiveClassifier(subtask='subtask_a', dim_vect=100 , pValid = 0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Loading the processed GloVe files : Done\n",
            "---- Creating the Pytorch Embedding Layer  : Done\n",
            "-- Data Handling : \n",
            "---- Load the Clean Adapted Dataset : Done\n",
            "---- Augmenting the Data : \n",
            "Before Augmentation :  {'NOT': 7072, 'OFF': 3520}\n",
            "After Augmentation :  {'NOT': 7072, 'OFF': 7072}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qZtzNykf3kKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2nd Step : Define Model : \n",
        "\n",
        "#FFNN_multiDim\n",
        "'''model = FFNN(taskAclassifier.embedding, 100 , taskAclassifier.nb_class, taskAclassifier.dim_vect)\n",
        "'''\n",
        "#CNN\n",
        "'''model = CNN( embedding= taskAclassifier.embedding, \n",
        "             embedding_dim = taskAclassifier.dim_vect,\n",
        "             out_channels= 100,\n",
        "             window_size= 3,\n",
        "             output_dim= taskAclassifier.nb_class,\n",
        "             dropout = 0.5)'''\n",
        "#LSTM\n",
        "\n",
        "model = LSTM(embedding= taskAclassifier.embedding, \n",
        "            embedding_dim = taskAclassifier.dim_vect,\n",
        "            hidden_size=taskAclassifier.dim_vect+50, \n",
        "            output_size=taskAclassifier.nb_class,\n",
        "            num_layers=1,\n",
        "            output_layer = output_layer(taskAclassifier.dim_vect+50, taskAclassifier.nb_class,\n",
        "                middle_layers = [])) #Change this to add/remove middle layers from output\n",
        "\n",
        "#RNN\n",
        "\"\"\"\n",
        "model = RNN(embedding= taskAclassifier.embedding,  \n",
        "            embedding_dim = taskAclassifier.dim_vect,\n",
        "            hidden_size=taskAclassifier.dim_vect, \n",
        "            output_size=taskAclassifier.nb_class,\n",
        "            num_layers=2,\n",
        "            output_layer = output_layer(taskAclassifier.dim_vect, taskAclassifier.nb_class,\n",
        "                middle_layers = [50])) #Change this to add/remove middle layers from output\n",
        "\n",
        "\"\"\"\n",
        "#GRU\n",
        "'''\n",
        "model = GRU(embedding= taskAclassifier.embedding,  \n",
        "            embedding_dim = taskAclassifier.dim_vect,\n",
        "            hidden_size=100, \n",
        "            output_size=taskAclassifier.nb_class,\n",
        "            num_layers=2,\n",
        "            output_layer = output_layer(taskAclassifier.dim_vect, taskAclassifier.nb_class,\n",
        "                middle_layers = [50])) #Change this to add/remove middle layers from output\n",
        "\n",
        "'''\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37oRMusp3kKN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3rd Step : Define Optinizer : \n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "b9JMlJQk3kKS",
        "colab_type": "code",
        "outputId": "29855a1d-32fe-4ed4-ea5a-7f9c0ee74e28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 4th Step : Train the Model\n",
        "taskAclassifier.train( nb_epochs= 500,\n",
        "                       optimizer= optimizer , \n",
        "                       model= model )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 00 | Train Loss: 0.694 | Train Acc: 50.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/adrian/anaconda3/envs/ic_std/lib/python3.6/site-packages/ipykernel/__main__.py:185: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Validation Accuracy : 33.23 %\n",
            "| Stats on the Validation : \n",
            "                 NOT       OFF\n",
            "precision        NaN  0.332326\n",
            "recall      0.000000  1.000000\n",
            "accuracy    0.332326  0.332326\n",
            "f1-measure  0.000000  0.498866\n",
            "\n",
            "| CM on the Validation : \n",
            "[[   0. 1768.]\n",
            " [   0.  880.]]\n",
            "\n",
            "=============================================================================\n",
            "\n",
            "| Epoch: 01 | Train Loss: 0.693 | Train Acc: 50.69%\n",
            "| Validation Accuracy : 66.77 %\n",
            "| Stats on the Validation : \n",
            "                 NOT       OFF\n",
            "precision   0.667674       NaN\n",
            "recall      1.000000  0.000000\n",
            "accuracy    0.667674  0.667674\n",
            "f1-measure  0.800725  0.000000\n",
            "\n",
            "| CM on the Validation : \n",
            "[[1768.    0.]\n",
            " [ 880.    0.]]\n",
            "\n",
            "=============================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wfzchoAj3kKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 5th Step : Test the Model and save data \n",
        "taskAclassifier.test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B9krVmx73kKZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LP0NYprr3kKb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unQNaplW3kKd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iAMT7zGq3kKg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-D3fKH03kKi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}