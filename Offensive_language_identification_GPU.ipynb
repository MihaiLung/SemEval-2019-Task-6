{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYritSMt3kJA"
   },
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import mkdir\n",
    "from os.path import join, isfile, isdir, exists\n",
    "import bcolz\n",
    "import pickle \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "#from pattern.en import spelling\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgD6haCe3kJE"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCvE01y26NKu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "GPU = True\n",
    "device_idx = 0\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaHuGbPm3kJP"
   },
   "source": [
    "## Word Embedings : GloVe\n",
    "This Class Loads the GloVe Embeding, processes it, and create a word embedding given the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJbPeSci3kJR"
   },
   "outputs": [],
   "source": [
    "class GloVe_embedding(object):\n",
    "    def __init__(self,dim_vect = 25 ):\n",
    "        ########## VARIABLES ##########\n",
    "        self.dim_vect = dim_vect\n",
    "\n",
    "        # Defining variables for GloVe: \n",
    "        self.words = []\n",
    "        self.word2idx = {}\n",
    "        self.glove_dict = {}\n",
    "        \n",
    "        ########## LOADING GLOVE DATA ##########\n",
    "        \n",
    "        # Defining path for GloVe Data : \n",
    "        self.path = join('..','data','glove') # Path of glove\n",
    "        self.path_glove = join(self.path,'glove.twitter.27B.'+str(dim_vect))\n",
    "        if not(isdir(self.path_glove)):\n",
    "            mkdir(self.path_glove)\n",
    "        self.path_vec_original = join(self.path,'glove.twitter.27B.'+str(dim_vect)+'d.txt') # Path of glove original vectors\n",
    "        self.path_vec_save = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.vectors.dat')  # Path of glove saved vectors\n",
    "        self.path_words = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.words.pkl')\n",
    "        self.path_word2idx = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.word2idx.pkl')\n",
    "                \n",
    "        if not(isdir(self.path_vec_save) and isfile(self.path_words) and isfile(self.path_word2idx)) : \n",
    "            # If files are allready processed, just load them\n",
    "            print('---- Processing the GloVe files : ',end='')\n",
    "            self.process_GloVe()\n",
    "            print('Done')\n",
    "            \n",
    "        # Load the wordvec files\n",
    "        print('---- Loading the processed GloVe files : ',end='')\n",
    "        self.load_GloVe()\n",
    "        print('Done')\n",
    "        \n",
    "        ########## TORCH EMBEDDING ##########\n",
    "        \n",
    "        # Defining variables for our Embedding:\n",
    "        self.size_vocab = len(self.words)\n",
    "        \n",
    "        # Creating the Pytorch Embedding Layer : \n",
    "        print('---- Creating the Pytorch Embedding Layer  : ',end='')\n",
    "        self.emb_layer = nn.Embedding(self.size_vocab, self.dim_vect)\n",
    "        self.create_emb_layer(non_trainable=True)\n",
    "        print('Done')\n",
    "\n",
    "               \n",
    "    def process_GloVe(self):\n",
    "        ''' Processes the GloVe Dataset - Saves files'''\n",
    "        words = []\n",
    "        word2idx = {}\n",
    "        \n",
    "        vectors = bcolz.carray(np.zeros(1) , rootdir=self.path_vec_save , mode='w' ) # defining vector saved\n",
    "        \n",
    "        # Adding Padding vector : \n",
    "        word2idx['<pad>'] = 0\n",
    "        words.append('<pad>')\n",
    "        #vect = np.random.normal(scale=0.6, size=(self.dim_vect , )) # random padding vect\n",
    "        vect = np.zeros((self.dim_vect , )) # 0's padding vect. \n",
    "        vectors.append(vect)\n",
    "        \n",
    "        idx = 1\n",
    "        with open(self.path_vec_original, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "                \n",
    "\n",
    "        vectors = bcolz.carray(vectors[:].reshape((-1, self.dim_vect)), rootdir=self.path_vec_save, mode='w')\n",
    "\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(self.path_words, 'wb'))\n",
    "        pickle.dump(word2idx, open(self.path_word2idx, 'wb'))\n",
    "        \n",
    "    def load_GloVe(self):\n",
    "        ''' Loads previously processed dataset'''\n",
    "        \n",
    "        vectors = bcolz.open(self.path_vec_save)[:]\n",
    "        \n",
    "        self.words = pickle.load(open(self.path_words, 'rb'))\n",
    "        self.word2idx = pickle.load(open(self.path_word2idx, 'rb'))\n",
    "        \n",
    "        self.glove_dict = {w: vectors[self.word2idx[w]] for w in self.words}\n",
    "        self.emb_matrix = torch.Tensor(vectors)\n",
    "    \n",
    "    def create_emb_layer(self, non_trainable=True):\n",
    "        self.emb_layer.load_state_dict({'weight': self.emb_matrix})\n",
    "        if non_trainable:\n",
    "            self.emb_layer.weight.requires_grad = False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjUfvDiG3kJU",
    "outputId": "b4b636f1-6228-461e-b4b8-0c0f69ab6724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n"
     ]
    }
   ],
   "source": [
    "myEmbedding = GloVe_embedding(dim_vect=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwEkfDbk3kJZ"
   },
   "source": [
    "## Data Loader\n",
    "This Class Loads the Tweet Dataset, Cleans it. It also enables the loading for the training and testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAZpfe4u3kJb"
   },
   "outputs": [],
   "source": [
    "class TestTweetDataset(Dataset):\n",
    "    ''' \n",
    "    Pytorch Dataset for the Test set. \n",
    "    initialisation : - data : training pandas dataframe\n",
    "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
    "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
    "    '''\n",
    "    def __init__(self,data, subtask):\n",
    "        self.id = data.index.tolist()\n",
    "        self.token = data.token.tolist()\n",
    "        self.token_id = data.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "\n",
    "        \n",
    "    \n",
    "class TweetDataset(Dataset):\n",
    "    ''' \n",
    "    Pytorch Dataset for the Training set. \n",
    "    initialisation : - data : training pandas dataframe\n",
    "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
    "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
    "    '''\n",
    "    def __init__(self,data,subtask):        \n",
    "        # Save in lists the ids, labels, label_id, token, and token_id . \n",
    "        self.id = data.index.tolist()\n",
    "        self.label_id = data[subtask].tolist()\n",
    "        self.token = data.token.tolist()\n",
    "        self.token_id = data.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.label_id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CudHeQVp3kJe"
   },
   "outputs": [],
   "source": [
    "class DataHandling(object):\n",
    "    def __init__(self, embedding, pValid):\n",
    "        print('-- Data Handling : ')\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.defineClasses()\n",
    "\n",
    "        # All the Text Data path\n",
    "        self.definePath()\n",
    "        \n",
    "        self.data = {}\n",
    "        \n",
    "        processed_ = True\n",
    "        for f in self.path:\n",
    "            processed_ = processed_ and isfile(self.path_clean[f])\n",
    "        \n",
    "        if  not(processed_) : \n",
    "            ### PROCESSING OF THE ORIGINAL DATASET\n",
    "            # Load, Clean and Tokenize the Datasets\n",
    "            print('---- Load, Clean and Tokensize Dataset : ',end='')\n",
    "            self.inital_dataload()\n",
    "            print('Done')\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            print('---- Finalize tokenized words and translation to id : ',end='')\n",
    "            self.compute_wordlist()\n",
    "            self.token2id()\n",
    "            print('Done')\n",
    "\n",
    "            # Add Embedding and correct clean the words not in embedding : \n",
    "            print('---- Adapt Dataset for Embedding : ',end='')\n",
    "            self.adaptDataset()\n",
    "            print('Done')\n",
    "\n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Saving all tokenized words : ',end='')\n",
    "            self.save_cleanDataset()\n",
    "            print('Done')\n",
    "        else : \n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Load the Clean Adapted Dataset : ',end='')\n",
    "            self.load_cleanDataset()\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            self.compute_wordlist()\n",
    "            print('Done')\n",
    "        \n",
    "        # Create Validation Set (split the test dataset) for every subtask\n",
    "        self.splitValidation(p=pValid)\n",
    "        self.prepareLabels()\n",
    "        \n",
    "        \n",
    "    def defineClasses(self):\n",
    "        ''' Function that defines the classes labels and id per subtask '''\n",
    "        self.classes_dict = {}\n",
    "        self.classes_dict['subtask_a'] = {'NOT' : 0 ,'OFF' : 1}\n",
    "        self.classes_dict['subtask_b'] = {'UNT' : 0 ,'TIN' : 1}\n",
    "        self.classes_dict['subtask_c'] = {'IND' : 0 ,'OTH' : 1, 'GRP' : 2}\n",
    "        \n",
    "    def definePath(self):\n",
    "        ''' Function that defines all the paths of the datasets. '''\n",
    "        self.path = {}\n",
    "        self.path_clean = {}\n",
    "        \n",
    "        self.path['train'] = join('..','data','start-kit','training-v1','offenseval-training-v1.tsv')\n",
    "        self.path_clean['train'] = join('..','data','start-kit','training-v1','clean-offenseval-training-v1.tsv')\n",
    "        \n",
    "        self.path['subtask_a'] = join('..','data','Test A Release','testset-taska.tsv')\n",
    "        self.path_clean['subtask_a'] = join('..','data','Test A Release','clean-testset-taska.tsv')\n",
    "        \n",
    "        self.path['subtask_b'] = join('..','data','Test B Release','testset-taskb.tsv')\n",
    "        self.path_clean['subtask_b'] = join('..','data','Test B Release','clean-testset-taskb.tsv')\n",
    "        \n",
    "        self.path['subtask_c'] = join('..','data','Test C Release','test_set_taskc.tsv')\n",
    "        self.path_clean['subtask_c'] = join('..','data','Test C Release','clean-test_set_taskc.tsv')\n",
    "        \n",
    "    def getDataset(self, dataT='train',subtask='subtask_a',balanced = True):\n",
    "        ''' Returns the pytorch Dataset\n",
    "            - file : {'train','test','validation'}\n",
    "            - subtask : {'subtask_a','subtask_b','subtask_c'} '''\n",
    "        \n",
    "            \n",
    "        if dataT == 'train':\n",
    "            if balanced : \n",
    "                data_train = self.balanceData(self.data[dataT][subtask],subtask)\n",
    "            else : \n",
    "                data_train = self.data[dataT][subtask]\n",
    "            dataset = TweetDataset(data_train, subtask)\n",
    "        elif dataT == 'validation':\n",
    "            dataset = TweetDataset(self.data[dataT][subtask], subtask)\n",
    "        elif dataT == 'test':\n",
    "            dataset = TestTweetDataset(self.data[subtask], subtask)\n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    def token2id(self):\n",
    "        ''' Function that translates the list of tokens into a list of token id of the embedding.\n",
    "            Adds a new 'token_id' column to the dataframe '''\n",
    "        for f in self.path : \n",
    "            def token2id_x(x):\n",
    "                \n",
    "                return [self.embedding.word2idx[k] for k in x if k in self.embedding.words]\n",
    "            self.data[f]['token_id'] = self.data[f]['token'].apply(lambda x : token2id_x(x))\n",
    "\n",
    "    def save_cleanDataset(self):\n",
    "        ''' Saves at the defined path the cleaned dataset '''\n",
    "        for f in self.path : \n",
    "            self.data[f].to_csv(self.path_clean[f])\n",
    "        \n",
    "    def load_cleanDataset(self):\n",
    "        ''' Loads at the defined path the cleaned dataset '''\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_csv(self.path_clean[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : ast.literal_eval(x))\n",
    "            self.data[f]['token_id'] = self.data[f]['token_id'].apply(lambda x : ast.literal_eval(x))\n",
    "           \n",
    "               \n",
    "    def adaptDataset(self):\n",
    "        ''' Function that finds all the words which are not in the embedding and tries to \n",
    "            correct them with the pattern.en package by taking the most probable replacement.\n",
    "            If the suggested word in very unlikely, the word is removed from the tweets. \n",
    "        '''\n",
    "        # Find all words wich are not in the Embedding :\n",
    "        missing_words = []\n",
    "        for i, word in enumerate(self.all_words) :\n",
    "            if self.embedding.word2idx.get(word) == None : \n",
    "                missing_words.append(word)\n",
    "        \n",
    "        # Correct if possible the missing_words : \n",
    "        ### We use theshold over which we correct the word. Under which we discard the word\n",
    "        t = 0.5 # threshold\n",
    "        rejected_words = []\n",
    "        corrected_words = {}\n",
    "        for word in tqdm(missing_words) : \n",
    "            suggestion, prob = spelling.suggest(word)[0]\n",
    "            if prob < t : \n",
    "                rejected_words.append(word)\n",
    "            else : \n",
    "                corrected_words[word] = suggestion\n",
    "        \n",
    "        # Modify the Original Datasets with those corrected_words : \n",
    "        for f in self.path : \n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [corrected_words.get(k,k) for k in x])\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [k for k in x if k not in rejected_words ])\n",
    "        nb_rejected = len(rejected_words)\n",
    "        nb_corrected = len(corrected_words)\n",
    "        nb_vocab = len(self.embedding.glove_dict)\n",
    "        p_rejected = 100* nb_rejected / nb_vocab\n",
    "        p_corrected = 100* nb_corrected / nb_vocab\n",
    "        print('---- Words removed   : {0:} / {1:.2f} - {2:} %'.format(nb_rejected,nb_vocab,p_rejected))\n",
    "        print('---- Words corrected : {0:} / {1:.2f} - {2:} %'.format(nb_corrected,nb_vocab,p_corrected))\n",
    "        \n",
    "    def inital_dataload(self):\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_table(self.path[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['tweet'].apply(lambda x : self.clean_tweet(x))\n",
    "            \n",
    "    def compute_wordlist(self):\n",
    "        self.all_words_freq = {}\n",
    "        self.all_words = []\n",
    "        \n",
    "        for f in self.data : \n",
    "            for i in range(len(self.data[f])):\n",
    "                for e in self.data[f].iloc[i].token:\n",
    "                    self.all_words_freq[e] = 1 + self.all_words_freq.get(e,0)\n",
    "        self.all_words = list(self.all_words_freq.keys())\n",
    "        \n",
    "    def splitValidation(self,p):\n",
    "        ''' Creates the validation set by  taking p % of the train dataset '''\n",
    "        data = self.data['train'].copy()\n",
    "        self.data['train'] = {}\n",
    "        self.data['validation'] = {}\n",
    "\n",
    "        for subtask in self.classes_dict: # per subtask\n",
    "            self.data['train'][subtask] = pd.DataFrame()\n",
    "            self.data['validation'][subtask]= pd.DataFrame()\n",
    "            for label in self.classes_dict[subtask]: #per label in this subtask \n",
    "                data_label =  data[data[subtask]==label]\n",
    "                self.data['train'][subtask] = self.data['train'][subtask].append(data.loc[data_label.index])\n",
    "                nb_valid = int(len(data_label)*p)\n",
    "                # Select randmoly (without repetition) the indexes of the selected vaidation tweets\n",
    "                index_valid = np.random.choice(data_label.index,(nb_valid,),replace=False)\n",
    "                # Add the the selected validation tweets to the new dataframe\n",
    "                self.data['validation'][subtask] = self.data['validation'][subtask].append(self.data['train'][subtask].loc[index_valid,:])\n",
    "                # Drop the selected validation tweets from the training set\n",
    "                self.data['train'][subtask] = self.data['train'][subtask].drop(index = index_valid)\n",
    "                \n",
    "    def prepareLabels(self) : \n",
    "        ''' Transform the labels into classes id '''\n",
    "        for subtask in self.classes_dict: # per subtask\n",
    "            self.data['validation'][subtask][subtask] =self.data['validation'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
    "            self.data['train'][subtask][subtask] = self.data['train'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
    "\n",
    "    def balanceData(self,data,subtask):\n",
    "        ''' Augments the Data given in input in order to balance the dataset'''\n",
    "        class_size = {}\n",
    "        for label in self.classes_dict[subtask]:\n",
    "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
    "        largest_class = max(class_size, key=class_size.get)\n",
    "        print('---- Augmenting the Data : ')\n",
    "        print('Before Augmentation : ',class_size)\n",
    "\n",
    "        for label in self.classes_dict[subtask]:  \n",
    "            if label != largest_class:\n",
    "                id_list = data[data[subtask]==self.classes_dict[subtask][label]].index\n",
    "                nb_augmentation = class_size[largest_class] - class_size[label]\n",
    "                id_augmentation = np.random.choice(id_list, (nb_augmentation,))\n",
    "                data = data.append(data.loc[id_augmentation,:])\n",
    "        # Check if it went well\n",
    "        for label in self.classes_dict[subtask]:\n",
    "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
    "        \n",
    "        print('After Augmentation : ',class_size)\n",
    "        return data\n",
    "    \n",
    "    def clean_tweet(self,text):\n",
    "        ''' Function that is applied to every to tweet in the dataset '''\n",
    "        \n",
    "        # =========== TEXT ===========\n",
    "        # Replace @USER by <user>\n",
    "        text = re.compile(r'@USER').sub(r'<user>',text)\n",
    "\n",
    "        # Replace URL by <url>\n",
    "        text = re.compile(r'URL').sub(r'<url>',text)\n",
    "\n",
    "        # Remove numbers :\n",
    "        text = re.compile(r'[0-9]+').sub(r' ',text)\n",
    "\n",
    "        # Remove some special characters\n",
    "        text = re.compile(r'([\\xa0_\\{\\}\\[\\]¬•$,:;/@#|\\^*%().~`”\"“-])').sub(r' ',text) \n",
    "\n",
    "        # Space the special characters with white spaces\n",
    "        text = re.compile(r'([$&+,:;=?@#|\\'.^*()%!\"’“-])').sub(r' \\1 ',text)\n",
    "        \n",
    "        # Replace some special characters : \n",
    "        replace_dict = {r'&' : 'and' , \n",
    "                        r'\\+' : 'plus'}\n",
    "        for cha in replace_dict:\n",
    "            text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
    "            \n",
    "        # Handle Emoji : translate some and delete the others\n",
    "        text = self.handle_emoji(text)\n",
    "        \n",
    "        # Word delengthening : \n",
    "        text = re.compile(r'(.)\\1{3,}').sub(r'\\1\\1',text)\n",
    "\n",
    "        # Cut the words with caps in them : \n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
    "        # =========== TOKENS ===========\n",
    "        # TOKENIZE \n",
    "        text = text.split(' ')\n",
    "\n",
    "        # Remove white spaces tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
    "\n",
    "        # Remove empty tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != '']\n",
    "\n",
    "        # Remove repetition in tokens (!!! => !)\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
    "\n",
    "        #  Handle the ALL CAPS Tweets \n",
    "        ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
    "        caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
    "        if caps_r > 0.6 : \n",
    "            text.append('<allcaps>')\n",
    "\n",
    "        # Lower Case : \n",
    "        text = [text[i].lower() for i in range(len(text))]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def handle_emoji(self,text):\n",
    "        # Dictionnary of \"important\" emojis : \n",
    "        emoji_dict =  {'♥️': ' love ',\n",
    "                       '❤️' : ' love ',\n",
    "                       '❤' : ' love ',\n",
    "                       '😘' : ' kisses ',\n",
    "                      '😭' : ' cry ',\n",
    "                      '💪' : ' strong ',\n",
    "                      '🌍' : ' earth ',\n",
    "                      '💰' : ' money ',\n",
    "                      '👍' : ' ok ',\n",
    "                       '👌' : ' ok ',\n",
    "                      '😡' : ' angry ',\n",
    "                      '🍆' : ' dick ',\n",
    "                      '🤣' : ' haha ',\n",
    "                      '😂' : ' haha ',\n",
    "                      '🖕' : ' fuck you '}\n",
    "\n",
    "        for cha in emoji_dict:\n",
    "            text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
    "        # Remove ALL emojis\n",
    "        text = emoji.get_emoji_regexp().sub(r' ',text) \n",
    "        text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
    "        text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
    "\n",
    "        # Add Space between  the Emoji Expressions : \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0f2pJn13kJh",
    "outputId": "7ffa0820-4b56-4a5c-aa53-b4130370a140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Data Handling : \n",
      "---- Load the Clean Adapted Dataset : Done\n"
     ]
    }
   ],
   "source": [
    "mydata = DataHandling(myEmbedding, pValid=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCYdTDX03kJl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16820</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[amazon, is, investigating, chinese, employees...</td>\n",
       "      <td>[2547, 33, 39459, 3492, 16270, 128, 71, 5442, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43605</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, obama, wanted, liberals, and, amp, il...</td>\n",
       "      <td>[1, 1382, 953, 30168, 27, 12801, 114750, 17, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45157</th>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, buy, more, icecream, !]</td>\n",
       "      <td>[1, 873, 146, 18658, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42992</th>\n",
       "      <td>@USER What’s the difference between #Kavanaugh...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, what, s, the, difference, between, an...</td>\n",
       "      <td>[1, 87, 138, 14, 2954, 1472, 27, 1, 97, 40, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54920</th>\n",
       "      <td>@USER @USER @USER It should scare every Americ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, it, should, scare, every, american, !...</td>\n",
       "      <td>[1, 34, 277, 9603, 382, 2034, 10, 148, 33, 701...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56392</th>\n",
       "      <td>@USER @USER @USER @USER @USER @USER @USER @USE...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, i, like, my, soda, like, i, like, my,...</td>\n",
       "      <td>[1, 11, 64, 30, 9477, 64, 11, 64, 30, 290103, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86735</th>\n",
       "      <td>@USER you are also the king of taste</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, you, are, also, the, king, of, taste]</td>\n",
       "      <td>[1, 16, 71, 895, 14, 1697, 40, 3212]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95686</th>\n",
       "      <td>#MAGA @USER  🎶 Sing like no one is listening  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[maga, &lt;user&gt;, sing, like, no, one, is, listen...</td>\n",
       "      <td>[29896, 1, 1547, 64, 31, 97, 33, 1484, 69, 69,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71446</th>\n",
       "      <td>5/5: @USER The time is right for this House to...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, the, time, is, right, for, this, hous...</td>\n",
       "      <td>[1, 14, 136, 33, 209, 38, 54, 544, 17, 7094, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67757</th>\n",
       "      <td>@USER @USER You are correct.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, you, are, correct]</td>\n",
       "      <td>[1, 16, 71, 5550]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  subtask_a subtask_b  \\\n",
       "id                                                                              \n",
       "16820  Amazon is investigating Chinese employees who ...          0       NaN   \n",
       "43605  @USER @USER Obama wanted liberals &amp; illega...          0       NaN   \n",
       "45157                         @USER Buy more icecream!!!          0       NaN   \n",
       "42992  @USER What’s the difference between #Kavanaugh...          0       NaN   \n",
       "54920  @USER @USER @USER It should scare every Americ...          0       NaN   \n",
       "56392  @USER @USER @USER @USER @USER @USER @USER @USE...          0       NaN   \n",
       "86735               @USER you are also the king of taste          0       NaN   \n",
       "95686  #MAGA @USER  🎶 Sing like no one is listening  ...          0       NaN   \n",
       "71446  5/5: @USER The time is right for this House to...          0       NaN   \n",
       "67757                       @USER @USER You are correct.          0       NaN   \n",
       "\n",
       "      subtask_c                                              token  \\\n",
       "id                                                                   \n",
       "16820       NaN  [amazon, is, investigating, chinese, employees...   \n",
       "43605       NaN  [<user>, obama, wanted, liberals, and, amp, il...   \n",
       "45157       NaN                   [<user>, buy, more, icecream, !]   \n",
       "42992       NaN  [<user>, what, s, the, difference, between, an...   \n",
       "54920       NaN  [<user>, it, should, scare, every, american, !...   \n",
       "56392       NaN  [<user>, i, like, my, soda, like, i, like, my,...   \n",
       "86735       NaN     [<user>, you, are, also, the, king, of, taste]   \n",
       "95686       NaN  [maga, <user>, sing, like, no, one, is, listen...   \n",
       "71446       NaN  [<user>, the, time, is, right, for, this, hous...   \n",
       "67757       NaN                        [<user>, you, are, correct]   \n",
       "\n",
       "                                                token_id  \n",
       "id                                                        \n",
       "16820  [2547, 33, 39459, 3492, 16270, 128, 71, 5442, ...  \n",
       "43605  [1, 1382, 953, 30168, 27, 12801, 114750, 17, 9...  \n",
       "45157                           [1, 873, 146, 18658, 10]  \n",
       "42992  [1, 87, 138, 14, 2954, 1472, 27, 1, 97, 40, 37...  \n",
       "54920  [1, 34, 277, 9603, 382, 2034, 10, 148, 33, 701...  \n",
       "56392  [1, 11, 64, 30, 9477, 64, 11, 64, 30, 290103, ...  \n",
       "86735               [1, 16, 71, 895, 14, 1697, 40, 3212]  \n",
       "95686  [29896, 1, 1547, 64, 31, 97, 33, 1484, 69, 69,...  \n",
       "71446  [1, 14, 136, 33, 209, 38, 54, 544, 17, 7094, 1...  \n",
       "67757                                  [1, 16, 71, 5550]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.data['train']['subtask_a'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxvQf9AH3kJp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER Get the hell out of my country that u hate @USER the u have trashed &amp; weaponized OUR GOVT for 8 yrs to destroy us &amp; quit telling our kids to hate the best county your a disgrace!\n",
      "['<user>', 'get', 'the', 'hell', 'out', 'of', 'my', 'country', 'that', 'u', 'hate', '<user>', 'the', 'u', 'have', 'trashed', 'and', 'amp', 'weaponized', 'our', 'govt', 'for', 'yrs', 'to', 'destroy', 'us', 'and', 'amp', 'quit', 'telling', 'our', 'kids', 'to', 'hate', 'the', 'best', 'county', 'your', 'a', 'disgrace', '!']\n",
      "[1, 88, 14, 674, 100, 40, 30, 1671, 46, 52, 258, 1, 14, 52, 65, 65748, 27, 12801, 610387, 278, 10666, 38, 6661, 17, 7994, 292, 27, 12801, 3269, 2013, 278, 885, 17, 258, 14, 210, 5741, 62, 12, 23234, 10]\n"
     ]
    }
   ],
   "source": [
    "# Example of Tweet Processing\n",
    "print(mydata.data['train']['subtask_a'].iloc[331]['tweet'])\n",
    "print(mydata.data['train']['subtask_a'].iloc[331]['token'])\n",
    "print(mydata.data['train']['subtask_a'].iloc[331]['token_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkPjJ7Kx3kJr"
   },
   "source": [
    "## Classifier \n",
    "Set of Classes used as classifier for the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHs2iTwV3kJs"
   },
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
    "        print('------ Creating FFNN : ',end='')\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        # Activation Layers\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.LogSoftmax(dim=1)\n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # we average the embeddings of words in a sentence\n",
    "        \n",
    "        non_zero_nb = (x!=0).sum(1,keepdim=True)\n",
    "        #print(x.shape, non_zero_nb,embedded.sum(1).shape)\n",
    "        averaged = embedded.sum(1) / non_zero_nb.float()\n",
    "        #averaged = embedded.mean(1)\n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjriTHog3kJw"
   },
   "outputs": [],
   "source": [
    "# CNN : \n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        #in_channels -- 1 text channel\n",
    "        #out_channels -- the number of output channels\n",
    "        #kernel_size is (window size x embedding dim)\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size,embedding_dim))\n",
    "        \n",
    "        #the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        #the output layer\n",
    "        self.fc = nn.Linear(out_channels, output_dim)\n",
    "        \n",
    "        self.out =  nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        #(batch size, max sent length)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        #print('embedded',embedded.shape)\n",
    "                \n",
    "        #(batch size, max sent length, embedding dim)\n",
    "        \n",
    "        #images have 3 RGB channels \n",
    "        #for the text we add 1 channel\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print('embedded_unsqueeze',embedded.shape)\n",
    "        \n",
    "        #(batch size, 1, max sent length, embedding dim)\n",
    "        \n",
    "        feature_maps = self.conv(embedded)\n",
    "        #print('feature_maps',feature_maps.shape)\n",
    "\n",
    "        #??? what is the shape of the convolution output\n",
    "        \n",
    "        #(batch size, n filters, max input length - window size +1, 1)\n",
    "        \n",
    "        feature_maps = feature_maps.squeeze(3)\n",
    "        #print('feature_maps_unsqueeze',feature_maps.shape)\n",
    "\n",
    "        #??? why do we reduce 1 dimention here\n",
    "        \n",
    "        # we do need the 1 channel anymore\n",
    "                \n",
    "        feature_maps = F.relu(feature_maps)\n",
    "        #print('feature_maps_relu',feature_maps.shape)\n",
    "\n",
    "  \n",
    "        #the max pooling layer\n",
    "        pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "        #print('pooled',pooled.shape)\n",
    "\n",
    "        pooled = pooled.squeeze(2)\n",
    "        #print('pooled_squeeze',pooled.shape)\n",
    "\n",
    "        #??? what is the shape of the pooling output\n",
    "        #(batch size, n_filters)\n",
    "        \n",
    "        dropped = self.dropout(pooled)\n",
    "        #print('dropped',dropped.shape)\n",
    "\n",
    "        preds = self.fc(dropped)\n",
    "        #print('preds', preds.shape)\n",
    "        out = self.out(preds)\n",
    "        #print('out',out.shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        return nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_paper(nn.Module):\n",
    "    '''https://arxiv.org/pdf/1408.5882'''\n",
    "    def __init__(self, embedding, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
    "        \n",
    "        super(CNN_paper, self).__init__()\n",
    "        \n",
    "        self.embedding = embedding\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size[0],embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size[1],embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=(window_size[2],embedding_dim))\n",
    "        #the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        #the output layer\n",
    "        self.fc = nn.Linear(out_channels*3, output_dim)\n",
    "        \n",
    "        self.out =  nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def conv_block(self,embedded,conv_layer):\n",
    "        feature_maps = conv_layer(embedded)\n",
    "        features_maps = feature_maps.squeeze(3)\n",
    "        feature_maps = F.relu(feature_maps)\n",
    "        return F.max_pool1d(feature_maps, feature_maps.shape[2]).squeeze(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "                \n",
    "        #(batch size, max sent length)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "                        \n",
    "        #images have 3 RGB channels \n",
    "        #for the text we add 1 channel\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "         \n",
    "\n",
    "        feature_maps1 = self.conv1(embedded)\n",
    "        feature_maps2 = self.conv2(embedded)\n",
    "        feature_maps3 = self.conv3(embedded)\n",
    "        \n",
    "        feature_maps1 = feature_maps1.squeeze(3)\n",
    "        feature_maps2 = feature_maps2.squeeze(3)\n",
    "        feature_maps3 = feature_maps3.squeeze(3)\n",
    "\n",
    "\n",
    "                \n",
    "        feature_maps1 = F.relu(feature_maps1)\n",
    "        feature_maps2 = F.relu(feature_maps2)\n",
    "        feature_maps3 = F.relu(feature_maps3)\n",
    "\n",
    "  \n",
    "        #the max pooling layer\n",
    "        pooled1 = F.max_pool1d(feature_maps1, feature_maps1.shape[2])\n",
    "        pooled2 = F.max_pool1d(feature_maps2, feature_maps2.shape[2])\n",
    "        pooled3 = F.max_pool1d(feature_maps3, feature_maps3.shape[2])\n",
    "\n",
    "        pooled1 = pooled1.squeeze(2)\n",
    "        pooled2 = pooled2.squeeze(2)\n",
    "        pooled3 = pooled3.squeeze(2)\n",
    "        all_pooled =torch.cat((pooled1, pooled2, pooled3), 1)\n",
    "        \n",
    "        dropped = self.dropout(all_pooled)\n",
    "\n",
    "        preds = self.fc(dropped)\n",
    "        out = self.out(preds)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        return nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFKLQ_2L3kJz"
   },
   "outputs": [],
   "source": [
    "# RNN, GRU and LSTM\n",
    "class RNN_base(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
    "        super(RNN_base, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        if output_layer==None:\n",
    "            self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        else:\n",
    "            self.i2o = output_layer\n",
    "        self.final_activation = nn.LogSoftmax(dim=1)\n",
    "        self.final_activation = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs_embedded = self.embedding(inputs).float()\n",
    " \n",
    "        sentence_lengths = (inputs_embedded.sum(dim=2,keepdim=False)!=0).sum(dim=1)\n",
    "        \n",
    "        output, hidden = self.rnn(inputs_embedded)\n",
    "        \n",
    "        output_final = torch.empty(inputs.shape[0],self.hidden_size).float()\n",
    "        output_final = output[torch.arange(inputs.shape[0]),sentence_lengths-1,:]\n",
    "\n",
    "        output = self.i2o(output_final)\n",
    "        output = self.final_activation(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.NLLLoss()\n",
    "    \n",
    "class RNN(RNN_base):\n",
    "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
    "        RNN_base.__init__(self, \n",
    "                          embedding=embedding, \n",
    "                          embedding_dim=embedding_dim, \n",
    "                          hidden_size=hidden_size, \n",
    "                          output_size=output_size, \n",
    "                          num_layers=num_layers, \n",
    "                          output_layer=output_layer)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = embedding_dim, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            nonlinearity = \"relu\",\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "class LSTM(RNN_base):\n",
    "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
    "        RNN_base.__init__(self, \n",
    "                          embedding=embedding, \n",
    "                          embedding_dim=embedding_dim, \n",
    "                          hidden_size=hidden_size, \n",
    "                          output_size=output_size, \n",
    "                          num_layers=num_layers, \n",
    "                          output_layer=output_layer)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = embedding_dim, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )  \n",
    "        \n",
    "class GRU(RNN_base):\n",
    "    def __init__(self, embedding, embedding_dim=25, hidden_size=25, output_size=2, num_layers=1, output_layer=None):\n",
    "        RNN_base.__init__(self, \n",
    "                          embedding=embedding, \n",
    "                          embedding_dim=embedding_dim, \n",
    "                          hidden_size=hidden_size, \n",
    "                          output_size=output_size, \n",
    "                          num_layers=num_layers, \n",
    "                          output_layer=output_layer)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size = embedding_dim, \n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        ) \n",
    "        \n",
    "def output_layer(input_size,output_size,middle_layers=[]):\n",
    "    \"Input and output size - scalars. Middle layers - list (ignore if just one layer wanted)\"\n",
    "    \n",
    "    #For current implementation, the input_size should be equal to the size of the hidden layer for the RNNs.\n",
    "    activation = nn.ReLU()\n",
    "\n",
    "    component_layers=[]\n",
    "    if middle_layers!=[]:\n",
    "\n",
    "        component_layers.append(\n",
    "            nn.Linear(\n",
    "                in_features  = input_size,\n",
    "                out_features  = middle_layers[0]))\n",
    "        component_layers.append(activation)\n",
    "\n",
    "\n",
    "        if len(middle_layers)>1:\n",
    "            for layer in range(1,len(middle_layers)):\n",
    "                component_layers.append(\n",
    "                    nn.Linear(in_features  = middle_layers[layer-1],\n",
    "                              out_features  = middle_layers[layer]))\n",
    "                component_layers.append(activation)\n",
    "\n",
    "        component_layers.append(\n",
    "            nn.Linear(in_features  = middle_layers[-1],\n",
    "                      out_features  = output_size))\n",
    "\n",
    "    else:\n",
    "        component_layers.append(\n",
    "            nn.Linear(in_features  = input_size,\n",
    "                      out_features  = output_size))\n",
    "\n",
    "    i2o = nn.Sequential(*component_layers)\n",
    "    \n",
    "    return i2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dEYQKa13kJ3"
   },
   "source": [
    "Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FArD3qIF3kJ4"
   },
   "outputs": [],
   "source": [
    "class padding_tweet:\n",
    "    '''\n",
    "    Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. \n",
    "    args: \n",
    "        batch - List of elements ( x , label )\n",
    "    return \n",
    "        batch - Padded ( list(x) , list(label))\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "    def __call__(self,batch):\n",
    "        \n",
    "        batch = list(zip(*batch))\n",
    "       \n",
    "        batch[0] = torch.stack([self.pad_tensor(vec=t, pad=self.max_len, dim=0) for t in batch[0]],dim=0)\n",
    "        batch[1] = torch.stack(batch[1])\n",
    "        return batch[0] , batch[1]\n",
    "\n",
    "    def pad_tensor(self,vec, pad, dim):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            vec - tensor to pad\n",
    "            pad - the size to pad to\n",
    "            dim - dimension to pad\n",
    "\n",
    "        return:\n",
    "            a new tensor padded to 'pad' in dimension 'dim'\n",
    "        \"\"\"\n",
    "        pad_size = list(vec.shape)\n",
    "        pad_size[dim] = pad - vec.size(dim)\n",
    "        return torch.cat([vec, torch.zeros(*pad_size,dtype=torch.long)], dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ql4Aim1m3kJ7"
   },
   "source": [
    "## Main Class : Trainer\n",
    "Main Class for the loading, training, testing etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q50IDC93kJ-"
   },
   "outputs": [],
   "source": [
    "class OffensiveClassifier(object):\n",
    "    ''' Main Class for the loading, training, testing etc ...'''\n",
    "    def __init__(self,subtask='subtask_a', dim_vect=25,pValid = 0.15):\n",
    "        \n",
    "        self.dim_vect = dim_vect\n",
    "        \n",
    "        self.subtask = subtask\n",
    "        \n",
    "\n",
    "        # Loading the GloVe Embedding and Torch Formating of this Embedding\n",
    "        self.GloVe = GloVe_embedding(dim_vect= dim_vect )\n",
    "        self.embedding = self.GloVe.emb_layer\n",
    "        \n",
    "        # Loading the Data Handler : \n",
    "        self.dataHandler = DataHandling(self.GloVe,pValid=pValid)\n",
    "        \n",
    "        # Retrieving Training DataSet (pytorch)\n",
    "        self.train_set = self.dataHandler.getDataset('train',subtask,balanced=True)\n",
    "        \n",
    "        # Retrieving the Validation Set (pytorch)\n",
    "        self.valid_set = self.dataHandler.getDataset('validation',subtask)\n",
    "\n",
    "        # Retrieving Test DataSet (pytorch)\n",
    "        self.test_set = self.dataHandler.getDataset('test',subtask)\n",
    "        \n",
    "        # Usefull info on classes : \n",
    "        self.class2id  = self.dataHandler.classes_dict[self.subtask]\n",
    "        self.id2class = dict(zip(self.class2id.values(),self.class2id.keys()))\n",
    "        self.nb_class = len(self.class2id)\n",
    "        self.max_len  = max([ max([len(t) for t in self.train_set.token]),\n",
    "                              max([len(t) for t in self.valid_set.token]),\n",
    "                              max([len(t) for t in self.test_set.token])])\n",
    "            \n",
    "        \n",
    "    \n",
    "    def train( self, nb_epochs, optimizer , model , batch_size = 1000 ,  ):\n",
    "        \n",
    "        self.train_generator = DataLoader(self.train_set, batch_size=batch_size,collate_fn=padding_tweet(self.max_len), shuffle=True)\n",
    "\n",
    "        self.model = model.to(device)\n",
    "        loss_fn = self.model.loss_fn()\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            i_batch = 0\n",
    "            accuracy_average = 0\n",
    "            self.model.train() \n",
    "            \n",
    "            for tokens, target  in self.train_generator :\n",
    "                i_batch += 1\n",
    "                target = target.long().view((-1,)).to(device)\n",
    "                tokens = tokens.long().to(device)\n",
    "                #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "                #good practice to include even if do not use here\n",
    "                self.model.train()\n",
    "\n",
    "                #we zero the gradients as they are not removed automatically\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = self.model(tokens)\n",
    "                #print(output.shape)\n",
    "\n",
    "                predictions = torch.argmax(output,dim=1).float()\n",
    "                loss = loss_fn(output, target)\n",
    "                acc, correct = self.accuracy(predictions, target)\n",
    "                accuracy_average += acc\n",
    "                #calculate the gradient of each parameter\n",
    "                loss.backward()\n",
    "                \n",
    "                #update the parameters using the gradients and optimizer algorithm \n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss = loss.item()\n",
    "\n",
    "                if i_batch % (nb_epochs -1 )  ==0:\n",
    "                    pass\n",
    "            print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {accuracy_average*100/i_batch:.2f}%')\n",
    "            self.validation(batch_size = 50)\n",
    "            print('=============================================================================')\n",
    "            print()\n",
    "            \n",
    "    def validation(self, batch_size = 1000):\n",
    "        all_correct = 0\n",
    "        self.validation_generator = DataLoader(self.valid_set, batch_size=batch_size, collate_fn=padding_tweet(self.max_len), shuffle=True)\n",
    "        loss_fn = self.model.loss_fn()\n",
    "        nb_valid = len(taskAclassifier.valid_set)\n",
    "        self.model.eval()  # set model to evaluation mode\n",
    "        all_prediction = []\n",
    "        all_target = []\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for tokens, target  in self.validation_generator :\n",
    "                target = target.long().view((-1,)).to(device)\n",
    "                tokens = tokens.long().to(device)\n",
    "                \n",
    "                output = self.model(tokens)\n",
    "                predictions = torch.argmax(output,dim=1)\n",
    "                \n",
    "                loss = loss_fn(output, target)\n",
    "                \n",
    "                all_prediction.extend(predictions.view(-1,).tolist())\n",
    "                all_target.extend(target.view(-1,).tolist())\n",
    "                \n",
    "        accuracy, CM, stats_df = self.results(all_prediction,all_target)\n",
    "        print(f'| Stats on the Validation : ')\n",
    "        print(stats_df)\n",
    "        \n",
    "        print(f'| Validation Accuracy : {100*accuracy:.2f} % - Macro F1 Score : {stats_df.loc[\"f1-measure\"].mean():.4f} ')\n",
    "        print(f'| CM on the Validation : ')\n",
    "        print(CM)\n",
    "        print()\n",
    "        \n",
    "    def test(self,folder):\n",
    "        ''' \n",
    "            Test Function : Tests the Network on the Test Data of the Subtask and Saves in a file\n",
    "        '''\n",
    "        test_result = pd.DataFrame()\n",
    "        self.test_generator = DataLoader(self.test_set,collate_fn= padding_tweet(self.max_len) )\n",
    "        nb_valid = len(taskAclassifier.valid_set)\n",
    "        self.model.eval()  # set model to evaluation mode\n",
    "        all_prediction = []\n",
    "        all_id = []\n",
    "        with torch.no_grad(): \n",
    "            for tokens, id_tweet  in self.test_generator :\n",
    "                id_tweet = id_tweet.long().view((-1,))\n",
    "                tokens = tokens.long().to(device)\n",
    "                \n",
    "                output = self.model(tokens)\n",
    "                predictions = torch.argmax(output,dim=1)\n",
    "                \n",
    "                all_prediction.extend(predictions.view(-1,).tolist())\n",
    "                all_id.extend(id_tweet.view(-1,).tolist())\n",
    "        results_df = pd.DataFrame(all_prediction, index=all_id)\n",
    "              \n",
    "        # Change id_labels to labels\n",
    "        results_df[0] = results_df[0].apply(lambda x : self.id2class[x])\n",
    "              \n",
    "        # Saving the Results in the right format in the folder ../data/results/*folder*\n",
    "        save_path = join('..','data','results')\n",
    "        if not(exists(save_path)) : mkdir(save_path)\n",
    "        save_path = join(save_path,folder)\n",
    "        if not(exists(save_path)) : mkdir(save_path)\n",
    "        \n",
    "        path_file = join(save_path, self.subtask + '.csv')\n",
    "        i = 0\n",
    "        while exists(path_file):\n",
    "              i += 1\n",
    "              path_file = join(save_path, self.subtask + '_'+ str(i) +'.csv')\n",
    "        results_df.to_csv(path_file, header=False)\n",
    "        print('| Successfully save to : ', path_file)\n",
    "        \n",
    "\n",
    "    def accuracy(self, output, target):\n",
    "        target = np.array(target.tolist()).astype(int)\n",
    "        output = np.array(torch.round(output).tolist()).astype(int)\n",
    "        correct = (output == target)\n",
    "        accuracy = correct.sum()/len(correct)\n",
    "        return accuracy, correct\n",
    "        \n",
    "    def results(self, output, target ):\n",
    "        target = np.array(target).astype(int)\n",
    "        output = np.round(output).astype(int)\n",
    "        correct = (output == target)\n",
    "        accuracy = correct.sum()/len(correct)\n",
    "        \n",
    "        CM = np.zeros((self.nb_class,self.nb_class))\n",
    "        for i in range(len(output)): \n",
    "            CM[target[i], output[i]] += 1\n",
    "        \n",
    "        # compute the interesting stats on the classification : \n",
    "        stats_df = self.stats(CM)\n",
    "        stats_df.columns = map(lambda x : self.id2class[x] ,stats_df.columns)\n",
    "        return (accuracy, CM, stats_df)\n",
    "\n",
    "    def stats(self,CM):   \n",
    "        n_class = self.nb_class\n",
    "    \n",
    "        stats = {}\n",
    "        stats['precision'] = {} \n",
    "        stats['recall'] = {} \n",
    "        stats['accuracy'] = {} \n",
    "        stats['f1-measure'] = {} \n",
    "\n",
    "        for t in range(self.nb_class):\n",
    "            tp = CM[t,t]\n",
    "            tn = np.sum([CM[i,i] for i in range(self.nb_class) if i != t])\n",
    "\n",
    "            fp = np.sum([CM[i,t] for i in range(self.nb_class) if i != t])\n",
    "            fn = np.sum([CM[t,i] for i in range(self.nb_class) if i != t])\n",
    "\n",
    "            # compute accuracy per class :\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "            # compute recall per class\n",
    "            recall = tp /(tp + fn)\n",
    "\n",
    "            # compute precision per class \n",
    "            precision = tp / (tp + fp)\n",
    "\n",
    "            # compute F1-measure per class \n",
    "            f1 = 2*tp / (2* tp + fp + fn)\n",
    "\n",
    "            # saving stats : \n",
    "\n",
    "            stats['precision'][t] = precision\n",
    "            stats['recall'][t] = recall\n",
    "            stats['accuracy'][t] = accuracy\n",
    "            stats['f1-measure'][t] = f1\n",
    "\n",
    "        # Compute Average classification rate\n",
    "        stats = pd.DataFrame(stats).transpose()\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaludation of the Model \n",
    "Choice of the sub-task, the dimension of the embedding vectors, and the percentage of validation taken from train.\n",
    "\n",
    "**Best choice** : `dim_vect = 200` and `pValid = 0.2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkfM8PQD3kKB",
    "outputId": "cc15ddea-6609-4e12-e9f8-6d65745198ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n",
      "-- Data Handling : \n",
      "---- Load the Clean Adapted Dataset : Done\n",
      "---- Augmenting the Data : \n",
      "Before Augmentation :  {'NOT': 7072, 'OFF': 3520}\n",
      "After Augmentation :  {'NOT': 7072, 'OFF': 7072}\n"
     ]
    }
   ],
   "source": [
    "# 1st step : Define Classifier for specific task\n",
    "taskAclassifier = OffensiveClassifier(subtask='subtask_a', dim_vect=200 , pValid = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) FFNN\n",
    "Best Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Step : Define Model : \n",
    "model = FFNN(embedding= taskAclassifier.embedding, \n",
    "             hidden_dim= 100 ,\n",
    "             num_classes = taskAclassifier.nb_class,\n",
    "             embedding_dim= taskAclassifier.dim_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Step : Define Optinizer : \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Step : Train the Model\n",
    "taskAclassifier.train( nb_epochs= 50,\n",
    "                       optimizer= optimizer , \n",
    "                       model= model ,\n",
    "                       batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfzchoAj3kKW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Successfully save to :  ../data/results/LSTM/subtask_a_2.csv\n"
     ]
    }
   ],
   "source": [
    "# 5th Step : Test the Model and save data \n",
    "taskAclassifier.test(folder='FFNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CNN : 1 Layer\n",
    "Best Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Step : Define Model : \n",
    "model = CNN( embedding= taskAclassifier.embedding, \n",
    "             embedding_dim = taskAclassifier.dim_vect,\n",
    "             out_channels= 150,\n",
    "             window_size= 3,\n",
    "             output_dim= taskAclassifier.nb_class,\n",
    "             dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Step : Define Optinizer : \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Step : Train the Model\n",
    "taskAclassifier.train( nb_epochs= 50,\n",
    "                       optimizer= optimizer , \n",
    "                       model= model ,\n",
    "                       batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfzchoAj3kKW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Successfully save to :  ../data/results/LSTM/subtask_a_2.csv\n"
     ]
    }
   ],
   "source": [
    "# 5th Step : Test the Model and save data \n",
    "taskAclassifier.test(folder='CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) CNN : 3 Layers\n",
    "Best Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Step : Define Model : \n",
    "model = CNN_paper(embedding= taskAclassifier.embedding, \n",
    "             embedding_dim = taskAclassifier.dim_vect,\n",
    "             out_channels= 100,\n",
    "             window_size= [3,4,5],\n",
    "             output_dim= taskAclassifier.nb_class,\n",
    "             dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Step : Define Optinizer : \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 00 | Train Loss: 0.664 | Train Acc: 50.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.853746  0.537302\n",
      "recall      0.670249  0.769318\n",
      "accuracy    0.703172  0.703172\n",
      "f1-measure  0.750951  0.632710\n",
      "| Validation Accuracy : 70.32 % - Macro F1 Score : 0.6918 \n",
      "| CM on the Validation : \n",
      "[[1185.  583.]\n",
      " [ 203.  677.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 01 | Train Loss: 0.883 | Train Acc: 75.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.844109  0.626385\n",
      "recall      0.790158  0.706818\n",
      "accuracy    0.762462  0.762462\n",
      "f1-measure  0.816243  0.664175\n",
      "| Validation Accuracy : 76.25 % - Macro F1 Score : 0.7402 \n",
      "| CM on the Validation : \n",
      "[[1397.  371.]\n",
      " [ 258.  622.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 02 | Train Loss: 0.610 | Train Acc: 75.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.884185  0.526504\n",
      "recall      0.626131  0.835227\n",
      "accuracy    0.695619  0.695619\n",
      "f1-measure  0.733113  0.645870\n",
      "| Validation Accuracy : 69.56 % - Macro F1 Score : 0.6895 \n",
      "| CM on the Validation : \n",
      "[[1107.  661.]\n",
      " [ 145.  735.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 03 | Train Loss: 0.152 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.827278  0.664726\n",
      "recall      0.837104  0.648864\n",
      "accuracy    0.774547  0.774547\n",
      "f1-measure  0.832162  0.656699\n",
      "| Validation Accuracy : 77.45 % - Macro F1 Score : 0.7444 \n",
      "| CM on the Validation : \n",
      "[[1480.  288.]\n",
      " [ 309.  571.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 04 | Train Loss: 0.213 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.840120  0.626789\n",
      "recall      0.793552  0.696591\n",
      "accuracy    0.761329  0.761329\n",
      "f1-measure  0.816172  0.659849\n",
      "| Validation Accuracy : 76.13 % - Macro F1 Score : 0.7380 \n",
      "| CM on the Validation : \n",
      "[[1403.  365.]\n",
      " [ 267.  613.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 05 | Train Loss: 0.476 | Train Acc: 75.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.835369  0.642626\n",
      "recall      0.812217  0.678409\n",
      "accuracy    0.767749  0.767749\n",
      "f1-measure  0.823631  0.660033\n",
      "| Validation Accuracy : 76.77 % - Macro F1 Score : 0.7418 \n",
      "| CM on the Validation : \n",
      "[[1436.  332.]\n",
      " [ 283.  597.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 06 | Train Loss: 0.341 | Train Acc: 75.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.828171  0.625400\n",
      "recall      0.801471  0.665909\n",
      "accuracy    0.756420  0.756420\n",
      "f1-measure  0.814602  0.645019\n",
      "| Validation Accuracy : 75.64 % - Macro F1 Score : 0.7298 \n",
      "| CM on the Validation : \n",
      "[[1417.  351.]\n",
      " [ 294.  586.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 07 | Train Loss: 1.362 | Train Acc: 25.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.812869  0.678161\n",
      "recall      0.857466  0.603409\n",
      "accuracy    0.773036  0.773036\n",
      "f1-measure  0.834572  0.638605\n",
      "| Validation Accuracy : 77.30 % - Macro F1 Score : 0.7366 \n",
      "| CM on the Validation : \n",
      "[[1516.  252.]\n",
      " [ 349.  531.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 08 | Train Loss: 0.243 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.822644  0.657310\n",
      "recall      0.834276  0.638636\n",
      "accuracy    0.769260  0.769260\n",
      "f1-measure  0.828419  0.647839\n",
      "| Validation Accuracy : 76.93 % - Macro F1 Score : 0.7381 \n",
      "| CM on the Validation : \n",
      "[[1475.  293.]\n",
      " [ 318.  562.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 09 | Train Loss: 0.072 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.852236  0.590045\n",
      "recall      0.743778  0.740909\n",
      "accuracy    0.742825  0.742825\n",
      "f1-measure  0.794322  0.656927\n",
      "| Validation Accuracy : 74.28 % - Macro F1 Score : 0.7256 \n",
      "| CM on the Validation : \n",
      "[[1315.  453.]\n",
      " [ 228.  652.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 10 | Train Loss: 0.112 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.851950  0.577974\n",
      "recall      0.729072  0.745455\n",
      "accuracy    0.734517  0.734517\n",
      "f1-measure  0.785736  0.651117\n",
      "| Validation Accuracy : 73.45 % - Macro F1 Score : 0.7184 \n",
      "| CM on the Validation : \n",
      "[[1289.  479.]\n",
      " [ 224.  656.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 11 | Train Loss: 0.147 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.818837  0.655991\n",
      "recall      0.835973  0.628409\n",
      "accuracy    0.766994  0.766994\n",
      "f1-measure  0.827316  0.641904\n",
      "| Validation Accuracy : 76.70 % - Macro F1 Score : 0.7346 \n",
      "| CM on the Validation : \n",
      "[[1478.  290.]\n",
      " [ 327.  553.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 12 | Train Loss: 0.068 | Train Acc: 100.00%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.814917  0.650358\n",
      "recall      0.834276  0.619318\n",
      "accuracy    0.762840  0.762840\n",
      "f1-measure  0.824483  0.634459\n",
      "| Validation Accuracy : 76.28 % - Macro F1 Score : 0.7295 \n",
      "| CM on the Validation : \n",
      "[[1475.  293.]\n",
      " [ 335.  545.]]\n",
      "\n",
      "=============================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-8df43a667700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        batch_size= 20)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-298206bc36b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_epochs, optimizer, model, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m#calculate the gradient of each parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-298206bc36b8>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, output, target)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4th Step : Train the Model\n",
    "taskAclassifier.train( nb_epochs= 50,\n",
    "                       optimizer= optimizer , \n",
    "                       model= model ,\n",
    "                       batch_size= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfzchoAj3kKW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Successfully save to :  ../data/results/LSTM/subtask_a_2.csv\n"
     ]
    }
   ],
   "source": [
    "# 5th Step : Test the Model and save data \n",
    "taskAclassifier.test(folder='CNN_paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) RNN : \n",
    "Best Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Step : Define Model : \n",
    "model = RNN(embedding= taskAclassifier.embedding,  \n",
    "            embedding_dim = taskAclassifier.dim_vect,\n",
    "            hidden_size=taskAclassifier.dim_vect, \n",
    "            output_size=taskAclassifier.nb_class,\n",
    "            num_layers=2,\n",
    "            output_layer = output_layer(taskAclassifier.dim_vect, taskAclassifier.nb_class,\n",
    "                middle_layers = [50])) #Change this to add/remove middle layers from output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Step : Define Optinizer : \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 00 | Train Loss: 0.692 | Train Acc: 52.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/mli/lib/python3.6/site-packages/ipykernel_launcher.py:197: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "| Validation Accuracy : 33.23 % - Macro F1 Score : 0.2494 \n",
      "| CM on the Validation : \n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 01 | Train Loss: 0.696 | Train Acc: 43.18%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.666667  0.332324\n",
      "recall      0.002262  0.997727\n",
      "accuracy    0.333082  0.333082\n",
      "f1-measure  0.004510  0.498580\n",
      "| Validation Accuracy : 33.31 % - Macro F1 Score : 0.2515 \n",
      "| CM on the Validation : \n",
      "[[   4. 1764.]\n",
      " [   2.  878.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 02 | Train Loss: 0.701 | Train Acc: 36.36%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.717850  0.344617\n",
      "recall      0.211538  0.832955\n",
      "accuracy    0.418051  0.418051\n",
      "f1-measure  0.326780  0.487529\n",
      "| Validation Accuracy : 41.81 % - Macro F1 Score : 0.4072 \n",
      "| CM on the Validation : \n",
      "[[ 374. 1394.]\n",
      " [ 147.  733.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 03 | Train Loss: 0.696 | Train Acc: 38.64%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.726177  0.363112\n",
      "recall      0.375000  0.715909\n",
      "accuracy    0.488293  0.488293\n",
      "f1-measure  0.494592  0.481836\n",
      "| Validation Accuracy : 48.83 % - Macro F1 Score : 0.4882 \n",
      "| CM on the Validation : \n",
      "[[ 663. 1105.]\n",
      " [ 250.  630.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 04 | Train Loss: 0.695 | Train Acc: 47.73%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.725589  0.379452\n",
      "recall      0.487557  0.629545\n",
      "accuracy    0.534743  0.534743\n",
      "f1-measure  0.583221  0.473504\n",
      "| Validation Accuracy : 53.47 % - Macro F1 Score : 0.5284 \n",
      "| CM on the Validation : \n",
      "[[862. 906.]\n",
      " [326. 554.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 05 | Train Loss: 0.686 | Train Acc: 54.55%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.725416  0.389894\n",
      "recall      0.542421  0.587500\n",
      "accuracy    0.557402  0.557402\n",
      "f1-measure  0.620712  0.468722\n",
      "| Validation Accuracy : 55.74 % - Macro F1 Score : 0.5447 \n",
      "| CM on the Validation : \n",
      "[[959. 809.]\n",
      " [363. 517.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 06 | Train Loss: 0.683 | Train Acc: 63.64%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.724679  0.396469\n",
      "recall      0.574661  0.561364\n",
      "accuracy    0.570242  0.570242\n",
      "f1-measure  0.641009  0.464722\n",
      "| Validation Accuracy : 57.02 % - Macro F1 Score : 0.5529 \n",
      "| CM on the Validation : \n",
      "[[1016.  752.]\n",
      " [ 386.  494.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 07 | Train Loss: 0.680 | Train Acc: 70.45%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.731793  0.407377\n",
      "recall      0.591063  0.564773\n",
      "accuracy    0.582326  0.582326\n",
      "f1-measure  0.653942  0.473333\n",
      "| Validation Accuracy : 58.23 % - Macro F1 Score : 0.5636 \n",
      "| CM on the Validation : \n",
      "[[1045.  723.]\n",
      " [ 383.  497.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 08 | Train Loss: 0.683 | Train Acc: 61.36%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.736121  0.416316\n",
      "recall      0.607466  0.562500\n",
      "accuracy    0.592523  0.592523\n",
      "f1-measure  0.665634  0.478492\n",
      "| Validation Accuracy : 59.25 % - Macro F1 Score : 0.5721 \n",
      "| CM on the Validation : \n",
      "[[1074.  694.]\n",
      " [ 385.  495.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 09 | Train Loss: 0.674 | Train Acc: 61.36%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.739241  0.438202\n",
      "recall      0.660633  0.531818\n",
      "accuracy    0.617825  0.617825\n",
      "f1-measure  0.697730  0.480493\n",
      "| Validation Accuracy : 61.78 % - Macro F1 Score : 0.5891 \n",
      "| CM on the Validation : \n",
      "[[1168.  600.]\n",
      " [ 412.  468.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 10 | Train Loss: 0.676 | Train Acc: 56.82%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.755895  0.437811\n",
      "recall      0.616516  0.600000\n",
      "accuracy    0.611027  0.611027\n",
      "f1-measure  0.679128  0.506232\n",
      "| Validation Accuracy : 61.10 % - Macro F1 Score : 0.5927 \n",
      "| CM on the Validation : \n",
      "[[1090.  678.]\n",
      " [ 352.  528.]]\n",
      "\n",
      "=============================================================================\n",
      "\n",
      "| Epoch: 11 | Train Loss: 0.638 | Train Acc: 61.36%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.784314  0.432584\n",
      "recall      0.542986  0.700000\n",
      "accuracy    0.595166  0.595166\n",
      "f1-measure  0.641711  0.534722\n",
      "| Validation Accuracy : 59.52 % - Macro F1 Score : 0.5882 \n",
      "| CM on the Validation : \n",
      "[[960. 808.]\n",
      " [264. 616.]]\n",
      "\n",
      "=============================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-d6d40675b37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        batch_size= 100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-298206bc36b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_epochs, optimizer, model, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb_epochs\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4th Step : Train the Model\n",
    "taskAclassifier.train( nb_epochs= 50,\n",
    "                       optimizer= optimizer , \n",
    "                       model= model ,\n",
    "                       batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfzchoAj3kKW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5th Step : Test the Model and save data \n",
    "taskAclassifier.test(folder='RNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) LSTM : \n",
    "Best Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Step : Define Model : \n",
    "model = LSTM(embedding= taskAclassifier.embedding, \n",
    "            embedding_dim = taskAclassifier.dim_vect,\n",
    "            hidden_size=taskAclassifier.dim_vect, \n",
    "            output_size=taskAclassifier.nb_class,\n",
    "            num_layers=1,\n",
    "            output_layer = output_layer(taskAclassifier.dim_vect, taskAclassifier.nb_class,\n",
    "                middle_layers = [150])) #Change this to add/remove middle layers from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Step : Define Optinizer : \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 00 | Train Loss: 0.695 | Train Acc: 47.73%\n",
      "| Stats on the Validation : \n",
      "                 NOT       OFF\n",
      "precision   0.668315  0.421053\n",
      "recall      0.993778  0.009091\n",
      "accuracy    0.666541  0.666541\n",
      "f1-measure  0.799181  0.017798\n",
      "| Validation Accuracy : 66.65 % - Macro F1 Score : 0.4085 \n",
      "| CM on the Validation : \n",
      "[[1757.   11.]\n",
      " [ 872.    8.]]\n",
      "\n",
      "=============================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d6d40675b37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        batch_size= 100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-298206bc36b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_epochs, optimizer, model, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;31m#print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-01972b1793d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutput_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutput_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4th Step : Train the Model\n",
    "taskAclassifier.train( nb_epochs= 50,\n",
    "                       optimizer= optimizer , \n",
    "                       model= model ,\n",
    "                       batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfzchoAj3kKW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5th Step : Test the Model and save data \n",
    "taskAclassifier.test(folder='LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) GRU : \n",
    "Best Parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Step : Define Model : \n",
    "model = GRU(embedding= taskAclassifier.embedding,  \n",
    "            embedding_dim = taskAclassifier.dim_vect,\n",
    "            hidden_size=100, \n",
    "            output_size=taskAclassifier.nb_class,\n",
    "            num_layers=2,\n",
    "            output_layer = output_layer(taskAclassifier.dim_vect, taskAclassifier.nb_class,\n",
    "                middle_layers = [50])) #Change this to add/remove middle layers from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Step : Define Optinizer : \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [100 x 100], m2: [200 x 50] at /opt/conda/conda-bld/pytorch_1549628766161/work/aten/src/THC/generic/THCTensorMathBlas.cu:266",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-d6d40675b37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        batch_size= 100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-298206bc36b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_epochs, optimizer, model, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;31m#print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-01972b1793d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutput_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_lengths\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi2o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/mli/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [100 x 100], m2: [200 x 50] at /opt/conda/conda-bld/pytorch_1549628766161/work/aten/src/THC/generic/THCTensorMathBlas.cu:266"
     ]
    }
   ],
   "source": [
    "# 4th Step : Train the Model\n",
    "taskAclassifier.train( nb_epochs= 50,\n",
    "                       optimizer= optimizer , \n",
    "                       model= model ,\n",
    "                       batch_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfzchoAj3kKW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5th Step : Test the Model and save data \n",
    "taskAclassifier.test(folder='GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Offensive language identification_GPU.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
