{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_output_layer(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_sentence_length):\n",
    "        super(LSTM_output_layer, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.final_activation = nn.Sigmoid()\n",
    "        \n",
    "        self.i2o = nn.Sequential(\n",
    "            nn.Linear(max_sentence_length*hidden_size,\n",
    "                      max_sentence_length*hidden_size//2),\n",
    "            self.activation,\n",
    "            nn.Linear(max_sentence_length*hidden_size//2,\n",
    "                      output_size*4),\n",
    "            self.activation,\n",
    "            nn.Linear(output_size*4,output_size),\n",
    "            self.final_activation)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.reshape(input.shape[0],-1)\n",
    "        output = self.i2o(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embeddingm batch_size=30, input_size=25, hidden_size=25, output_size=1, num_layers=1, max_sentence_length = 20):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size        \n",
    "        \n",
    "        self.LSTM_intermediary = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True)\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.LSTM_final = LSTM_output_layer(hidden_size, output_size, max_sentence_length)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        inputs_embedded = self.embedding(inputs)\n",
    "        \n",
    "        sentence_lengths = (inputs_embedded.sum(dim=2,keepdim=False)!=0).sum(dim=1)\n",
    "        \n",
    "        output, (hidden, cell_state) = self.LSTM_intermediary(inputs_embedded)\n",
    "        print(\"Output shape: \",output.shape)\n",
    "        \n",
    "        output = self.LSTM_final(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_building_block(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RNN_building_block, self).__init__()\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.i2h(input)\n",
    "               \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class RNN_hidden_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, intermediary_layer_sizes=[]):\n",
    "        super(RNN_hidden_layer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.component_layers=[]\n",
    "        if intermediary_layer_sizes!=[]:\n",
    "            \n",
    "            self.component_layers.append(\n",
    "                RNN_building_block(input_size = input_size + hidden_size,\n",
    "                                   output_size = intermediary_layer_sizes[0]))\n",
    "            self.component_layers.append(self.activation)\n",
    "                \n",
    "                \n",
    "            if len(intermediary_layer_sizes)>1:\n",
    "                for layer in range(1,len(intermediary_layer_sizes)):\n",
    "                    self.component_layers.append(\n",
    "                        RNN_building_block(\n",
    "                                input_size = intermediary_layer_sizes[layer-1],\n",
    "                                output_size = intermediary_layer_sizes[layer]))\n",
    "                    self.component_layers.append(self.activation)\n",
    "                    \n",
    "            self.component_layers.append(\n",
    "                RNN_building_block(\n",
    "                        input_size = intermediary_layer_sizes[-1],\n",
    "                        output_size = hidden_size))\n",
    "                \n",
    "        else:\n",
    "            self.component_layers.append(\n",
    "                RNN_building_block(input_size = input_size + hidden_size,\n",
    "                                   output_size = hidden_size))\n",
    "            \n",
    "        self.i2h = nn.Sequential(*self.component_layers)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        #print(input.shape)\n",
    "        #print(hidden.shape)\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "               \n",
    "        return hidden\n",
    "    \n",
    "class RNN_output_layer(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(RNN_output_layer, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation=nn.ReLU()\n",
    "        \n",
    "        self.i2o = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            self.activation,\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            self.activation,\n",
    "            nn.Linear(hidden_size//2, output_size))\n",
    "        self.final_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, hidden): \n",
    "        output = self.i2o(hidden)\n",
    "        output = self.final_activation(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding, batch_size=16, intermediary_dims=[25,25,25], input_size=25, hidden_size=25, output_size=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        #Convolutional layers\n",
    "        \n",
    "        self.rnn_hidden = RNN_hidden_layer(input_size, hidden_size,intermediary_dims)\n",
    "        self.rnn_final = RNN_output_layer(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        hidden = torch.zeros(inputs.shape[0],self.hidden_size)\n",
    "        \n",
    "        inputs_embedded = self.embedding(inputs)\n",
    "        \n",
    "        sentence_lengths = (inputs_embedded.sum(dim=2,keepdim=False)!=0).sum(dim=1)\n",
    "        \n",
    "        final_hidden = torch.zeros(inputs_embedded.shape[0],self.hidden_size)\n",
    "        \n",
    "        for word_no in range(inputs.shape[1]):\n",
    "            hidden = self.rnn_hidden(inputs_embedded[:,word_no,:],hidden)\n",
    "            \n",
    "            is_final_word = (sentence_lengths==word_no+1).nonzero().flatten()\n",
    "\n",
    "            final_hidden[is_final_word] = hidden[is_final_word] \n",
    "\n",
    "        output = self.rnn_final(final_hidden)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        emoji_dict =  {'♥️': ' love ',\n",
    "                       '❤️' : ' love ',\n",
    "                       '❤' : ' love ',\n",
    "                       '😘' : ' kisses ',\n",
    "                      '😭' : ' cry ',\n",
    "                      '💪' : ' strong ',\n",
    "                      '🌍' : ' earth ',\n",
    "                      '💰' : ' money ',\n",
    "                      '👍' : ' ok ',\n",
    "                       '👌' : ' ok ',\n",
    "                      '😡' : ' angry ',\n",
    "                      '🍆' : ' dick ',\n",
    "                      '🤣' : ' haha ',\n",
    "                      '😂' : ' haha ',\n",
    "                      '🖕' : ' fuck you ',\n",
    "                      '💩': 'shit'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        elif cType == 'RNN':\n",
    "            self.model = RNN(self.embedding, 1000, [200,200], dim_vect, 200, 1)\n",
    "        elif cType == \"LSTM\":\n",
    "            self.model = LSTM(self.embedding,  batch_size=16, input_size=dim_vect, hidden_size=dim_vect, output_size=1, num_layers=1)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
